{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0582d22-1b9a-46ff-b867-809a9bf593bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pathlib\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from sec_cik_mapper import StockMapper\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27638f61-691d-4f28-9112-546575675702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "ROOT_OUT = pathlib.Path(\n",
    "    \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/\"\n",
    "    \"Documents/Master Finance/MasterThesis/ThesisData\"\n",
    ")\n",
    "\n",
    "# russell 3000 ticker data\n",
    "RUSSELL_FILE = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/iShares-Russell-3000-ETF_fund.csv\"\n",
    "\n",
    "# api settings\n",
    "UA = \"VU-AM Finance Lab / daniel@vu.nl\"\n",
    "SLEEP = 0.12\n",
    "CHECKPOINT_INTERVAL = 50\n",
    "\n",
    "# years to process (fiscal years 2020-2024, some filing dates in 2025)\n",
    "YEARS_TO_PROCESS = range(2024, 2019, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d4634-90e2-4385-a255-e265f165e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load russell 3000 data\n",
    "df = pd.read_csv(RUSSELL_FILE, skiprows=7)\n",
    "print(f\"loaded {len(df)} companies from russell 3000\")\n",
    "\n",
    "# normalize tickers\n",
    "df[\"Ticker\"] = df[\"Ticker\"].str.upper().str.strip()\n",
    "\n",
    "# map tickers to ciks\n",
    "mapper = StockMapper()\n",
    "df[\"cik\"] = df[\"Ticker\"].map(mapper.ticker_to_cik)\n",
    "\n",
    "# check mapping success\n",
    "mapped_count = df[\"cik\"].notna().sum()\n",
    "print(f\"mapped {mapped_count} out of {len(df)} tickers to ciks\")\n",
    "\n",
    "# drop unmapped tickers\n",
    "df = df.dropna(subset=['cik'])\n",
    "print(f\"final dataset: {len(df)} companies with valid ciks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfc970-08ba-49cd-8497-7e2f480d083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Text Cleaning Functions\n",
    "\n",
    "def get_full_cleaned_10k_text(html_content: str) -> str:\n",
    "    \"\"\"clean html and xbrl markup from 10-k filing to extract readable text\"\"\"\n",
    "    if not isinstance(html_content, str) or not html_content.strip():\n",
    "        return \"\"\n",
    "\n",
    "    processed_content_for_soup = html_content\n",
    "    content_start_pattern = re.compile(r\"(<\\?xml|<html|<body|<!DOCTYPE html)\", re.IGNORECASE)\n",
    "    match = content_start_pattern.search(html_content)\n",
    "    \n",
    "    if match:\n",
    "        text_before_match = html_content[:match.start()]\n",
    "        if \"<TYPE>\" in text_before_match.upper() and \\\n",
    "           (\"<TEXT>\" in text_before_match.upper() or \"<XBRL>\" in text_before_match.upper()):\n",
    "            processed_content_for_soup = html_content[match.start():]\n",
    "        elif not text_before_match.strip():\n",
    "            processed_content_for_soup = html_content[match.start():]\n",
    "        else:\n",
    "            processed_content_for_soup = html_content[match.start():]\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(processed_content_for_soup, 'lxml')\n",
    "        \n",
    "        # remove unwanted tags\n",
    "        for tag_to_remove in soup([\"head\", \"title\", \"meta\", \"script\", \"style\"]):\n",
    "            tag_to_remove.decompose()\n",
    "        \n",
    "        # remove hidden elements\n",
    "        for hidden_element in soup.find_all(style=lambda value: value and \"display:none\" in value.replace(\" \", \"\").lower()):\n",
    "            hidden_element.decompose()\n",
    "        \n",
    "        # remove xbrl wrapper tags\n",
    "        common_ix_wrappers = [\"ix:header\", \"ix:hidden\", \"ix:resources\"]\n",
    "        for ix_tag_name in common_ix_wrappers:\n",
    "            for tag in soup.find_all(ix_tag_name.lower()):\n",
    "                tag.decompose()\n",
    "        \n",
    "        xbrl_tags_to_decompose = [\"xbrli:context\", \"xbrli:unit\", \"link:schemaref\", \n",
    "                                  \"link:linkbaseref\", \"link:roleref\", \"link:arcrolef\"]\n",
    "        for x_tag_name in xbrl_tags_to_decompose:\n",
    "            for tag in soup.find_all(x_tag_name.lower()):\n",
    "                tag.decompose()\n",
    "        \n",
    "        # get cleaned text\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        text = re.sub(r'\\s*\\n\\s*', '\\n', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        # fallback to regex cleaning if beautifulsoup fails\n",
    "        text = re.sub(r\"<[^>]+>\", \" \", processed_content_for_soup)\n",
    "        text = re.sub(r'[ \\t]+', ' ', text).strip()\n",
    "        text = re.sub(r'\\s*\\n\\s*', '\\n', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_main_10k(doc_txt: str) -> str:\n",
    "    \"\"\"extract the main 10-k document block from sec filing text\"\"\"\n",
    "    for part in doc_txt.split(\"<DOCUMENT>\"):\n",
    "        match = re.search(r\"<TYPE>\\s*([\\w\\-/]+)\", part, flags=re.I)\n",
    "        if match and match.group(1).upper() == \"10-K\":\n",
    "            return part\n",
    "    \n",
    "    # fallback - check whole document\n",
    "    match = re.search(r\"<TYPE>\\s*([\\w\\-]+)\", doc_txt, flags=re.I)\n",
    "    if match and match.group(1).upper() == \"10-K\":\n",
    "        return doc_txt\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e67b3e-98da-4d0c-a3cf-56182161f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Section Extraction Functions\n",
    "\n",
    "def extract_risk_factors_section_v4(cleaned_full_text: str) -> str:\n",
    "    \"\"\"extract item 1a risk factors section from cleaned 10-k text\"\"\"\n",
    "    if not isinstance(cleaned_full_text, str) or not cleaned_full_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # flexible pattern for risk factors\n",
    "    item_1a_heading_pattern = re.compile(\n",
    "        r\"Item\\s*1A\\s*\\.?\\s*\" +\n",
    "        r\"(?:R\\s*I\\s*S\\s*K)\\s+\" +\n",
    "        r\"(?:F\\s*A\\s*C\\s*T\\s*O\\s*R\\s*S)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # fallback pattern\n",
    "    simple_rf_pattern = re.compile(\n",
    "        r\"^\\s*(?:R\\s*I\\s*S\\s*K)\\s+(?:F\\s*A\\s*C\\s*T\\s*O\\s*R\\s*S)\\s*$\",\n",
    "        re.IGNORECASE | re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    # end patterns\n",
    "    end_patterns = [\n",
    "        re.compile(r\"Item\\s*1B\\s*\\.?\\s*Unresolved\\s*Staff\\s*Comments\", re.IGNORECASE),\n",
    "        re.compile(r\"Item\\s*2\\s*\\.?\\s*Properties\", re.IGNORECASE),\n",
    "        re.compile(r\"Item\\s*3\\s*\\.?\\s*Legal\\s*Proceedings\", re.IGNORECASE),\n",
    "        re.compile(r\"PART\\s*II\", re.IGNORECASE)\n",
    "    ]\n",
    "\n",
    "    candidate_sections = []\n",
    "    search_patterns_for_start = [item_1a_heading_pattern, simple_rf_pattern]\n",
    "\n",
    "    for start_pattern_idx, start_pattern in enumerate(search_patterns_for_start):\n",
    "        for start_match in start_pattern.finditer(cleaned_full_text):\n",
    "            # skip table of contents entries\n",
    "            potential_toc_indicators_text = cleaned_full_text[start_match.end() : start_match.end() + 100]\n",
    "            if re.match(r\"^\\s*\\.{3,}\", potential_toc_indicators_text):\n",
    "                continue\n",
    "            \n",
    "            line_start_index = cleaned_full_text.rfind('\\n', 0, start_match.start()) + 1\n",
    "            line_end_index = cleaned_full_text.find('\\n', start_match.end())\n",
    "            if line_end_index == -1:\n",
    "                line_end_index = len(cleaned_full_text)\n",
    "            \n",
    "            current_line_of_match = cleaned_full_text[line_start_index:line_end_index]\n",
    "            text_on_line_after_heading = current_line_of_match[start_match.end() - line_start_index:]\n",
    "            \n",
    "            # check for page numbers\n",
    "            cond1_dots_and_page = re.search(r\"^\\s*\\.{3,}\\s*\\d+\\s*$\", text_on_line_after_heading)\n",
    "            cond2_just_page = re.match(r\"^\\s*[ivxlcdm\\d]+(\\b|\\s|$)\", text_on_line_after_heading.lower())\n",
    "            if cond1_dots_and_page or cond2_just_page:\n",
    "                continue\n",
    "\n",
    "            content_start_index = start_match.end()\n",
    "            text_after_heading = cleaned_full_text[content_start_index:]\n",
    "            min_end_idx_in_slice = len(text_after_heading)\n",
    "            end_pattern_found = False\n",
    "            \n",
    "            for end_pattern in end_patterns:\n",
    "                end_match = end_pattern.search(text_after_heading)\n",
    "                if end_match:\n",
    "                    if end_match.start() < min_end_idx_in_slice:\n",
    "                        min_end_idx_in_slice = end_match.start()\n",
    "                    end_pattern_found = True\n",
    "            \n",
    "            if not end_pattern_found:\n",
    "                continue\n",
    "\n",
    "            current_extracted_section = text_after_heading[:min_end_idx_in_slice].strip()\n",
    "            if len(current_extracted_section) > 50:\n",
    "                candidate_sections.append(current_extracted_section)\n",
    "        \n",
    "        if candidate_sections and start_pattern_idx == 0:\n",
    "            break\n",
    "    \n",
    "    if not candidate_sections:\n",
    "        return \"\"\n",
    "    \n",
    "    return max(candidate_sections, key=len)\n",
    "\n",
    "def extract_item7_mda(cleaned_full_text: str) -> str:\n",
    "    \"\"\"extract item 7 management discussion and analysis from cleaned 10-k text\"\"\"\n",
    "    if not isinstance(cleaned_full_text, str) or not cleaned_full_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    item7_heading_pattern = re.compile(\n",
    "        r\"Item\\s*7\\s*\\.\\s*Management['']?s\\s*Discussion\\s*and\\s*Analysis\\s*of\\s*Financial\\s*Condition\\s*and\\s*Results\\s*of\\s*Operations\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    simple_mda_heading_pattern = re.compile(\n",
    "        r\"^\\s*MANAGEMENT['']?S\\s*DISCUSSION\\s*AND\\s*ANALYSIS\\s*(?:OF\\s*FINANCIAL\\s*CONDITION\\s*AND\\s*RESULTS\\s*OF\\s*OPERATIONS)?\\s*$\",\n",
    "        re.IGNORECASE | re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    end_patterns = [\n",
    "        re.compile(r\"Item\\s*7A\\s*\\.?\\s*Quantitative\\s*and\\s*Qualitative\\s*Disclosures\\s*About\\s*Market\\s*Risk\", re.IGNORECASE),\n",
    "        re.compile(r\"Item\\s*8\\s*\\.?\\s*Financial\\s*Statements\\s*and\\s*Supplementary\\s*Data\", re.IGNORECASE),\n",
    "        re.compile(r\"PART\\s*III\", re.IGNORECASE),\n",
    "        re.compile(r\"SIGNATURES\", re.IGNORECASE)\n",
    "    ]\n",
    "\n",
    "    candidate_sections = []\n",
    "    search_patterns_for_start = [item7_heading_pattern, simple_mda_heading_pattern]\n",
    "\n",
    "    for start_pattern_idx, start_pattern in enumerate(search_patterns_for_start):\n",
    "        for start_match in start_pattern.finditer(cleaned_full_text):\n",
    "            # skip table of contents\n",
    "            potential_toc_indicators_text = cleaned_full_text[start_match.end() : start_match.end() + 100]\n",
    "            if re.match(r\"^\\s*\\.{3,}\", potential_toc_indicators_text):\n",
    "                continue\n",
    "            \n",
    "            line_start_index = cleaned_full_text.rfind('\\n', 0, start_match.start()) + 1\n",
    "            line_end_index = cleaned_full_text.find('\\n', start_match.end())\n",
    "            if line_end_index == -1:\n",
    "                line_end_index = len(cleaned_full_text)\n",
    "            \n",
    "            current_line_of_match = cleaned_full_text[line_start_index:line_end_index]\n",
    "            text_on_line_after_heading = current_line_of_match[start_match.end() - line_start_index:]\n",
    "            \n",
    "            cond1_dots_and_page = re.search(r\"^\\s*\\.{3,}\\s*\\d+\\s*$\", text_on_line_after_heading)\n",
    "            cond2_just_page = re.match(r\"^\\s*[ivxlcdm\\d]+(\\b|\\s|$)\", text_on_line_after_heading.lower())\n",
    "            if cond1_dots_and_page or cond2_just_page:\n",
    "                continue\n",
    "\n",
    "            content_start_index = start_match.end()\n",
    "            text_after_heading = cleaned_full_text[content_start_index:]\n",
    "            min_end_idx_in_slice = len(text_after_heading)\n",
    "            end_pattern_found = False\n",
    "            \n",
    "            for end_pattern in end_patterns:\n",
    "                end_match = end_pattern.search(text_after_heading)\n",
    "                if end_match:\n",
    "                    if end_match.start() < min_end_idx_in_slice:\n",
    "                        min_end_idx_in_slice = end_match.start()\n",
    "                    end_pattern_found = True\n",
    "            \n",
    "            if not end_pattern_found:\n",
    "                continue\n",
    "\n",
    "            current_extracted_section = text_after_heading[:min_end_idx_in_slice].strip()\n",
    "            if len(current_extracted_section) > 50:\n",
    "                candidate_sections.append(current_extracted_section)\n",
    "        \n",
    "        if candidate_sections and start_pattern_idx == 0:\n",
    "            break\n",
    "\n",
    "    if not candidate_sections:\n",
    "        return \"\"\n",
    "    \n",
    "    return max(candidate_sections, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a76eae-e700-49ff-9b7c-e926babf1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup requests session\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": UA, \"Accept-Encoding\": \"gzip, deflate\"})\n",
    "\n",
    "def fetch_json(url: str) -> dict | None:\n",
    "    \"\"\"fetch json data from sec api\"\"\"\n",
    "    try:\n",
    "        r = session.get(url, headers={\"Host\": \"data.sec.gov\"})\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        tqdm.write(f\"json error {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_txt(url: str) -> str | None:\n",
    "    \"\"\"fetch text data from sec website\"\"\"\n",
    "    try:\n",
    "        r = session.get(url)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        tqdm.write(f\"text error {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fd6a6-dfe3-4e92-b1dd-936169c3f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column mappings\n",
    "USER_CIK_COLUMN = 'cik'\n",
    "USER_TICKER_COLUMN = 'Ticker'\n",
    "USER_COMPANY_NAME_COLUMN = 'Name'\n",
    "USER_SECTOR_COLUMN = 'Sector'\n",
    "\n",
    "# prepare cik dataframe\n",
    "df_ciks_input = df[[USER_CIK_COLUMN, USER_TICKER_COLUMN, USER_COMPANY_NAME_COLUMN, USER_SECTOR_COLUMN]].copy()\n",
    "df_ciks_input[USER_CIK_COLUMN] = df_ciks_input[USER_CIK_COLUMN].astype(str).str.zfill(10)\n",
    "df_ciks_input.drop_duplicates(subset=[USER_CIK_COLUMN], keep='first', inplace=True)\n",
    "\n",
    "print(f\"processing {len(df_ciks_input)} unique ciks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373c697-6635-4179-891a-25efa8fa1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve 10-K Filings for All Years\n",
    "\n",
    "# main loop to retrieve 10-k filings\n",
    "ROOT_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for current_year in tqdm(YEARS_TO_PROCESS, desc=\"processing years\"):\n",
    "    tqdm.write(f\"\\nprocessing year: {current_year}\")\n",
    "    output_folder_year = ROOT_OUT / str(current_year)\n",
    "    output_folder_year.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # file paths for this year\n",
    "    checkpoint_path = output_folder_year / f\"10k_fullcleaned_info_{current_year}_checkpoint.parquet\"\n",
    "    final_path = output_folder_year / f\"10k_fullcleaned_info_{current_year}_final.parquet\"\n",
    "\n",
    "    records_for_year = []\n",
    "    processed_filing_ids_this_year = set()\n",
    "\n",
    "    # load checkpoint if exists\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            df_checkpoint = pd.read_parquet(checkpoint_path)\n",
    "            records_for_year = df_checkpoint.to_dict('records')\n",
    "            for record in records_for_year:\n",
    "                if USER_CIK_COLUMN in record and 'accession' in record:\n",
    "                    processed_filing_ids_this_year.add((str(record[USER_CIK_COLUMN]).zfill(10), record['accession']))\n",
    "            tqdm.write(f\"resumed for year {current_year}. loaded {len(records_for_year)} records\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"error loading checkpoint: {e}. starting fresh for {current_year}\")\n",
    "            records_for_year, processed_filing_ids_this_year = [], set()\n",
    "\n",
    "    newly_added_count_this_session = 0\n",
    "    \n",
    "    for index, row in tqdm(df_ciks_input.iterrows(), total=len(df_ciks_input), desc=f\"ciks for {current_year}\", leave=False):\n",
    "        cik_from_df = str(row[USER_CIK_COLUMN]).zfill(10)\n",
    "        ticker_from_df = row[USER_TICKER_COLUMN]\n",
    "        company_name_from_df = row[USER_COMPANY_NAME_COLUMN]\n",
    "        sector_from_df = row[USER_SECTOR_COLUMN]\n",
    "        \n",
    "        # fetch submissions data\n",
    "        sub_url = f\"https://data.sec.gov/submissions/CIK{cik_from_df}.json\"\n",
    "        submission_data = fetch_json(sub_url)\n",
    "        time.sleep(SLEEP)\n",
    "        \n",
    "        if not submission_data:\n",
    "            continue\n",
    "        \n",
    "        # get company info from sec\n",
    "        company_name_sec = submission_data.get('name', 'N/A')\n",
    "        tickers_list_sec = submission_data.get('tickers', [])\n",
    "        tickers_sec_str = \", \".join(tickers_list_sec) if tickers_list_sec else 'N/A'\n",
    "\n",
    "        if \"filings\" not in submission_data or \"recent\" not in submission_data[\"filings\"]:\n",
    "            continue\n",
    "            \n",
    "        filings = submission_data[\"filings\"][\"recent\"]\n",
    "        req_keys = [\"form\", \"accessionNumber\", \"filingDate\"]\n",
    "        \n",
    "        if not all(k in filings for k in req_keys) or \\\n",
    "           not all(isinstance(filings[k], list) for k in req_keys) or \\\n",
    "           (len(filings[\"form\"]) > 0 and len(set(len(filings[k]) for k in req_keys)) > 1):\n",
    "            continue\n",
    "\n",
    "        # process each filing\n",
    "        for i in range(len(filings[\"form\"])):\n",
    "            form_type = filings[\"form\"][i]\n",
    "            acc_num = filings[\"accessionNumber\"][i]\n",
    "            fdate_str = filings[\"filingDate\"][i]\n",
    "            \n",
    "            if form_type not in {\"10-K\", \"10-K/A\"}:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                filed_date_obj = dt.strptime(fdate_str, \"%Y-%m-%d\").date()\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            if filed_date_obj.year != current_year or (cik_from_df, acc_num) in processed_filing_ids_this_year:\n",
    "                continue\n",
    "            \n",
    "            # download filing\n",
    "            acc_nodash = acc_num.replace(\"-\", \"\")\n",
    "            filing_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik_from_df)}/{acc_nodash}/{acc_num}.txt\"\n",
    "            raw_filing_text = fetch_txt(filing_url)\n",
    "            time.sleep(SLEEP)\n",
    "            \n",
    "            if not raw_filing_text:\n",
    "                continue\n",
    "\n",
    "            # extract main 10-k document\n",
    "            extracted_block = extract_main_10k(raw_filing_text)\n",
    "            if not extracted_block:\n",
    "                continue\n",
    "            \n",
    "            # get cleaned text\n",
    "            full_cleaned_text = get_full_cleaned_10k_text(extracted_block)\n",
    "            \n",
    "            if not full_cleaned_text.strip():\n",
    "                tqdm.write(f\"cik {cik_from_df}, acc {acc_num}, year {current_year}: empty text after cleaning\")\n",
    "                continue\n",
    "            \n",
    "            # save record\n",
    "            records_for_year.append({\n",
    "                \"cik\": cik_from_df,\n",
    "                \"companyName_user\": company_name_from_df,\n",
    "                \"ticker_user\": ticker_from_df,\n",
    "                \"sector_user\": sector_from_df,\n",
    "                \"companyName_sec\": company_name_sec,\n",
    "                \"tickers_sec\": tickers_sec_str,\n",
    "                \"filingDate\": fdate_str,\n",
    "                \"accession\": acc_num,\n",
    "                \"form\": form_type,\n",
    "                \"year\": current_year,\n",
    "                \"text\": full_cleaned_text,\n",
    "            })\n",
    "            \n",
    "            processed_filing_ids_this_year.add((cik_from_df, acc_num))\n",
    "            newly_added_count_this_session += 1\n",
    "\n",
    "            # save checkpoint periodically\n",
    "            if newly_added_count_this_session > 0 and newly_added_count_this_session % CHECKPOINT_INTERVAL == 0 and records_for_year:\n",
    "                try:\n",
    "                    df_chkp = pd.DataFrame(records_for_year).drop_duplicates(subset=['cik', 'accession'], keep='last')\n",
    "                    df_chkp.to_parquet(checkpoint_path, index=False, compression='zstd')\n",
    "                    tqdm.write(f\"checkpoint for {current_year}: {len(df_chkp)} records saved\")\n",
    "                except Exception as e:\n",
    "                    try:\n",
    "                        df_chkp.to_parquet(checkpoint_path, index=False, compression='gzip')\n",
    "                        tqdm.write(f\"checkpoint (gzip) for {current_year}: {len(df_chkp)} records saved\")\n",
    "                    except Exception as e2:\n",
    "                        tqdm.write(f\"error saving checkpoint: {e2}\")\n",
    "    \n",
    "    # save final data for year\n",
    "    if records_for_year:\n",
    "        try:\n",
    "            df_final = pd.DataFrame(records_for_year).drop_duplicates(subset=['cik', 'accession'], keep='last')\n",
    "            df_final.to_parquet(final_path, index=False, compression='zstd')\n",
    "            tqdm.write(f\"\\nyear {current_year} complete: {len(df_final)} filings saved to {final_path}\")\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                df_final.to_parquet(final_path, index=False, compression='gzip')\n",
    "                tqdm.write(f\"\\nyear {current_year} complete (gzip): {len(df_final)} filings saved\")\n",
    "            except Exception as e2:\n",
    "                tqdm.write(f\"error saving final data for year {current_year}: {e2}\")\n",
    "    else:\n",
    "        tqdm.write(f\"\\nyear {current_year}: no new filings processed\")\n",
    "\n",
    "tqdm.write(\"\\nall years processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0bcf74-d5b0-4906-9ed1-20634e84872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Year Data and Extract Sections\n",
    "\n",
    "def process_year_data(year_int: int, base_file_path_template: str) -> pd.DataFrame | None:\n",
    "    \"\"\"load and process data for a single year, extract risk factors and mda sections\"\"\"\n",
    "    year_str = str(year_int)\n",
    "    file_path = base_file_path_template.format(year=year_str)\n",
    "\n",
    "    columns_to_return_schema = [\n",
    "        'cik', 'companyName_user', 'ticker_user', 'sector_user', \n",
    "        'companyName_sec', 'tickers_sec', 'filingDate', 'accession', \n",
    "        'form', 'year', 'risk_factors_text', 'item7_mda_text'\n",
    "    ]\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"file not found for year {year_str}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"processing data for {year_str}...\")\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        if 'cik' not in df.columns:\n",
    "            print(f\"error: 'cik' column missing for {year_str}\")\n",
    "            return pd.DataFrame(columns=columns_to_return_schema)\n",
    "            \n",
    "        if 'text' not in df.columns:\n",
    "            print(f\"error: 'text' column missing for {year_str}\")\n",
    "            return pd.DataFrame(columns=columns_to_return_schema)\n",
    "\n",
    "        print(f\"loaded {file_path}\")\n",
    "        df['year'] = year_int\n",
    "\n",
    "        # extract sections\n",
    "        print(f\"extracting risk factors for {year_str}...\")\n",
    "        df['risk_factors_text'] = df['text'].apply(extract_risk_factors_section_v4)\n",
    "        \n",
    "        print(f\"extracting md&a for {year_str}...\")\n",
    "        df['item7_mda_text'] = df['text'].apply(extract_item7_mda)\n",
    "        \n",
    "        # filter out rows where extraction failed\n",
    "        is_risk_missing = df['risk_factors_text'].isna() | df['risk_factors_text'].str.strip().eq(\"\")\n",
    "        is_mda_missing = df['item7_mda_text'].isna() | df['item7_mda_text'].str.strip().eq(\"\")\n",
    "        df_cleaned = df[~(is_risk_missing | is_mda_missing)].copy()\n",
    "        \n",
    "        if df_cleaned.empty:\n",
    "            print(f\"no valid rows for {year_str} after extraction\")\n",
    "            return pd.DataFrame(columns=columns_to_return_schema)\n",
    "\n",
    "        # select final columns\n",
    "        final_cols = [col for col in columns_to_return_schema if col in df_cleaned.columns]\n",
    "        df_for_return = df_cleaned[list(dict.fromkeys(final_cols))]\n",
    "\n",
    "        print(f\"finished {year_str}: {len(df_for_return)} rows after cleaning\")\n",
    "        return df_for_return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error processing {year_str}: {e}\")\n",
    "        return pd.DataFrame(columns=columns_to_return_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e7081-7539-4a1a-9a73-6640e7e35a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataset\n",
    "\n",
    "# build panel data from all years\n",
    "target_years = [2024, 2023, 2022, 2021, 2020]\n",
    "base_path_template = str(ROOT_OUT / \"{year}/10k_fullcleaned_info_{year}_final.parquet\")\n",
    "\n",
    "processed_yearly_dfs = {}\n",
    "all_target_year_files_found = True\n",
    "\n",
    "for year_val in target_years:\n",
    "    print(f\"\\nprocessing year: {year_val}\")\n",
    "    df_year_processed = process_year_data(year_val, base_path_template)\n",
    "    \n",
    "    if df_year_processed is None:\n",
    "        all_target_year_files_found = False\n",
    "        print(f\"critical: file for year {year_val} not found\")\n",
    "        break\n",
    "        \n",
    "    processed_yearly_dfs[year_val] = df_year_processed\n",
    "\n",
    "panel_df = pd.DataFrame()\n",
    "\n",
    "if not all_target_year_files_found:\n",
    "    print(\"\\nnot all year files were found\")\n",
    "elif len(processed_yearly_dfs) != len(target_years):\n",
    "    print(f\"\\nprocessing error: expected {len(target_years)} dataframes, got {len(processed_yearly_dfs)}\")\n",
    "else:\n",
    "    print(f\"\\nall {len(target_years)} year files processed. identifying common ciks...\")\n",
    "    \n",
    "    # find ciks present in all years\n",
    "    list_of_cik_sets = []\n",
    "    for year_val in target_years:\n",
    "        df = processed_yearly_dfs[year_val]\n",
    "        if 'cik' in df.columns and not df['cik'].dropna().empty:\n",
    "            list_of_cik_sets.append(set(df['cik'].dropna().unique()))\n",
    "        else:\n",
    "            list_of_cik_sets.append(set())\n",
    "            print(f\"warning: year {year_val} has no valid ciks\")\n",
    "\n",
    "    if len(list_of_cik_sets) == len(target_years):\n",
    "        common_ciks = list_of_cik_sets[0]\n",
    "        for cik_set in list_of_cik_sets[1:]:\n",
    "            common_ciks.intersection_update(cik_set)\n",
    "\n",
    "        print(f\"found {len(common_ciks)} ciks present in all {len(target_years)} years\")\n",
    "\n",
    "        if common_ciks:\n",
    "            # combine data for common ciks\n",
    "            final_data_to_concat = []\n",
    "            for year_val in target_years:\n",
    "                df_year = processed_yearly_dfs[year_val]\n",
    "                if 'cik' in df_year.columns:\n",
    "                    filtered_df = df_year[df_year['cik'].isin(common_ciks)].copy()\n",
    "                    final_data_to_concat.append(filtered_df)\n",
    "            \n",
    "            if final_data_to_concat and len(final_data_to_concat) == len(target_years):\n",
    "                panel_df = pd.concat(final_data_to_concat, ignore_index=True)\n",
    "                print(f\"concatenated data: {len(panel_df)} total rows\")\n",
    "\n",
    "                if not panel_df.empty:\n",
    "                    # sort by cik and year\n",
    "                    panel_df['year'] = pd.to_numeric(panel_df['year'])\n",
    "                    panel_df.sort_values(by=['cik', 'year'], ascending=[True, False], inplace=True)\n",
    "                    print(\"panel data sorted\")\n",
    "\n",
    "                    # verify all ciks have correct number of years\n",
    "                    cik_counts = panel_df.groupby('cik').size()\n",
    "                    ciks_with_unexpected_counts = cik_counts[cik_counts != len(target_years)]\n",
    "                    if not ciks_with_unexpected_counts.empty:\n",
    "                        print(f\"\\nwarning: some ciks do not have exactly {len(target_years)} entries\")\n",
    "                        print(ciks_with_unexpected_counts)\n",
    "                    else:\n",
    "                        print(f\"\\nverified: all {panel_df['cik'].nunique()} ciks have {len(target_years)} years of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b81842-592e-4da8-8d3f-66e5ac51b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the initial panel data\n",
    "output_filename = \"panel_data_2020_2024_strict_5years.parquet\"\n",
    "output_full_path = ROOT_OUT / output_filename\n",
    "\n",
    "if not panel_df.empty:\n",
    "    try:\n",
    "        panel_df.to_parquet(output_full_path, index=False)\n",
    "        print(f\"\\npanel data saved to {output_full_path}\")\n",
    "        print(f\"final panel: {len(panel_df)} rows, {panel_df['cik'].nunique()} unique ciks\")\n",
    "    except Exception as e:\n",
    "        print(f\"error saving panel data: {e}\")\n",
    "else:\n",
    "    print(\"\\npanel data is empty, nothing saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24014e42-45b5-4143-8bdb-e0778e152661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate filings (some ciks may have multiple 10-k filings per year)\n",
    "print(\"\\nchecking for duplicate filings...\")\n",
    "\n",
    "# reload the panel data\n",
    "panel_df = pd.read_parquet(output_full_path)\n",
    "print(f\"loaded panel data: {len(panel_df)} rows\")\n",
    "\n",
    "# check for ciks with more than expected entries\n",
    "cik_counts = panel_df.groupby('cik').size()\n",
    "problematic_ciks = cik_counts[cik_counts > 5]\n",
    "\n",
    "if not problematic_ciks.empty:\n",
    "    print(f\"found {len(problematic_ciks)} ciks with more than 5 entries\")\n",
    "    print(problematic_ciks.head())\n",
    "else:\n",
    "    print(\"no duplicate issues found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9e9ad-815d-4120-9c4f-f8e218184b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate by keeping most recent filing per cik-year\n",
    "panel_df['filingDate'] = pd.to_datetime(panel_df['filingDate'])\n",
    "print(\"converted filing dates to datetime\")\n",
    "\n",
    "# sort by cik, year, and filing date (most recent first)\n",
    "panel_df_sorted = panel_df.sort_values(\n",
    "    by=['cik', 'year', 'filingDate'],\n",
    "    ascending=[True, True, False]\n",
    ")\n",
    "\n",
    "# keep only the most recent filing for each cik-year combination\n",
    "panel_df_deduplicated = panel_df_sorted.drop_duplicates(\n",
    "    subset=['cik', 'year'],\n",
    "    keep='first'\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"deduplication complete: {len(panel_df)} -> {len(panel_df_deduplicated)} rows\")\n",
    "\n",
    "# verify all ciks now have exactly 5 entries\n",
    "cik_counts_after = panel_df_deduplicated.groupby('cik').size()\n",
    "problematic_after = cik_counts_after[cik_counts_after != 5]\n",
    "\n",
    "if problematic_after.empty:\n",
    "    print(f\"success: all {panel_df_deduplicated['cik'].nunique()} ciks have exactly 5 entries\")\n",
    "else:\n",
    "    print(f\"warning: {len(problematic_after)} ciks still have incorrect entry counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b02160-6943-4626-b4af-559280918ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate text lengths for filtering\n",
    "print(\"\\ncalculating text lengths...\")\n",
    "\n",
    "if 'risk_factors_text' in panel_df_deduplicated.columns:\n",
    "    panel_df_deduplicated['len_risk_factors'] = panel_df_deduplicated['risk_factors_text'].str.len()\n",
    "    print(\"calculated risk factors text length\")\n",
    "\n",
    "if 'item7_mda_text' in panel_df_deduplicated.columns:\n",
    "    panel_df_deduplicated['len_item7_mda'] = panel_df_deduplicated['item7_mda_text'].str.len()\n",
    "    print(\"calculated md&a text length\")\n",
    "\n",
    "# combined length\n",
    "if 'len_risk_factors' in panel_df_deduplicated.columns and 'len_item7_mda' in panel_df_deduplicated.columns:\n",
    "    panel_df_deduplicated['combined_len_rf_mda'] = (\n",
    "        panel_df_deduplicated['len_risk_factors'] + panel_df_deduplicated['len_item7_mda']\n",
    "    )\n",
    "    print(\"calculated combined text length\")\n",
    "\n",
    "# show length statistics\n",
    "print(\"\\ntext length statistics:\")\n",
    "print(panel_df_deduplicated[['len_risk_factors', 'len_item7_mda', 'combined_len_rf_mda']].describe())\n",
    "\n",
    "# filter out ciks with extremely long combined text (>200k characters)\n",
    "threshold = 200000\n",
    "\n",
    "# find ciks exceeding threshold\n",
    "max_combined_len_per_cik = panel_df_deduplicated.groupby('cik')['combined_len_rf_mda'].max()\n",
    "ciks_exceeding_threshold = max_combined_len_per_cik[max_combined_len_per_cik > threshold]\n",
    "ciks_to_exclude = ciks_exceeding_threshold.index.tolist()\n",
    "\n",
    "print(f\"found {len(ciks_to_exclude)} ciks with combined text > {threshold:,} characters\")\n",
    "\n",
    "# create filtered dataset\n",
    "panel_df_filtered = panel_df_deduplicated[\n",
    "    ~panel_df_deduplicated['cik'].isin(ciks_to_exclude)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nfiltering results:\")\n",
    "print(f\"original ciks: {panel_df_deduplicated['cik'].nunique()}\")\n",
    "print(f\"remaining ciks: {panel_df_filtered['cik'].nunique()}\")\n",
    "print(f\"original rows: {len(panel_df_deduplicated)}\")\n",
    "print(f\"remaining rows: {len(panel_df_filtered)}\")\n",
    "\n",
    "# show updated statistics\n",
    "print(\"\\nfiltered text length statistics:\")\n",
    "print(panel_df_filtered[['len_risk_factors', 'len_item7_mda', 'combined_len_rf_mda']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b428e-2d66-40a4-ba6f-4b80777fd823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save deduplicated panel data\n",
    "output_deduplicated_path = ROOT_OUT / \"panel_data_2020_2024.parquet\"\n",
    "panel_df_deduplicated.to_parquet(output_deduplicated_path, index=False)\n",
    "print(f\"saved deduplicated panel data to: {output_deduplicated_path}\")\n",
    "\n",
    "# save filtered panel data (with text length constraint)\n",
    "panel_data_charactermax200k = panel_df_filtered.copy()\n",
    "output_filtered_path = ROOT_OUT / \"panel_data_charactermax200k.parquet\"\n",
    "\n",
    "try:\n",
    "    panel_data_charactermax200k.to_parquet(output_filtered_path, index=False)\n",
    "    print(f\"\\nsaved filtered panel data to: {output_filtered_path}\")\n",
    "    print(f\"rows saved: {len(panel_data_charactermax200k)}\")\n",
    "    print(f\"unique ciks: {panel_data_charactermax200k['cik'].nunique()}\")\n",
    "    \n",
    "    # final verification\n",
    "    cik_counts_final = panel_data_charactermax200k.groupby('cik').size()\n",
    "    if not cik_counts_final[cik_counts_final != 5].empty:\n",
    "        print(\"\\nwarning: some ciks in final dataset don't have exactly 5 entries\")\n",
    "    else:\n",
    "        print(f\"\\nverified: all {panel_data_charactermax200k['cik'].nunique()} ciks have exactly 5 years of data\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"error saving filtered data: {e}\")\n",
    "\n",
    "# load and verify the final saved data\n",
    "df = pd.read_parquet(output_filtered_path)\n",
    "\n",
    "print(f\"successfully loaded final dataset from: {output_filtered_path}\")\n",
    "print(f\"shape: {df.shape}\")\n",
    "print(f\"unique ciks: {df['cik'].nunique()}\")\n",
    "print(f\"\\ncolumns: {list(df.columns)}\")\n",
    "\n",
    "# show sample data\n",
    "print(\"\\nsample data (first 5 rows):\")\n",
    "print(df[['cik', 'ticker_user', 'companyName_user', 'year', 'filingDate', \n",
    "         'len_risk_factors', 'len_item7_mda', 'combined_len_rf_mda']].head())\n",
    "\n",
    "# year distribution\n",
    "print(\"\\nyear distribution:\")\n",
    "print(df['year'].value_counts().sort_index())\n",
    "\n",
    "# sector distribution\n",
    "print(\"\\nsector distribution:\")\n",
    "print(df['sector_user'].value_counts().head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
