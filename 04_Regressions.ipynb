{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153b3c2-c3ab-408b-bac6-3d86031c381e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load the data\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/final_clean_dataset_filtered_with_corrected_factor_loadings.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"Years covered: {df['Year'].min()} - {df['Year'].max()}\")\n",
    "print(f\"Unique companies: {df['gvkey'].nunique()}\")\n",
    "\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3641b3-3f93-471b-9651-1335710d8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fix_annualized_returns_in_factor_dataset(df):\n",
    "    \"\"\"\n",
    "    apply additional filtering to annualized returns in the factor loadings dataset\n",
    "    to remove extreme values that snuck back in through annualization\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Fixing annualized returns in factor loadings dataset\")\n",
    "    print(f\"Input dataset: {df.shape}\")\n",
    "    \n",
    "    # identify annualized return columns\n",
    "    annualized_cols = [col for col in df.columns if 'annualized' in col]\n",
    "    raw_return_cols = [col for col in df.columns if 'return_' in col and 'excess' not in col and 'annualized' not in col]\n",
    "    excess_return_cols = [col for col in df.columns if 'excess_return_' in col and 'annualized' not in col]\n",
    "    \n",
    "    print(f\"Found columns:\")\n",
    "    print(f\"   Raw returns (non-annualized): {len(raw_return_cols)}\")\n",
    "    print(f\"   Excess returns (non-annualized): {len(excess_return_cols)}\")  \n",
    "    print(f\"   Annualized returns: {len(annualized_cols)}\")\n",
    "    \n",
    "    # check current statistics before fixing\n",
    "    print(f\"Before fixing - extreme value counts:\")\n",
    "    for col in annualized_cols:\n",
    "        if col in df.columns:\n",
    "            values = df[col].dropna()\n",
    "            if len(values) > 0:\n",
    "                extreme_count = (values.abs() > 2.0).sum()  # >200%\n",
    "                very_extreme = (values.abs() > 5.0).sum()   # >500%\n",
    "                \n",
    "                print(f\"   {col[:30]:30}: {extreme_count:4d} >200% ({extreme_count/len(values):.1%}), \"\n",
    "                      f\"{very_extreme:4d} >500% ({very_extreme/len(values):.1%})\")\n",
    "    \n",
    "    # apply reasonable bounds to annualized returns\n",
    "    print(\"Applying quality filters to annualized returns\")\n",
    "    print(\"Bounds: -95% to +500% (reasonable for annualized individual stock returns)\")\n",
    "    \n",
    "    fixed_count = 0\n",
    "    total_observations = 0\n",
    "    \n",
    "    for col in annualized_cols:\n",
    "        if col in df.columns:\n",
    "            original_data = df[col].copy()\n",
    "            valid_data = original_data.dropna()\n",
    "            \n",
    "            if len(valid_data) > 0:\n",
    "                # apply reasonable bounds for annualized returns\n",
    "                lower_bound = -0.95  # -95% (total loss)\n",
    "                upper_bound = 5.00   # +500% (very high but possible for individual stocks)\n",
    "                \n",
    "                # count what will be filtered\n",
    "                too_low = (valid_data < lower_bound).sum()\n",
    "                too_high = (valid_data > upper_bound).sum()\n",
    "                total_filtered = too_low + too_high\n",
    "                \n",
    "                # apply filter\n",
    "                df[col] = df[col].where(\n",
    "                    (df[col] >= lower_bound) & (df[col] <= upper_bound), \n",
    "                    np.nan\n",
    "                )\n",
    "                \n",
    "                # report results\n",
    "                remaining = df[col].notna().sum()\n",
    "                print(f\"   {col[:30]:30}: filtered {total_filtered:4d}/{len(valid_data):4d} \"\n",
    "                      f\"({total_filtered/len(valid_data):5.1%}), remaining: {remaining:4d}\")\n",
    "                \n",
    "                fixed_count += total_filtered\n",
    "                total_observations += len(valid_data)\n",
    "    \n",
    "    print(f\"Filtering summary:\")\n",
    "    print(f\"   Total annualized observations processed: {total_observations:,}\")\n",
    "    print(f\"   Total extreme values filtered: {fixed_count:,}\")\n",
    "    print(f\"   Filtering rate: {fixed_count/total_observations:.1%}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# run the fix\n",
    "print(\"Return filtering fix for factor loadings dataset\")\n",
    "\n",
    "# paths\n",
    "enhanced_dataset_path = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/final_enhanced_dataset.csv\"\n",
    "factor_loadings_path = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/final_clean_dataset_filtered_with_corrected_factor_loadings.csv\"\n",
    "\n",
    "print(\"What this fixes:\")\n",
    "print(\"Keeps the excellent factor loadings (40% r squared)\")\n",
    "print(\"Keeps clean non-annualized returns\") \n",
    "print(\"Filters extreme annualized returns that snuck back in\")\n",
    "print(\"Makes dataset consistent with the original filtering\")\n",
    "\n",
    "# clean the data\n",
    "clean_df = fix_annualized_returns_in_factor_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70237ec-1d82-4ceb-993b-cb917d815027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "def create_regression_variables_summary(df, winsorize_level=0.01):\n",
    "    \"\"\"\n",
    "    create comprehensive summary of regression variables with winsorization analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"regression variables analysis & winsorization\")\n",
    "    \n",
    "    # define key regression variables\n",
    "    regression_vars = {\n",
    "        # dependent variables (y)\n",
    "        'Dependent Variables (Y)': {\n",
    "            'excess_return_3mo': '3-Month Excess Return',\n",
    "            'excess_return_6mo': '6-Month Excess Return', \n",
    "            'excess_return_9mo': '9-Month Excess Return',\n",
    "            'excess_return_12mo': '12-Month Excess Return'\n",
    "        },\n",
    "        \n",
    "        # control variables (x)\n",
    "        'Control Variables': {\n",
    "            'calc_log_market_cap': 'Log Market Cap (Size)',\n",
    "            'calc_price_to_book': 'Price-to-Book (Value)',\n",
    "            'calc_roa': 'ROA (Profitability)'\n",
    "        },\n",
    "        \n",
    "        # factor betas (3-month window as example)\n",
    "        'Factor Betas (3mo)': {\n",
    "            'beta_mktrf_t3': 'Market Beta (MKTRF)',\n",
    "            'beta_smb_t3': 'Size Beta (SMB)',\n",
    "            'beta_hml_t3': 'Value Beta (HML)',\n",
    "            'beta_rmw_t3': 'Profitability Beta (RMW)',\n",
    "            'beta_cma_t3': 'Investment Beta (CMA)', \n",
    "            'beta_umd_t3': 'Momentum Beta (UMD)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # flatten variable dictionary for processing\n",
    "    all_vars = {}\n",
    "    for category, vars_dict in regression_vars.items():\n",
    "        all_vars.update(vars_dict)\n",
    "    \n",
    "    # filter to only existing columns\n",
    "    existing_vars = {var: desc for var, desc in all_vars.items() if var in df.columns}\n",
    "    print(f\"found {len(existing_vars)} regression variables in dataset\")\n",
    "    \n",
    "    # create summary statistics before winsorization\n",
    "    print(\"summary statistics - before winsorization\")\n",
    "    \n",
    "    summary_before = create_summary_table(df, existing_vars, \"Before Winsorization\")\n",
    "    print(summary_before.round(4))\n",
    "    \n",
    "    # apply winsorization\n",
    "    print(f\"applying winsorization ({winsorize_level*100:.0f}%/{100-winsorize_level*100:.0f}%)\")\n",
    "    \n",
    "    df_winsorized = df.copy()\n",
    "    winsorization_report = {}\n",
    "    \n",
    "    for var in existing_vars.keys():\n",
    "        if var in df.columns:\n",
    "            original_data = df[var].dropna()\n",
    "            \n",
    "            if len(original_data) > 0:\n",
    "                # apply winsorization\n",
    "                winsorized_data = winsorize(original_data, limits=[winsorize_level, winsorize_level])\n",
    "                df_winsorized[var] = df[var].copy()\n",
    "                df_winsorized.loc[original_data.index, var] = winsorized_data\n",
    "                \n",
    "                # track changes\n",
    "                n_changed = (original_data != winsorized_data).sum()\n",
    "                pct_changed = (n_changed / len(original_data)) * 100\n",
    "                \n",
    "                winsorization_report[var] = {\n",
    "                    'n_obs': len(original_data),\n",
    "                    'n_changed': n_changed,\n",
    "                    'pct_changed': pct_changed,\n",
    "                    'original_min': original_data.min(),\n",
    "                    'original_max': original_data.max(),\n",
    "                    'winsor_min': winsorized_data.min(),\n",
    "                    'winsor_max': winsorized_data.max()\n",
    "                }\n",
    "    \n",
    "    # display winsorization impact\n",
    "    print(\"winsorization impact summary:\")\n",
    "    print(f\"{'Variable':30} {'N Obs':>8} {'Changed':>8} {'% Changed':>10} {'Orig Range':>20} {'Winsor Range':>20}\")\n",
    "    print(\"-\" * 105)\n",
    "    \n",
    "    for var, report in winsorization_report.items():\n",
    "        var_name = existing_vars[var][:25]\n",
    "        orig_range = f\"[{report['original_min']:.3f}, {report['original_max']:.3f}]\"\n",
    "        winsor_range = f\"[{report['winsor_min']:.3f}, {report['winsor_max']:.3f}]\"\n",
    "        \n",
    "        print(f\"{var_name:30} {report['n_obs']:>8,} {report['n_changed']:>8} {report['pct_changed']:>9.1f}% {orig_range:>20} {winsor_range:>20}\")\n",
    "    \n",
    "    # create summary statistics after winsorization\n",
    "    print(\"summary statistics - after winsorization\")\n",
    "    \n",
    "    summary_after = create_summary_table(df_winsorized, existing_vars, \"After Winsorization\")\n",
    "    print(summary_after.round(4))\n",
    "    \n",
    "    return df_winsorized, summary_before, summary_after\n",
    "\n",
    "def create_summary_table(df, variables_dict, title):\n",
    "    \"\"\"create detailed summary statistics table\"\"\"\n",
    "    \n",
    "    summary_stats = []\n",
    "    \n",
    "    for var, description in variables_dict.items():\n",
    "        if var in df.columns:\n",
    "            data = df[var].dropna()\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                stats_dict = {\n",
    "                    'Variable': description,\n",
    "                    'N': len(data),\n",
    "                    'Mean': data.mean(),\n",
    "                    'Std': data.std(),\n",
    "                    'Min': data.min(),\n",
    "                    'P25': data.quantile(0.25),\n",
    "                    'Median': data.median(),\n",
    "                    'P75': data.quantile(0.75),\n",
    "                    'Max': data.max(),\n",
    "                    'Skewness': data.skew(),\n",
    "                    'Kurtosis': data.kurtosis()\n",
    "                }\n",
    "                summary_stats.append(stats_dict)\n",
    "    \n",
    "    return pd.DataFrame(summary_stats).set_index('Variable')\n",
    "\n",
    "# run the complete analysis\n",
    "df_winsorized, before_stats, after_stats = create_regression_variables_summary(df, winsorize_level=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d2cee6-49a4-4061-a0f5-e2d2cf54a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def investigate_data_quality_issues(df):\n",
    "    \"\"\"\n",
    "    investigate problematic p/b ratios and factor betas\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"data quality investigation\")\n",
    "    \n",
    "    # price-to-book investigation\n",
    "    print(\"1. price-to-book ratio investigation\")\n",
    "    \n",
    "    pb_col = 'calc_price_to_book'\n",
    "    if pb_col in df.columns:\n",
    "        pb_data = df[pb_col].dropna()\n",
    "        \n",
    "        print(f\"price-to-book analysis:\")\n",
    "        print(f\"   total observations: {len(pb_data):,}\")\n",
    "        print(f\"   negative values: {(pb_data < 0).sum():,} ({(pb_data < 0).sum()/len(pb_data)*100:.1f}%)\")\n",
    "        print(f\"   extreme negative (<-100): {(pb_data < -100).sum():,}\")\n",
    "        print(f\"   extreme positive (>100): {(pb_data > 100).sum():,}\")\n",
    "        print(f\"   normal range (0-20): {((pb_data >= 0) & (pb_data <= 20)).sum():,}\")\n",
    "        \n",
    "        # show most extreme values\n",
    "        print(\"most extreme p/b values:\")\n",
    "        print(\"most negative:\")\n",
    "        extreme_neg = pb_data.nsmallest(5)\n",
    "        for idx, val in extreme_neg.items():\n",
    "            company = df.loc[idx, 'Company Name'] if 'Company Name' in df.columns else 'unknown'\n",
    "            ticker = df.loc[idx, 'Ticker'] if 'Ticker' in df.columns else 'unknown'\n",
    "            year = df.loc[idx, 'Year'] if 'Year' in df.columns else 'unknown'\n",
    "            print(f\"   {val:8.1f} - {ticker} ({company[:30]}) - {year}\")\n",
    "        \n",
    "        print(\"most positive:\")\n",
    "        extreme_pos = pb_data.nlargest(5)\n",
    "        for idx, val in extreme_pos.items():\n",
    "            company = df.loc[idx, 'Company Name'] if 'Company Name' in df.columns else 'unknown'\n",
    "            ticker = df.loc[idx, 'Ticker'] if 'Ticker' in df.columns else 'unknown'\n",
    "            year = df.loc[idx, 'Year'] if 'Year' in df.columns else 'unknown'\n",
    "            print(f\"   {val:8.1f} - {ticker} ({company[:30]}) - {year}\")\n",
    "    \n",
    "    # factor betas investigation  \n",
    "    print(\"2. factor betas investigation\")\n",
    "    \n",
    "    beta_cols = [col for col in df.columns if col.startswith('beta_') and col.endswith('_t3')]\n",
    "    \n",
    "    for beta_col in beta_cols:\n",
    "        if beta_col in df.columns:\n",
    "            beta_data = df[beta_col].dropna()\n",
    "            factor_name = beta_col.replace('beta_', '').replace('_t3', '').upper()\n",
    "            \n",
    "            print(f\"{factor_name} beta analysis:\")\n",
    "            print(f\"   total observations: {len(beta_data):,}\")\n",
    "            print(f\"   extreme negative (<-5): {(beta_data < -5).sum():,}\")\n",
    "            print(f\"   extreme positive (>5): {(beta_data > 5).sum():,}\")\n",
    "            print(f\"   normal range (-3 to 3): {((beta_data >= -3) & (beta_data <= 3)).sum():,} ({((beta_data >= -3) & (beta_data <= 3)).sum()/len(beta_data)*100:.1f}%)\")\n",
    "            \n",
    "            # show most extreme betas\n",
    "            if (beta_data.abs() > 5).any():\n",
    "                print(f\"   extreme {factor_name} betas:\")\n",
    "                extreme_betas = beta_data[beta_data.abs() > 5].sort_values(key=abs, ascending=False).head(3)\n",
    "                for idx, val in extreme_betas.items():\n",
    "                    company = df.loc[idx, 'Company Name'] if 'Company Name' in df.columns else 'unknown'\n",
    "                    ticker = df.loc[idx, 'Ticker'] if 'Ticker' in df.columns else 'unknown'\n",
    "                    year = df.loc[idx, 'Year'] if 'Year' in df.columns else 'unknown'\n",
    "                    print(f\"      {val:6.2f} - {ticker} ({company[:25]}) - {year}\")\n",
    "    \n",
    "    # return data quality check\n",
    "    print(\"3. return data quality check\")\n",
    "    \n",
    "    return_cols = ['excess_return_3mo', 'excess_return_6mo', 'excess_return_9mo', 'excess_return_12mo']\n",
    "    \n",
    "    for ret_col in return_cols:\n",
    "        if ret_col in df.columns:\n",
    "            ret_data = df[ret_col].dropna()\n",
    "            horizon = ret_col.replace('excess_return_', '').replace('mo', '')\n",
    "            \n",
    "            extreme_positive = (ret_data > 1).sum()  # >100%\n",
    "            extreme_negative = (ret_data < -0.8).sum()  # <-80%\n",
    "            \n",
    "            print(f\"   {horizon}-month: {extreme_positive} extreme positive, {extreme_negative} extreme negative\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "run_data_quality_investigation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76bfb2-feb5-4339-8518-b2319c127cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def enhanced_winsorization_strategy(df):\n",
    "    \"\"\"\n",
    "    Apply differentiated winsorization strategy based on variable characteristics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Enhanced winsorization strategy\")\n",
    "    \n",
    "    # Define variable-specific winsorization levels\n",
    "    winsorization_strategy = {\n",
    "        # More aggressive for highly problematic variables\n",
    "        'Aggressive (5%/95%)': {\n",
    "            'variables': ['calc_price_to_book'],\n",
    "            'level': 0.05,\n",
    "            'reason': 'High concentration of extreme outliers'\n",
    "        },\n",
    "        \n",
    "        # Moderate for factor betas (small % of outliers)\n",
    "        'Moderate (2%/98%)': {\n",
    "            'variables': [\n",
    "                'beta_mktrf_t3', 'beta_smb_t3', 'beta_hml_t3', \n",
    "                'beta_rmw_t3', 'beta_cma_t3', 'beta_umd_t3',\n",
    "                'beta_mktrf_t6', 'beta_smb_t6', 'beta_hml_t6',\n",
    "                'beta_rmw_t6', 'beta_cma_t6', 'beta_umd_t6',\n",
    "                'beta_mktrf_t9', 'beta_smb_t9', 'beta_hml_t9',\n",
    "                'beta_rmw_t9', 'beta_cma_t9', 'beta_umd_t9',\n",
    "                'beta_mktrf_t12', 'beta_smb_t12', 'beta_hml_t12',\n",
    "                'beta_rmw_t12', 'beta_cma_t12', 'beta_umd_t12'\n",
    "            ],\n",
    "            'level': 0.02,\n",
    "            'reason': '96-99% of values in normal range'\n",
    "        },\n",
    "        \n",
    "        # Standard for other financial variables\n",
    "        'Standard (1%/99%)': {\n",
    "            'variables': [\n",
    "                'excess_return_3mo', 'excess_return_6mo', 'excess_return_9mo', 'excess_return_12mo',\n",
    "                'calc_log_market_cap', 'calc_roa', 'calc_roe', 'calc_debt_to_assets',\n",
    "                'calc_debt_to_equity', 'calc_operating_margin', 'calc_profit_margin'\n",
    "            ],\n",
    "            'level': 0.01,\n",
    "            'reason': 'Standard practice for financial variables'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create copy for winsorized data\n",
    "    df_winsorized = df.copy()\n",
    "    winsorization_report = {}\n",
    "    \n",
    "    print(\"Applying differentiated winsorization:\")\n",
    "    \n",
    "    # Apply winsorization by strategy\n",
    "    for strategy_name, strategy_info in winsorization_strategy.items():\n",
    "        print(f\"\\n{strategy_name}:\")\n",
    "        print(f\"   Level: {strategy_info['level']*100:.0f}%/{100-strategy_info['level']*100:.0f}%\")\n",
    "        print(f\"   Reason: {strategy_info['reason']}\")\n",
    "        \n",
    "        variables_processed = []\n",
    "        \n",
    "        for var in strategy_info['variables']:\n",
    "            if var in df.columns:\n",
    "                original_data = df[var].dropna()\n",
    "                \n",
    "                if len(original_data) > 0:\n",
    "                    # Apply winsorization\n",
    "                    winsorized_data = winsorize(original_data, limits=[strategy_info['level'], strategy_info['level']])\n",
    "                    df_winsorized[var] = df[var].copy()\n",
    "                    df_winsorized.loc[original_data.index, var] = winsorized_data\n",
    "                    \n",
    "                    # Track changes\n",
    "                    n_changed = (original_data != winsorized_data).sum()\n",
    "                    pct_changed = (n_changed / len(original_data)) * 100\n",
    "                    \n",
    "                    winsorization_report[var] = {\n",
    "                        'strategy': strategy_name,\n",
    "                        'level': strategy_info['level'],\n",
    "                        'n_obs': len(original_data),\n",
    "                        'n_changed': n_changed,\n",
    "                        'pct_changed': pct_changed,\n",
    "                        'original_min': original_data.min(),\n",
    "                        'original_max': original_data.max(),\n",
    "                        'winsor_min': winsorized_data.min(),\n",
    "                        'winsor_max': winsorized_data.max(),\n",
    "                        'original_std': original_data.std(),\n",
    "                        'winsor_std': winsorized_data.std()\n",
    "                    }\n",
    "                    \n",
    "                    variables_processed.append(var)\n",
    "        \n",
    "        print(f\"   Variables processed: {len(variables_processed)}\")\n",
    "        if variables_processed:\n",
    "            print(f\"   Sample: {variables_processed[:3]}{'...' if len(variables_processed) > 3 else ''}\")\n",
    "    \n",
    "    return df_winsorized, winsorization_report\n",
    "\n",
    "# apply the enhanced winsorization\n",
    "df_final, report = enhanced_winsorization_strategy(df)\n",
    "\n",
    "print(\"Enhanced winsorization strategy complete\")\n",
    "print(\"This approach provides:\")\n",
    "print(\"Variable-specific winsorization levels\")\n",
    "print(\"5%/95% for problematic variables (p/b)\")\n",
    "print(\"2%/98% for factor betas\")\n",
    "print(\"1%/99% for standard financial variables\")\n",
    "print(\"Quality flags for flexible filtering\")\n",
    "print(\"Comprehensive before/after analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe913f-bdd3-4849-980b-57250eacafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_final_regression_summary_table(df_input, export_to_csv=True, filename=\"final_regression_summary.csv\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive summary statistics table for all regression variables\n",
    "    after enhanced winsorization - ready for thesis/publication\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Final regression variables summary statistics\")\n",
    "    print(f\"Dataset shape: {df_input.shape}\")\n",
    "    \n",
    "    # Define all regression variables with proper categorization\n",
    "    regression_variables = {\n",
    "        # dependent variables\n",
    "        'Dependent Variables': {\n",
    "            'excess_return_3mo': '3-Month Excess Return',\n",
    "            'excess_return_6mo': '6-Month Excess Return',\n",
    "            'excess_return_9mo': '9-Month Excess Return', \n",
    "            'excess_return_12mo': '12-Month Excess Return'\n",
    "        },\n",
    "        \n",
    "        # control variables  \n",
    "        'Control Variables': {\n",
    "            'calc_log_market_cap': 'Log Market Cap (Size)',\n",
    "            'calc_price_to_book': 'Price-to-Book (Value)',\n",
    "            'calc_roa': 'ROA (Profitability)'\n",
    "        },\n",
    "        \n",
    "        # factor betas (3-month estimation window)\n",
    "        'Factor Betas (3-month)': {\n",
    "            'beta_mktrf_t3': 'Market Beta (MKTRF)',\n",
    "            'beta_smb_t3': 'Size Beta (SMB)', \n",
    "            'beta_hml_t3': 'Value Beta (HML)',\n",
    "            'beta_rmw_t3': 'Profitability Beta (RMW)',\n",
    "            'beta_cma_t3': 'Investment Beta (CMA)',\n",
    "            'beta_umd_t3': 'Momentum Beta (UMD)'\n",
    "        },\n",
    "        \n",
    "        # factor betas (6-month estimation window)\n",
    "        'Factor Betas (6-month)': {\n",
    "            'beta_mktrf_t6': 'Market Beta (MKTRF)',\n",
    "            'beta_smb_t6': 'Size Beta (SMB)',\n",
    "            'beta_hml_t6': 'Value Beta (HML)', \n",
    "            'beta_rmw_t6': 'Profitability Beta (RMW)',\n",
    "            'beta_cma_t6': 'Investment Beta (CMA)',\n",
    "            'beta_umd_t6': 'Momentum Beta (UMD)'\n",
    "        },\n",
    "        \n",
    "        # factor betas (9-month estimation window)\n",
    "        'Factor Betas (9-month)': {\n",
    "            'beta_mktrf_t9': 'Market Beta (MKTRF)',\n",
    "            'beta_smb_t9': 'Size Beta (SMB)',\n",
    "            'beta_hml_t9': 'Value Beta (HML)',\n",
    "            'beta_rmw_t9': 'Profitability Beta (RMW)',\n",
    "            'beta_cma_t9': 'Investment Beta (CMA)',\n",
    "            'beta_umd_t9': 'Momentum Beta (UMD)'\n",
    "        },\n",
    "        \n",
    "        # factor betas (12-month estimation window)  \n",
    "        'Factor Betas (12-month)': {\n",
    "            'beta_mktrf_t12': 'Market Beta (MKTRF)',\n",
    "            'beta_smb_t12': 'Size Beta (SMB)',\n",
    "            'beta_hml_t12': 'Value Beta (HML)',\n",
    "            'beta_rmw_t12': 'Profitability Beta (RMW)', \n",
    "            'beta_cma_t12': 'Investment Beta (CMA)',\n",
    "            'beta_umd_t12': 'Momentum Beta (UMD)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_data = []\n",
    "    for category, variables in regression_variables.items():\n",
    "        print(f\"\\n{category.upper()}\")\n",
    "        category_data = []\n",
    "        for var_code, var_name in variables.items():\n",
    "            if var_code in df_input.columns:\n",
    "                data = df_input[var_code].dropna()\n",
    "                if len(data) > 0:\n",
    "                    stats = {\n",
    "                        'Category': category, 'Variable': var_name, 'Code': var_code,\n",
    "                        'N': len(data), 'Mean': data.mean(), 'Median': data.median(),\n",
    "                        'Std': data.std(), 'Min': data.min(), 'P25': data.quantile(0.25),\n",
    "                        'P75': data.quantile(0.75), 'Max': data.max(), 'Skewness': data.skew(),\n",
    "                        'Kurtosis': data.kurtosis(), 'Missing': len(df_input) - len(data),\n",
    "                        'Missing_Pct': ((len(df_input) - len(data)) / len(df_input)) * 100\n",
    "                    }\n",
    "                    summary_data.append(stats)\n",
    "                    category_data.append(stats)\n",
    "        \n",
    "        if category_data:\n",
    "            cat_df = pd.DataFrame(category_data)\n",
    "            print(f\"{'Variable':<25} {'N':<6} {'Mean':<8} {'Std':<8} {'Min':<8} {'Max':<8}\")\n",
    "            print(\"-\" * 70)\n",
    "            for _, row in cat_df.iterrows():\n",
    "                print(f\"{row['Variable'][:24]:<25} {row['N']:<6,.0f} {row['Mean']:<8.3f} {row['Std']:<8.3f} {row['Min']:<8.3f} {row['Max']:<8.3f}\")\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    if export_to_csv:\n",
    "        try:\n",
    "            summary_df.to_csv(filename, index=False)\n",
    "            print(f\"Exported files:\")\n",
    "            print(f\"Detailed summary: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Export error: {e}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# run the complete analysis\n",
    "summary_df = create_final_regression_summary_table(\n",
    "    df_input=df_final, \n",
    "    export_to_csv=True, \n",
    "    filename=\"final_regression_summary.csv\"\n",
    ")\n",
    "\n",
    "print(\"All summary tables and exports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d193c-8653-41b4-9e62-2466302b62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0cb1c-b164-4485-a9a1-dc4e6067db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_averaged_ai_factors(df_final):\n",
    "    \"\"\"\n",
    "    Create averaged AI factors from the three models\n",
    "    \"\"\"\n",
    "    print(\"Creating averaged AI factors\")\n",
    "    \n",
    "    ai_factor_mapping = {\n",
    "        'AI_Washing': 'AI Washing Index', 'Disclosure_Sentiment': 'Disclosure Sentiment',\n",
    "        'Forward_Looking': 'Forward-Looking', 'Strategic_Depth': 'Strategic Depth',\n",
    "        'Talent_Investment': 'Talent & Investment', 'Risk_External': 'Risk - External Threats',\n",
    "        'Risk_Non_Adoption': 'Risk - Non-Adoption', 'Risk_Own_Adoption': 'Risk - Own Adoption'\n",
    "    }\n",
    "    models = ['flash1_5', 'flash2_5', 'gpt4o']\n",
    "    df_with_averages = df_final.copy()\n",
    "    \n",
    "    for clean_name, original_name in ai_factor_mapping.items():\n",
    "        factor_columns = [f\"{original_name}_{model}_Numeric\" for model in models if f\"{original_name}_{model}_Numeric\" in df_final.columns]\n",
    "        if factor_columns:\n",
    "            df_with_averages[clean_name] = df_final[factor_columns].mean(axis=1)\n",
    "\n",
    "    available_ai_factors = [name for name in ai_factor_mapping.keys() if name in df_with_averages.columns]\n",
    "    if available_ai_factors:\n",
    "        df_with_averages['AI_Cumulative_Score'] = df_with_averages[available_ai_factors].sum(axis=1)\n",
    "        print(\"Averaged AI factors and cumulative score created successfully\")\n",
    "\n",
    "    return df_with_averages\n",
    "\n",
    "def create_final_regression_dataset(df_with_ai):\n",
    "    \"\"\"\n",
    "    Create the final regression dataset, now including t9 betas\n",
    "    \"\"\"\n",
    "    print(\"Creating final regression dataset\")\n",
    "    \n",
    "    essential_columns = [\n",
    "        # identifiers\n",
    "        'CIK', 'Ticker', 'Year', 'gvkey', 'Company Name', 'Sector',\n",
    "        # dependent variables\n",
    "        'excess_return_3mo', 'excess_return_6mo', 'excess_return_9mo', 'excess_return_12mo',\n",
    "        # control variables\n",
    "        'calc_log_market_cap', 'calc_price_to_book', 'calc_roa',\n",
    "        # factor betas t3\n",
    "        'beta_mktrf_t3', 'beta_smb_t3', 'beta_hml_t3', 'beta_rmw_t3', 'beta_cma_t3', 'beta_umd_t3',\n",
    "        # factor betas t6\n",
    "        'beta_mktrf_t6', 'beta_smb_t6', 'beta_hml_t6', 'beta_rmw_t6', 'beta_cma_t6', 'beta_umd_t6',\n",
    "        # factor betas t9\n",
    "        'beta_mktrf_t9', 'beta_smb_t9', 'beta_hml_t9', 'beta_rmw_t9', 'beta_cma_t9', 'beta_umd_t9',\n",
    "        # factor betas t12\n",
    "        'beta_mktrf_t12', 'beta_smb_t12', 'beta_hml_t12', 'beta_rmw_t12', 'beta_cma_t12', 'beta_umd_t12',\n",
    "        # averaged ai factors\n",
    "        'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking', 'Strategic_Depth',\n",
    "        'Talent_Investment', 'Risk_External', 'Risk_Non_Adoption', 'Risk_Own_Adoption',\n",
    "        'AI_Cumulative_Score'\n",
    "    ]\n",
    "    \n",
    "    available_columns = [col for col in essential_columns if col in df_with_ai.columns]\n",
    "    regression_dataset = df_with_ai[available_columns].copy()\n",
    "    \n",
    "    print(f\"Final regression dataset created with shape: {regression_dataset.shape}\")\n",
    "    return regression_dataset\n",
    "\n",
    "print(\"Complete AI factor integration & final data assembly\")\n",
    "print(\"This script takes df_final (after winsorization), adds the averaged AI scores,\")\n",
    "print(\"and selects all necessary columns to create the master final_reg_data\")\n",
    "\n",
    "# step 1: create averaged ai factors from the raw scores in df_final\n",
    "df_with_ai_factors = create_averaged_ai_factors(df_final)\n",
    "\n",
    "# step 2: assemble the final, complete regression dataset\n",
    "final_reg_data = create_final_regression_dataset(df_with_ai_factors)\n",
    "\n",
    "print(\"Master regression dataframe final_reg_data created successfully\")\n",
    "print(\"Can now proceed with the regression and diagnostic analysis cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3f6839-8328-413e-aed3-438063215acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4565c5e-1058-49fd-b2e8-d4fd0f2f9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_portfolio_sorts(df, ai_factors, return_periods=['3mo', '6mo', '9mo', '12mo']):\n",
    "    \"\"\"\n",
    "    create portfolio sorts for ai factors following standard asset pricing methodology\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Univariate portfolio analysis\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for factor in ai_factors:\n",
    "        if factor not in df.columns:\n",
    "            print(f\"Warning: {factor} not found in dataframe\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Analyzing factor: {factor}\")\n",
    "        \n",
    "        factor_results = {}\n",
    "        \n",
    "        # for each return period\n",
    "        for period in return_periods:\n",
    "            return_col = f'excess_return_{period}'\n",
    "            if return_col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            period_results = {}\n",
    "            \n",
    "            # create dataset with complete observations\n",
    "            analysis_data = df[[factor, return_col, 'Year']].dropna()\n",
    "            \n",
    "            if len(analysis_data) < 50:  # minimum observations check\n",
    "                print(f\"   Insufficient data for {period}: {len(analysis_data)} observations\")\n",
    "                continue\n",
    "            \n",
    "            # group by year and create quintiles within each year\n",
    "            yearly_portfolios = []\n",
    "            \n",
    "            for year in analysis_data['Year'].unique():\n",
    "                year_data = analysis_data[analysis_data['Year'] == year].copy()\n",
    "                \n",
    "                if len(year_data) < 10:  # need minimum firms per year\n",
    "                    continue\n",
    "                \n",
    "                # create quintiles based on ai factor - handle ties robustly\n",
    "                try:\n",
    "                    year_data['quintile'] = pd.qcut(year_data[factor], \n",
    "                                                  q=5, \n",
    "                                                  labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'],\n",
    "                                                  duplicates='drop')\n",
    "                except ValueError:\n",
    "                    # if qcut fails due to too many duplicates, use rank-based approach\n",
    "                    year_data['rank'] = year_data[factor].rank(method='first', pct=True)\n",
    "                    year_data['quintile'] = pd.cut(year_data['rank'], \n",
    "                                                 bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                                 labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'],\n",
    "                                                 include_lowest=True)\n",
    "                    year_data = year_data.drop('rank', axis=1)\n",
    "                \n",
    "                yearly_portfolios.append(year_data)\n",
    "            \n",
    "            if not yearly_portfolios:\n",
    "                continue\n",
    "                \n",
    "            # combine all years\n",
    "            all_portfolios = pd.concat(yearly_portfolios, ignore_index=True)\n",
    "            \n",
    "            # calculate portfolio returns\n",
    "            portfolio_returns = all_portfolios.groupby('quintile')[return_col].agg([\n",
    "                'mean', 'std', 'count'\n",
    "            ]).round(4)\n",
    "            \n",
    "            # calculate high-minus-low spread (q5 - q1)\n",
    "            if 'Q5' in portfolio_returns.index and 'Q1' in portfolio_returns.index:\n",
    "                hml_return = portfolio_returns.loc['Q5', 'mean'] - portfolio_returns.loc['Q1', 'mean']\n",
    "                \n",
    "                # get returns for statistical test\n",
    "                q5_returns = all_portfolios[all_portfolios['quintile'] == 'Q5'][return_col]\n",
    "                q1_returns = all_portfolios[all_portfolios['quintile'] == 'Q1'][return_col]\n",
    "                \n",
    "                # perform t-test for hml spread\n",
    "                t_stat, p_value = stats.ttest_ind(q5_returns, q1_returns, equal_var=False)\n",
    "                \n",
    "                period_results = {\n",
    "                    'portfolio_returns': portfolio_returns,\n",
    "                    'hml_spread': hml_return,\n",
    "                    'hml_tstat': t_stat,\n",
    "                    'hml_pvalue': p_value,\n",
    "                    'n_observations': len(all_portfolios)\n",
    "                }\n",
    "                \n",
    "                print(f\"   {period}: hml = {hml_return:.4f} (t={t_stat:.2f}, p={p_value:.3f}), n={len(all_portfolios)}\")\n",
    "            \n",
    "            factor_results[period] = period_results\n",
    "        \n",
    "        results[factor] = factor_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_complete_univariate_analysis(final_reg_data):\n",
    "    \"\"\"\n",
    "    run the complete univariate analysis as specified in the thesis methodology\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Running complete univariate analysis\")\n",
    "    print(\"Following the methodology outlined in section 3.5 of the thesis...\")\n",
    "    \n",
    "    # define ai factors to analyze\n",
    "    ai_factors = [\n",
    "        'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking', 'Strategic_Depth',\n",
    "        'Talent_Investment', 'Risk_External', 'Risk_Non_Adoption', 'Risk_Own_Adoption',\n",
    "        'AI_Cumulative_Score'\n",
    "    ]\n",
    "    \n",
    "    # check data availability\n",
    "    print(f\"Dataset overview:\")\n",
    "    print(f\"   Total observations: {len(final_reg_data)}\")\n",
    "    print(f\"   Years covered: {final_reg_data['Year'].min()} - {final_reg_data['Year'].max()}\")\n",
    "    print(f\"   Unique firms: {final_reg_data['CIK'].nunique()}\")\n",
    "    \n",
    "    # data quality check for ai factors\n",
    "    print(\"AI factors data quality check:\")\n",
    "    for factor in ai_factors:\n",
    "        if factor in final_reg_data.columns:\n",
    "            factor_data = final_reg_data[factor].dropna()\n",
    "            n_unique = factor_data.nunique()\n",
    "            n_total = len(factor_data)\n",
    "            print(f\"   {factor}: {n_total} obs, {n_unique} unique values ({n_unique/n_total:.3f} ratio)\")\n",
    "        else:\n",
    "            print(f\"   {factor}: not found in dataset\")\n",
    "    \n",
    "    # 1. portfolio sorts analysis\n",
    "    portfolio_results = create_portfolio_sorts(final_reg_data, ai_factors)\n",
    "    \n",
    "    print(\"Univariate analysis complete\")\n",
    "    print(\"Next step: proceed with multivariate regression analysis\")\n",
    "    \n",
    "    return portfolio_results\n",
    "\n",
    "# run the analysis\n",
    "portfolio_results = run_complete_univariate_analysis(final_reg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6653b-8473-44c4-b11d-9814f941c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_sector_neutral_sorts(df, ai_factors, return_periods=['12mo'], min_firms_per_tercile=5):\n",
    "    \"\"\"\n",
    "    create sector-neutral portfolio sorts using terciles within each sector\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Sector-neutral portfolio analysis\")\n",
    "    print(\"Using terciles within sectors to control for industry effects\")\n",
    "    \n",
    "    sector_results = {}\n",
    "    \n",
    "    for factor in ai_factors:\n",
    "        if factor not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Analyzing factor: {factor}\")\n",
    "        \n",
    "        factor_results = {}\n",
    "        \n",
    "        for period in return_periods:\n",
    "            return_col = f'excess_return_{period}'\n",
    "            if return_col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # get complete data\n",
    "            analysis_data = df[[factor, return_col, 'Sector', 'Year']].dropna()\n",
    "            \n",
    "            if len(analysis_data) < 50:\n",
    "                continue\n",
    "            \n",
    "            sector_spreads = []\n",
    "            sector_summary = []\n",
    "            \n",
    "            # analyze each sector separately\n",
    "            for sector in analysis_data['Sector'].unique():\n",
    "                sector_data = analysis_data[analysis_data['Sector'] == sector].copy()\n",
    "                \n",
    "                if len(sector_data) < min_firms_per_tercile * 3:  # need minimum firms for terciles\n",
    "                    continue\n",
    "                \n",
    "                # create terciles within sector - handle ties robustly\n",
    "                try:\n",
    "                    sector_data['tercile'] = pd.qcut(sector_data[factor], \n",
    "                                                   q=3, \n",
    "                                                   labels=['Low', 'Mid', 'High'],\n",
    "                                                   duplicates='drop')\n",
    "                except ValueError:\n",
    "                    # handle ties with rank-based approach\n",
    "                    sector_data['rank'] = sector_data[factor].rank(method='first', pct=True)\n",
    "                    sector_data['tercile'] = pd.cut(sector_data['rank'], \n",
    "                                                  bins=[0, 0.33, 0.67, 1.0],\n",
    "                                                  labels=['Low', 'Mid', 'High'],\n",
    "                                                  include_lowest=True)\n",
    "                    sector_data = sector_data.drop('rank', axis=1)\n",
    "                \n",
    "                # calculate tercile returns\n",
    "                tercile_returns = sector_data.groupby('tercile')[return_col].agg([\n",
    "                    'mean', 'std', 'count'\n",
    "                ])\n",
    "                \n",
    "                if 'High' in tercile_returns.index and 'Low' in tercile_returns.index:\n",
    "                    # high-minus-low spread\n",
    "                    hml_spread = tercile_returns.loc['High', 'mean'] - tercile_returns.loc['Low', 'mean']\n",
    "                    \n",
    "                    # get individual returns for t-test\n",
    "                    high_returns = sector_data[sector_data['tercile'] == 'High'][return_col]\n",
    "                    low_returns = sector_data[sector_data['tercile'] == 'Low'][return_col]\n",
    "                    \n",
    "                    # perform t-test\n",
    "                    if len(high_returns) > 1 and len(low_returns) > 1:\n",
    "                        t_stat, p_value = stats.ttest_ind(high_returns, low_returns, equal_var=False)\n",
    "                        \n",
    "                        sector_spreads.append(hml_spread)\n",
    "                        sector_summary.append({\n",
    "                            'Sector': sector,\n",
    "                            'HML_Spread': hml_spread,\n",
    "                            'T_Stat': t_stat,\n",
    "                            'P_Value': p_value,\n",
    "                            'N_Firms': len(sector_data),\n",
    "                            'N_High': len(high_returns),\n",
    "                            'N_Low': len(low_returns)\n",
    "                        })\n",
    "                        \n",
    "                        # add significance stars\n",
    "                        if p_value < 0.01:\n",
    "                            sig_stars = '***'\n",
    "                        elif p_value < 0.05:\n",
    "                            sig_stars = '**'\n",
    "                        elif p_value < 0.10:\n",
    "                            sig_stars = '*'\n",
    "                        else:\n",
    "                            sig_stars = ''\n",
    "                        \n",
    "                        print(f\"   {sector}: {hml_spread:.4f}{sig_stars} (t={t_stat:.2f}, n={len(sector_data)})\")\n",
    "            \n",
    "            # overall sector-neutral analysis\n",
    "            if sector_spreads:\n",
    "                overall_spread = np.mean(sector_spreads)\n",
    "                spread_std = np.std(sector_spreads, ddof=1)\n",
    "                n_sectors = len(sector_spreads)\n",
    "                \n",
    "                # t-test for overall sector-neutral spread\n",
    "                if n_sectors > 1:\n",
    "                    overall_t_stat = overall_spread / (spread_std / np.sqrt(n_sectors))\n",
    "                    overall_p_value = 2 * (1 - stats.t.cdf(abs(overall_t_stat), n_sectors - 1))\n",
    "                    \n",
    "                    print(f\"Sector-neutral overall: {overall_spread:.4f} (t={overall_t_stat:.2f}, p={overall_p_value:.3f})\")\n",
    "                    print(f\"Based on {n_sectors} sectors\")\n",
    "                \n",
    "                factor_results[period] = {\n",
    "                    'sector_summary': pd.DataFrame(sector_summary),\n",
    "                    'overall_spread': overall_spread,\n",
    "                    'overall_t_stat': overall_t_stat,\n",
    "                    'overall_p_value': overall_p_value,\n",
    "                    'n_sectors': n_sectors\n",
    "                }\n",
    "        \n",
    "        sector_results[factor] = factor_results\n",
    "    \n",
    "    return sector_results\n",
    "\n",
    "def create_pooled_sector_neutral_portfolio(df, factor='AI_Cumulative_Score', return_period='12mo'):\n",
    "    \"\"\"\n",
    "    create pooled high/low ai portfolios balanced across sectors\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Pooled sector-neutral analysis: {factor}\")\n",
    "    \n",
    "    return_col = f'excess_return_{return_period}'\n",
    "    analysis_data = df[[factor, return_col, 'Sector', 'Year', 'CIK']].dropna()\n",
    "    \n",
    "    pooled_high = []\n",
    "    pooled_low = []\n",
    "    sector_contributions = []\n",
    "    \n",
    "    for sector in analysis_data['Sector'].unique():\n",
    "        sector_data = analysis_data[analysis_data['Sector'] == sector].copy()\n",
    "        \n",
    "        if len(sector_data) < 9:  # need minimum for terciles\n",
    "            continue\n",
    "        \n",
    "        # create terciles within sector\n",
    "        try:\n",
    "            sector_data['tercile'] = pd.qcut(sector_data[factor], \n",
    "                                           q=3, \n",
    "                                           labels=['Low', 'Mid', 'High'],\n",
    "                                           duplicates='drop')\n",
    "        except ValueError:\n",
    "            # handle ties with rank-based approach\n",
    "            sector_data['rank'] = sector_data[factor].rank(method='first', pct=True)\n",
    "            sector_data['tercile'] = pd.cut(sector_data['rank'], \n",
    "                                          bins=[0, 0.33, 0.67, 1.0],\n",
    "                                          labels=['Low', 'Mid', 'High'],\n",
    "                                          include_lowest=True)\n",
    "            sector_data = sector_data.drop('rank', axis=1)\n",
    "        \n",
    "        # get high and low tercile firms\n",
    "        high_firms = sector_data[sector_data['tercile'] == 'High']\n",
    "        low_firms = sector_data[sector_data['tercile'] == 'Low']\n",
    "        \n",
    "        if len(high_firms) > 0 and len(low_firms) > 0:\n",
    "            pooled_high.extend(high_firms[return_col].tolist())\n",
    "            pooled_low.extend(low_firms[return_col].tolist())\n",
    "            \n",
    "            sector_contributions.append({\n",
    "                'Sector': sector,\n",
    "                'N_High': len(high_firms),\n",
    "                'N_Low': len(low_firms),\n",
    "                'High_Mean': high_firms[return_col].mean(),\n",
    "                'Low_Mean': low_firms[return_col].mean(),\n",
    "                'Sector_Spread': high_firms[return_col].mean() - low_firms[return_col].mean()\n",
    "            })\n",
    "    \n",
    "    # perform pooled t-test\n",
    "    if len(pooled_high) > 0 and len(pooled_low) > 0:\n",
    "        pooled_high = np.array(pooled_high)\n",
    "        pooled_low = np.array(pooled_low)\n",
    "        \n",
    "        hml_spread = pooled_high.mean() - pooled_low.mean()\n",
    "        t_stat, p_value = stats.ttest_ind(pooled_high, pooled_low, equal_var=False)\n",
    "        \n",
    "        print(f\"Pooled portfolio results:\")\n",
    "        print(f\"   High AI portfolio: {pooled_high.mean():.4f} ({len(pooled_high)} firms)\")\n",
    "        print(f\"   Low AI portfolio:  {pooled_low.mean():.4f} ({len(pooled_low)} firms)\")\n",
    "        print(f\"   High-minus-low:    {hml_spread:.4f} (t={t_stat:.2f}, p={p_value:.3f})\")\n",
    "        \n",
    "        # significance stars\n",
    "        if p_value < 0.01:\n",
    "            sig_stars = '***'\n",
    "        elif p_value < 0.05:\n",
    "            sig_stars = '**'\n",
    "        elif p_value < 0.10:\n",
    "            sig_stars = '*'\n",
    "        else:\n",
    "            sig_stars = ''\n",
    "        \n",
    "        print(f\"   Final result: {hml_spread:.4f}{sig_stars}\")\n",
    "        \n",
    "        # show sector contributions\n",
    "        contrib_df = pd.DataFrame(sector_contributions)\n",
    "        print(f\"Sector contributions:\")\n",
    "        print(contrib_df.round(4))\n",
    "        \n",
    "        return {\n",
    "            'hml_spread': hml_spread,\n",
    "            't_stat': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'n_high': len(pooled_high),\n",
    "            'n_low': len(pooled_low),\n",
    "            'high_mean': pooled_high.mean(),\n",
    "            'low_mean': pooled_low.mean(),\n",
    "            'sector_contributions': contrib_df\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_sector_summary_table(sector_results, focus_factor='AI_Cumulative_Score', period='12mo'):\n",
    "    \"\"\"\n",
    "    create summary table matching table 12 in the thesis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Sector summary table - {focus_factor}\")\n",
    "    \n",
    "    if focus_factor not in sector_results or period not in sector_results[focus_factor]:\n",
    "        print(\"Data not available for summary table\")\n",
    "        return None\n",
    "    \n",
    "    sector_data = sector_results[focus_factor][period]['sector_summary']\n",
    "    \n",
    "    # format the summary table\n",
    "    summary_table = sector_data.copy()\n",
    "    summary_table['HML_Spread_Pct'] = summary_table['HML_Spread'] * 100  # convert to percentage\n",
    "    \n",
    "    # add significance stars\n",
    "    def add_stars(row):\n",
    "        if row['P_Value'] < 0.01:\n",
    "            return f\"{row['HML_Spread_Pct']:.2f}***\"\n",
    "        elif row['P_Value'] < 0.05:\n",
    "            return f\"{row['HML_Spread_Pct']:.2f}**\"\n",
    "        elif row['P_Value'] < 0.10:\n",
    "            return f\"{row['HML_Spread_Pct']:.2f}*\"\n",
    "        else:\n",
    "            return f\"{row['HML_Spread_Pct']:.2f}\"\n",
    "    \n",
    "    summary_table['Spread_Display'] = summary_table.apply(add_stars, axis=1)\n",
    "    \n",
    "    # create final display table\n",
    "    final_table = summary_table[['Sector', 'Spread_Display', 'T_Stat', 'P_Value', 'N_Firms']].copy()\n",
    "    final_table.columns = ['Sector', 'High-Low Spread (%)', 'T-Stat', 'P-Value', 'N Firms']\n",
    "    final_table = final_table.round({'T-Stat': 2, 'P-Value': 3})\n",
    "    \n",
    "    print(final_table.to_string(index=False))\n",
    "    \n",
    "    return final_table\n",
    "\n",
    "def plot_sector_results(sector_results, focus_factor='AI_Cumulative_Score', period='12mo'):\n",
    "    \"\"\"\n",
    "    visualize sector-specific results\n",
    "    \"\"\"\n",
    "    \n",
    "    if focus_factor not in sector_results or period not in sector_results[focus_factor]:\n",
    "        return\n",
    "    \n",
    "    sector_data = sector_results[focus_factor][period]['sector_summary']\n",
    "    \n",
    "    # create bar plot of sector spreads\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = ['red' if x < 0 else 'blue' for x in sector_data['HML_Spread']]\n",
    "    bars = plt.bar(range(len(sector_data)), sector_data['HML_Spread'] * 100, color=colors, alpha=0.7)\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    plt.xlabel('Sector')\n",
    "    plt.ylabel('High-minus-low spread (%)')\n",
    "    plt.title(f'Sector-specific AI factor effects ({focus_factor})')\n",
    "    plt.xticks(range(len(sector_data)), sector_data['Sector'], rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # add significance indicators\n",
    "    for i, (_, row) in enumerate(sector_data.iterrows()):\n",
    "        if row['P_Value'] < 0.01:\n",
    "            plt.text(i, row['HML_Spread'] * 100 + 0.5, '***', ha='center', fontsize=12)\n",
    "        elif row['P_Value'] < 0.05:\n",
    "            plt.text(i, row['HML_Spread'] * 100 + 0.5, '**', ha='center', fontsize=12)\n",
    "        elif row['P_Value'] < 0.10:\n",
    "            plt.text(i, row['HML_Spread'] * 100 + 0.5, '*', ha='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def run_complete_sector_analysis(final_reg_data):\n",
    "    \"\"\"\n",
    "    run the complete sector-specific analysis as outlined in section 4.1.2\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Running complete sector-specific analysis\")\n",
    "    print(\"Following methodology from section 4.1.2: controlling for sectors\")\n",
    "    \n",
    "    # check sector distribution\n",
    "    print(f\"Sector distribution:\")\n",
    "    sector_counts = final_reg_data['Sector'].value_counts()\n",
    "    print(sector_counts)\n",
    "    \n",
    "    # define key ai factors for sector analysis\n",
    "    ai_factors = ['AI_Cumulative_Score']  # start with main factor\n",
    "    \n",
    "    # 1. sector-neutral sorts for all factors\n",
    "    sector_results = create_sector_neutral_sorts(final_reg_data, ai_factors)\n",
    "    \n",
    "    # 2. pooled sector-neutral portfolio analysis (main test)\n",
    "    pooled_results = create_pooled_sector_neutral_portfolio(final_reg_data, \n",
    "                                                           factor='AI_Cumulative_Score',\n",
    "                                                           return_period='12mo')\n",
    "    \n",
    "    # 3. create summary table (table 12 equivalent)\n",
    "    summary_table = create_sector_summary_table(sector_results, \n",
    "                                               focus_factor='AI_Cumulative_Score', \n",
    "                                               period='12mo')\n",
    "    \n",
    "    # 4. visualization\n",
    "    plot_sector_results(sector_results, focus_factor='AI_Cumulative_Score', period='12mo')\n",
    "    \n",
    "    print(\"Sector-specific analysis complete!\")\n",
    "    print(\"   Key finding: effects persist even within sectors (sector-neutral)\")\n",
    "    \n",
    "    return sector_results, pooled_results, summary_table\n",
    "\n",
    "# run the analysis\n",
    "sector_results, pooled_results, summary_table = run_complete_sector_analysis(final_reg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85117ae0-5707-430e-a145-3aff84cc3710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_academic_regression_table(reg_data, ai_factor, dependent_var, sector_col, beta_horizon='t12'):\n",
    "    \"\"\"\n",
    "    run regression analysis and return results in academic table format\n",
    "    \"\"\"\n",
    "    \n",
    "    factor_models = {\n",
    "        'Base': [],\n",
    "        'FF3': [f'beta_mktrf_{beta_horizon}', f'beta_smb_{beta_horizon}', f'beta_hml_{beta_horizon}'],\n",
    "        'FF5': [f'beta_mktrf_{beta_horizon}', f'beta_smb_{beta_horizon}', f'beta_hml_{beta_horizon}', \n",
    "                f'beta_rmw_{beta_horizon}', f'beta_cma_{beta_horizon}'],\n",
    "        'FF6': [f'beta_mktrf_{beta_horizon}', f'beta_smb_{beta_horizon}', f'beta_hml_{beta_horizon}', \n",
    "                f'beta_rmw_{beta_horizon}', f'beta_cma_{beta_horizon}', f'beta_umd_{beta_horizon}']\n",
    "    }\n",
    "    \n",
    "    base_formula = f\"{dependent_var} ~ {ai_factor}\"\n",
    "    if sector_col and sector_col in reg_data.columns:\n",
    "        base_formula += f\" + C({sector_col})\"\n",
    "    if 'Year' in reg_data.columns:\n",
    "        base_formula += f\" + C(Year)\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, factors in factor_models.items():\n",
    "        available_factors = [f for f in factors if f in reg_data.columns]\n",
    "        \n",
    "        formula = base_formula\n",
    "        if available_factors:\n",
    "            formula += \" + \" + \" + \".join(available_factors)\n",
    "        \n",
    "        try:\n",
    "            model = smf.ols(formula, data=reg_data).fit()\n",
    "            \n",
    "            ai_coef = model.params.get(ai_factor, np.nan)\n",
    "            ai_stderr = model.bse.get(ai_factor, np.nan)\n",
    "            ai_tstat = model.tvalues.get(ai_factor, np.nan)\n",
    "            ai_pvalue = model.pvalues.get(ai_factor, np.nan)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'model': model, 'ai_coefficient': ai_coef, 'ai_stderr': ai_stderr,\n",
    "                'ai_tstat': ai_tstat, 'ai_pvalue': ai_pvalue,\n",
    "                'r_squared': model.rsquared, 'adj_r_squared': model.rsquared_adj,\n",
    "                'n_obs': model.nobs, 'available_factors': available_factors\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error running regression for model '{model_name}'. Formula: '{formula}'. Error: {e}\")\n",
    "            results[model_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_all_academic_tables(reg_data, ai_factors, export_tables=True):\n",
    "    \"\"\"\n",
    "    main orchestrator function. runs regression analysis for all ai factors\n",
    "    and exports the results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating academic regression tables\")\n",
    "    print(\"Using pre-computed AI factors and cumulative score from the master dataset\")\n",
    "    \n",
    "    all_ai_factors_to_test = ai_factors + ['AI_Cumulative_Score']\n",
    "    \n",
    "    horizons = {\n",
    "        't3': 'excess_return_3mo', 't6': 'excess_return_6mo',\n",
    "        't9': 'excess_return_9mo', 't12': 'excess_return_12mo'\n",
    "    }\n",
    "    \n",
    "    all_tables = {}\n",
    "    \n",
    "    for ai_factor in all_ai_factors_to_test:\n",
    "        if ai_factor not in reg_data.columns:\n",
    "            print(f\"'{ai_factor}' not found, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Creating tables for: {ai_factor.replace('_', ' ').title()}\")\n",
    "        factor_tables = {}\n",
    "        \n",
    "        for horizon, dependent_var in horizons.items():\n",
    "            if dependent_var not in reg_data.columns:\n",
    "                print(f\"  Dependent var '{dependent_var}' not found, skipping {horizon}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Running for {horizon} horizon ({dependent_var})\")\n",
    "            \n",
    "            results = run_academic_regression_table(\n",
    "                reg_data, ai_factor, dependent_var, 'Sector', horizon\n",
    "            )\n",
    "            \n",
    "            factor_tables[horizon] = {\n",
    "                'results': results,\n",
    "                'dependent_var': dependent_var\n",
    "            }\n",
    "            \n",
    "        all_tables[ai_factor] = factor_tables\n",
    "    \n",
    "    return all_tables, reg_data\n",
    "\n",
    "# 1. define the list of individual, averaged ai factors\n",
    "ai_factors = [\n",
    "    'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking',\n",
    "    'Strategic_Depth', 'Talent_Investment', 'Risk_External',\n",
    "    'Risk_Non_Adoption', 'Risk_Own_Adoption'\n",
    "]\n",
    "\n",
    "# 2. run the complete analysis using the master dataframe\n",
    "all_tables, final_data_for_reg = run_all_academic_tables(final_reg_data, ai_factors)\n",
    "\n",
    "print(\"Academic regression analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f38592-aa27-45a1-9f7d-e58694919ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# diagnostic functions\n",
    "# This section defines all the necessary tools for the analysis.\n",
    "\n",
    "def calculate_vif_individual_regressions(reg_data, ai_factors, ff_factors, horizon='t12'):\n",
    "    \"\"\"\n",
    "    Calculate VIF for individual AI factor regressions.\n",
    "    \"\"\"\n",
    "    print(f\"VIF analysis for individual AI factor regressions (horizon: {horizon.upper()})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    available_ff_factors = [f for f in ff_factors if f in reg_data.columns]\n",
    "    available_ai_factors = [f for f in ai_factors if f in reg_data.columns]\n",
    "    \n",
    "    all_vif_results = []\n",
    "    \n",
    "    for ai_factor in available_ai_factors:\n",
    "        factors_for_this_regression = [ai_factor] + available_ff_factors\n",
    "        vif_data = reg_data[factors_for_this_regression].dropna()\n",
    "        \n",
    "        if len(vif_data) < 50:\n",
    "            print(f\"  Skipping VIF for {ai_factor} due to insufficient data ({len(vif_data)} obs).\")\n",
    "            continue\n",
    "        \n",
    "        for i, factor in enumerate(factors_for_this_regression):\n",
    "            try:\n",
    "                vif_value = variance_inflation_factor(vif_data.values, i)\n",
    "                all_vif_results.append({\n",
    "                    'AI_Factor_In_Model': ai_factor, 'Variable': factor,\n",
    "                    'VIF': vif_value, 'Concern': 'High' if vif_value > 10 else 'Moderate' if vif_value > 5 else 'Low'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"  Error calculating VIF for {factor}: {e}\")\n",
    "        \n",
    "    vif_df = pd.DataFrame(all_vif_results)\n",
    "    \n",
    "    if not vif_df.empty:\n",
    "        max_vif = vif_df['VIF'].max()\n",
    "        print(f\"VIF analysis complete for {horizon.upper()}. Max VIF observed: {max_vif:.2f}\")\n",
    "        if max_vif > 10:\n",
    "            print(\"  High multicollinearity detected in at least one specification.\")\n",
    "        else:\n",
    "            print(\"  No concerning multicollinearity in any individual regression specification.\")\n",
    "            \n",
    "    return vif_df\n",
    "\n",
    "def run_diagnostic_tests(model_results, test_name=\"\"):\n",
    "    \"\"\"\n",
    "    Run a suite of diagnostic tests on a fitted regression model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = model_results['model']\n",
    "        residuals = model.resid\n",
    "        \n",
    "        bp_stat, bp_pvalue, _, _ = het_breuschpagan(residuals, model.model.exog)\n",
    "        white_stat, white_pvalue, _, _ = het_white(residuals, model.model.exog)\n",
    "        dw_stat = durbin_watson(residuals)\n",
    "        jb_stat, jb_pvalue = stats.jarque_bera(residuals)\n",
    "        \n",
    "        return {\n",
    "            'Test_Name': test_name, 'BP_pvalue': bp_pvalue, 'White_pvalue': white_pvalue,\n",
    "            'Durbin_Watson': dw_stat, 'JB_pvalue': jb_pvalue\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in diagnostic tests for '{test_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "def run_comprehensive_diagnostics(reg_data, ai_factors, horizon, dependent_var):\n",
    "    \"\"\"\n",
    "    Run a full suite of diagnostics for a specific regression setup.\n",
    "    \"\"\"\n",
    "    print(f\"\\nRunning comprehensive diagnostics (horizon: {horizon.upper()})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ff_factors = [f'beta_mktrf_{horizon}', f'beta_smb_{horizon}', f'beta_hml_{horizon}',\n",
    "                  f'beta_rmw_{horizon}', f'beta_cma_{horizon}', f'beta_umd_{horizon}']\n",
    "    \n",
    "    vif_results = calculate_vif_individual_regressions(reg_data, ai_factors, ff_factors, horizon)\n",
    "    \n",
    "    print(\"\\nRunning diagnostics on AI cumulative score model...\")\n",
    "    all_diagnostics = []\n",
    "    if 'AI_Cumulative_Score' in reg_data.columns:\n",
    "        base_formula = f\"{dependent_var} ~ AI_Cumulative_Score + C(Sector) + C(Year)\"\n",
    "        available_ff = [f for f in ff_factors if f in reg_data.columns]\n",
    "        \n",
    "        ff6_formula = base_formula\n",
    "        if available_ff:\n",
    "            ff6_formula += \" + \" + \" + \".join(available_ff)\n",
    "        \n",
    "        try:\n",
    "            base_model = smf.ols(base_formula, data=reg_data).fit()\n",
    "            all_diagnostics.append(run_diagnostic_tests({'model': base_model}, \"Cumulative Score - Base Model\"))\n",
    "            \n",
    "            if available_ff:\n",
    "                ff6_model = smf.ols(ff6_formula, data=reg_data).fit()\n",
    "                all_diagnostics.append(run_diagnostic_tests({'model': ff6_model}, \"Cumulative Score - FF6 Model\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error running diagnostic models for {horizon.upper()}: {e}\")\n",
    "            \n",
    "    recommendations = []\n",
    "    if not vif_results.empty and vif_results['VIF'].max() > 10:\n",
    "        recommendations.append(\"VIF > 10 detected. Check VIF report.\")\n",
    "    else:\n",
    "        recommendations.append(\"Multicollinearity is not a concern in individual regressions.\")\n",
    "\n",
    "    # Check for heteroskedasticity in the first valid diagnostic result\n",
    "    het_detected = False\n",
    "    for diag_result in all_diagnostics:\n",
    "        if diag_result and diag_result.get('BP_pvalue', 1) < 0.05:\n",
    "            het_detected = True\n",
    "            break\n",
    "            \n",
    "    if het_detected:\n",
    "        recommendations.append(\"Heteroskedasticity detected (p < 0.05). Recommendation: use robust standard errors, e.g., .fit(cov_type='HC3').\")\n",
    "    else:\n",
    "        recommendations.append(\"No significant heteroskedasticity detected.\")\n",
    "        \n",
    "    print(\"\\nDiagnostic summary & recommendations\")\n",
    "    print(\"-\" * 50)\n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "        \n",
    "    return {'vif_results': vif_results, 'model_diagnostics': [d for d in all_diagnostics if d is not None]}\n",
    "\n",
    "def export_diagnostic_results(diagnostic_results, horizon):\n",
    "    \"\"\"\n",
    "    Exports comprehensive results for all tests.\n",
    "    \"\"\"\n",
    "    print(f\"\\nExporting diagnostic results for {horizon.upper()}\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    try:\n",
    "        if diagnostic_results.get('vif_results') is not None and not diagnostic_results['vif_results'].empty:\n",
    "            vif_file = f\"diagnostics_vif_analysis_{horizon}.csv\"\n",
    "            diagnostic_results['vif_results'].to_csv(vif_file, index=False)\n",
    "            print(f\"VIF results exported to: {vif_file}\")\n",
    "            \n",
    "        if diagnostic_results.get('model_diagnostics'):\n",
    "            diag_df = pd.DataFrame(diagnostic_results['model_diagnostics'])\n",
    "            diag_file = f\"diagnostics_model_summary_{horizon}.csv\"\n",
    "            diag_df.to_csv(diag_file, index=False)\n",
    "            print(f\"Model diagnostics (BP, White, etc.) exported to: {diag_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during export for {horizon.upper()}: {e}\")\n",
    "\n",
    "# 1. Define AI factors\n",
    "ai_factors = [\n",
    "    'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking',\n",
    "    'Strategic_Depth', 'Talent_Investment', 'Risk_External',\n",
    "    'Risk_Non_Adoption', 'Risk_Own_Adoption'\n",
    "]\n",
    "\n",
    "# 2. Define the horizons to test\n",
    "horizons_to_test = {\n",
    "    't3': 'excess_return_3mo', 't6': 'excess_return_6mo',\n",
    "    't9': 'excess_return_9mo', 't12': 'excess_return_12mo'\n",
    "}\n",
    "\n",
    "# 3. Loop through, run diagnostics, and export for each horizon\n",
    "all_diagnostics_results = {}\n",
    "for horizon, dependent_var in horizons_to_test.items():\n",
    "    if dependent_var in final_reg_data.columns:\n",
    "        diagnostics = run_comprehensive_diagnostics(\n",
    "            final_reg_data, ai_factors, horizon, dependent_var\n",
    "        )\n",
    "        export_diagnostic_results(diagnostics, horizon)\n",
    "        all_diagnostics_results[horizon] = diagnostics\n",
    "    else:\n",
    "        print(f\"\\nSkipping horizon {horizon.upper()} because dependent variable '{dependent_var}' is not in the data.\")\n",
    "\n",
    "print(\"\\n\\nAll diagnostic tests and exports complete for all horizons!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc3925-1dc5-4609-8bed-b77ce935beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_academic_regression_table(reg_data, ai_factor, dependent_var, sector_col, beta_horizon='t12', cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    run regression using firm-clustered standard errors\n",
    "    manually handles missing data before fitting to prevent errors\n",
    "    \"\"\"\n",
    "    \n",
    "    factor_models = {\n",
    "        'Base': [],\n",
    "        'FF3': [f'beta_mktrf_{beta_horizon}', f'beta_smb_{beta_horizon}', f'beta_hml_{beta_horizon}'],\n",
    "        'FF5': [f'beta_mktrf_{beta_horizon}', f'beta_smb_{beta_horizon}', f'beta_hml_{beta_horizon}', \n",
    "                f'beta_rmw_{beta_horizon}', f'beta_cma_{beta_horizon}'],\n",
    "        'FF6': [f'beta_mktrf_{beta_horizon}', f'beta_smb_{beta_horizon}', f'beta_hml_{beta_horizon}', \n",
    "                f'beta_rmw_{beta_horizon}', f'beta_cma_{beta_horizon}', f'beta_umd_{beta_horizon}']\n",
    "    }\n",
    "    \n",
    "    base_formula = f\"{dependent_var} ~ {ai_factor}\"\n",
    "    if sector_col and sector_col in reg_data.columns:\n",
    "        base_formula += f\" + C({sector_col})\"\n",
    "    if 'Year' in reg_data.columns:\n",
    "        base_formula += f\" + C(Year)\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if not cluster_col or cluster_col not in reg_data.columns:\n",
    "        print(f\"Error: cluster column '{cluster_col}' not found. Aborting\")\n",
    "        return None\n",
    "\n",
    "    for model_name, factors in factor_models.items():\n",
    "        formula = base_formula\n",
    "        available_factors = [f for f in factors if f in reg_data.columns]\n",
    "        if available_factors:\n",
    "            formula += \" + \" + \" + \".join(available_factors)\n",
    "\n",
    "        # manually prepare the data to handle missing values before fitting\n",
    "        # 1. identify all variables needed for this specific regression\n",
    "        all_vars_in_formula = [dependent_var, ai_factor, sector_col, 'Year', cluster_col] + available_factors\n",
    "        \n",
    "        # 2. create a clean subset of the data with no missing values for these variables\n",
    "        clean_data = reg_data[all_vars_in_formula].dropna()\n",
    "\n",
    "        if len(clean_data) < 20: # check if there's enough data to run a regression\n",
    "            print(f\"Skipping model '{model_name}' due to insufficient data after dropping NaNs ({len(clean_data)} obs)\")\n",
    "            results[model_name] = None\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # 3. run the model on the clean data and provide the perfectly aligned cluster groups\n",
    "            model = smf.ols(formula, data=clean_data).fit(\n",
    "                cov_type='cluster', \n",
    "                cov_kwds={'groups': clean_data[cluster_col]}\n",
    "            )\n",
    "            \n",
    "            ai_coef = model.params.get(ai_factor, np.nan)\n",
    "            ai_stderr = model.bse.get(ai_factor, np.nan)\n",
    "            ai_tstat = model.tvalues.get(ai_factor, np.nan)\n",
    "            ai_pvalue = model.pvalues.get(ai_factor, np.nan)\n",
    "            \n",
    "            factor_stats = {}\n",
    "            for factor in available_factors:\n",
    "                factor_stats[factor] = {\n",
    "                    'coef': model.params.get(factor, np.nan),\n",
    "                    'stderr': model.bse.get(factor, np.nan),\n",
    "                    'tstat': model.tvalues.get(factor, np.nan),\n",
    "                    'pvalue': model.pvalues.get(factor, np.nan)\n",
    "                }\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'model': model, 'ai_coefficient': ai_coef, 'ai_stderr': ai_stderr,\n",
    "                'ai_tstat': ai_tstat, 'ai_pvalue': ai_pvalue, 'factor_stats': factor_stats,\n",
    "                'r_squared': model.rsquared, 'adj_r_squared': model.rsquared_adj,\n",
    "                'n_obs': model.nobs, 'available_factors': available_factors\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error running regression for model '{model_name}'. Formula: '{formula}'. Error: {e}\")\n",
    "            results[model_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def format_coefficient_academic(coef, stderr, pvalue):\n",
    "    \"\"\"\n",
    "    format a coefficient and its standard error, adding significance stars\n",
    "    \"\"\"\n",
    "    if pd.isna(coef) or pd.isna(stderr):\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    if pvalue < 0.01: stars = \"***\"\n",
    "    elif pvalue < 0.05: stars = \"**\"\n",
    "    elif pvalue < 0.1: stars = \"*\"\n",
    "    else: stars = \"\"\n",
    "    \n",
    "    coef_str = f\"{coef:.4f}{stars}\"\n",
    "    stderr_str = f\"({stderr:.4f})\"\n",
    "    \n",
    "    return coef_str, stderr_str\n",
    "\n",
    "def create_academic_table_single_factor(reg_data, ai_factor, beta_horizon, dependent_var, sector_col='Sector', cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    assemble the full academic-style regression table for one ai factor\n",
    "    \"\"\"\n",
    "    results = run_academic_regression_table(reg_data, ai_factor, dependent_var, sector_col, beta_horizon, cluster_col)\n",
    "    \n",
    "    if results is None:\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "    variable_order = [\n",
    "        ai_factor, f'beta_mktrf_{beta_horizon}', f'beta_smb_{beta_horizon}', f'beta_hml_{beta_horizon}',\n",
    "        f'beta_rmw_{beta_horizon}', f'beta_cma_{beta_horizon}', f'beta_umd_{beta_horizon}'\n",
    "    ]\n",
    "    \n",
    "    variable_names = {\n",
    "        ai_factor: ai_factor.replace('_', ' ').title(),\n",
    "        f'beta_mktrf_{beta_horizon}': 'Market Beta', f'beta_smb_{beta_horizon}': 'Size Beta',\n",
    "        f'beta_hml_{beta_horizon}': 'Value Beta', f'beta_rmw_{beta_horizon}': 'Profitability Beta',\n",
    "        f'beta_cma_{beta_horizon}': 'Investment Beta', f'beta_umd_{beta_horizon}': 'Momentum Beta'\n",
    "    }\n",
    "    \n",
    "    table_data = []\n",
    "    for var in variable_order:\n",
    "        row_data = {'Variable': variable_names.get(var, var)}\n",
    "        for model_name in ['Base', 'FF3', 'FF5', 'FF6']:\n",
    "            if results.get(model_name) is None:\n",
    "                row_data[f'{model_name}_coef'], row_data[f'{model_name}_stderr'] = \"\", \"\"\n",
    "                continue\n",
    "                \n",
    "            if var == ai_factor:\n",
    "                coef, stderr, pvalue = results[model_name]['ai_coefficient'], results[model_name]['ai_stderr'], results[model_name]['ai_pvalue']\n",
    "            else:\n",
    "                factor_stats = results[model_name]['factor_stats']\n",
    "                if var in factor_stats:\n",
    "                    coef, stderr, pvalue = factor_stats[var]['coef'], factor_stats[var]['stderr'], factor_stats[var]['pvalue']\n",
    "                else:\n",
    "                    coef = stderr = pvalue = np.nan\n",
    "            \n",
    "            coef_str, stderr_str = format_coefficient_academic(coef, stderr, pvalue)\n",
    "            row_data[f'{model_name}_coef'] = coef_str\n",
    "            row_data[f'{model_name}_stderr'] = stderr_str\n",
    "            \n",
    "        table_data.append(row_data)\n",
    "    \n",
    "    summary_rows = []\n",
    "    for stat_name in ['R', 'Adj. R', 'Observations']:\n",
    "        row = {'Variable': stat_name}\n",
    "        for model_name in ['Base', 'FF3', 'FF5', 'FF6']:\n",
    "            if results.get(model_name) is not None:\n",
    "                if stat_name == 'R': value = f\"{results[model_name]['r_squared']:.4f}\"\n",
    "                elif stat_name == 'Adj. R': value = f\"{results[model_name]['adj_r_squared']:.4f}\"\n",
    "                else: value = f\"{int(results[model_name]['n_obs']):,}\"\n",
    "                row[f'{model_name}_coef'] = value\n",
    "                row[f'{model_name}_stderr'] = \"\"\n",
    "            else:\n",
    "                row[f'{model_name}_coef'], row[f'{model_name}_stderr'] = \"\", \"\"\n",
    "        summary_rows.append(row)\n",
    "        \n",
    "    return pd.DataFrame(table_data + summary_rows), results\n",
    "\n",
    "def run_all_academic_tables(reg_data, ai_factors, export_tables=True, cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    main orchestrator function. runs regression analysis and exports the results\n",
    "    using firm-clustered standard errors\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating academic regression tables with firm-clustered standard errors\")\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"Clustering standard errors by firm identifier: '{cluster_col}'\")\n",
    "    \n",
    "    all_ai_factors_to_test = ai_factors + ['AI_Cumulative_Score']\n",
    "    \n",
    "    horizons = {\n",
    "        't3': 'excess_return_3mo', 't6': 'excess_return_6mo',\n",
    "        't9': 'excess_return_9mo', 't12': 'excess_return_12mo'\n",
    "    }\n",
    "    \n",
    "    all_tables = {}\n",
    "    \n",
    "    for ai_factor in all_ai_factors_to_test:\n",
    "        if ai_factor not in reg_data.columns:\n",
    "            print(f\"'{ai_factor}' not found, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nCreating tables for: {ai_factor.replace('_', ' ').title()}\")\n",
    "        factor_tables = {}\n",
    "        \n",
    "        for horizon, dependent_var in horizons.items():\n",
    "            if dependent_var not in reg_data.columns:\n",
    "                print(f\"  Dependent var '{dependent_var}' not found, skipping {horizon}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Running for {horizon} horizon ({dependent_var})\")\n",
    "            \n",
    "            table_df, results = create_academic_table_single_factor(\n",
    "                reg_data, ai_factor, horizon, dependent_var, cluster_col=cluster_col\n",
    "            )\n",
    "            \n",
    "            if results: \n",
    "                factor_tables[horizon] = {\n",
    "                    'table_df': table_df, 'results': results, 'dependent_var': dependent_var\n",
    "                }\n",
    "            \n",
    "        all_tables[ai_factor] = factor_tables\n",
    "    \n",
    "    if export_tables:\n",
    "        export_academic_tables(all_tables)\n",
    "    \n",
    "    return all_tables, reg_data\n",
    "\n",
    "def export_academic_tables(all_tables):\n",
    "    \"\"\"\n",
    "    exports regression results into two formats: a detailed summary and individual tables\n",
    "    \"\"\"\n",
    "    print(f\"\\nExporting regression results\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    print(\"1. Exporting detailed summary of all models...\")\n",
    "    summary_data = []\n",
    "    for ai_factor, factor_tables in all_tables.items():\n",
    "        for horizon, table_info in factor_tables.items():\n",
    "            for model_name, result in table_info['results'].items():\n",
    "                if result:\n",
    "                    summary_data.append({\n",
    "                        'AI_Factor': ai_factor, 'Horizon': horizon,\n",
    "                        'Dependent_Variable': table_info['dependent_var'], 'Model': model_name,\n",
    "                        'AI_Coefficient': result['ai_coefficient'], 'AI_Std_Error': result['ai_stderr'],\n",
    "                        'AI_t_stat': result['ai_tstat'], 'AI_p_value': result['ai_pvalue'],\n",
    "                        'R_Squared': result['r_squared'], 'Adj_R_Squared': result['adj_r_squared'],\n",
    "                        'N_Obs': result['n_obs']\n",
    "                    })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv('academic_regression_summary_all_results_CLUSTERED.csv', index=False)\n",
    "    print(f\"Detailed summary exported to: academic_regression_summary_all_results_CLUSTERED.csv\")\n",
    "\n",
    "    print(\"\\n2. Exporting individual, publication-ready academic tables...\")\n",
    "    exported_count = 0\n",
    "    for ai_factor, factor_tables in all_tables.items():\n",
    "        for horizon, table_info in factor_tables.items():\n",
    "            table_df = table_info['table_df']\n",
    "            \n",
    "            output_rows = [['Variable', 'Base Model', 'FF3 Model', 'FF5 Model', 'FF6 Model']]\n",
    "            for _, row in table_df.iterrows():\n",
    "                output_rows.append([\n",
    "                    row['Variable'], row['Base_coef'], row['FF3_coef'],\n",
    "                    row['FF5_coef'], row['FF6_coef']\n",
    "                ])\n",
    "                if any(row[f'{m}_stderr'] for m in ['Base', 'FF3', 'FF5', 'FF6']):\n",
    "                    output_rows.append([\n",
    "                        '', row['Base_stderr'], row['FF3_stderr'],\n",
    "                        row['FF5_stderr'], row['FF6_stderr']\n",
    "                    ])\n",
    "\n",
    "            export_df = pd.DataFrame(output_rows[1:], columns=output_rows[0])\n",
    "            export_df.to_csv(f\"table_{ai_factor}_{horizon}_CLUSTERED.csv\", index=False)\n",
    "            exported_count += 1\n",
    "            \n",
    "    print(f\"All {exported_count} individual academic tables exported successfully\")\n",
    "\n",
    "\n",
    "def display_sample_table(all_tables, ai_factor='AI_Cumulative_Score', horizon='t12'):\n",
    "    \"\"\"\n",
    "    display a single, formatted sample table in the console for a quick check\n",
    "    \"\"\"\n",
    "    if ai_factor in all_tables and horizon in all_tables[ai_factor]:\n",
    "        table_info = all_tables[ai_factor][horizon]\n",
    "        table_df = table_info['table_df']\n",
    "        \n",
    "        print(f\"\\n\\nSample table (with firm-clustered SE): {ai_factor.replace('_', ' ').title()} ({horizon})\")\n",
    "        print(f\"Dependent variable: {table_info['dependent_var']}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Variable':<20} {'Base':<15} {'FF3':<15} {'FF5':<15} {'FF6':<15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for _, row in table_df.iterrows():\n",
    "            var_name = row['Variable'][:19]\n",
    "            print(f\"{var_name:<20} {row['Base_coef']:<15} {row['FF3_coef']:<15} {row['FF5_coef']:<15} {row['FF6_coef']:<15}\")\n",
    "            if any(row[f'{m}_stderr'] for m in ['Base', 'FF3', 'FF5', 'FF6']):\n",
    "                print(f\"{'':<20} {row['Base_stderr']:<15} {row['FF3_stderr']:<15} {row['FF5_stderr']:<15} {row['FF6_stderr']:<15}\")\n",
    "        \n",
    "        print(\"\\nNotes: firm-clustered standard errors (by ticker) in parentheses. *p<0.1, **p<0.05, ***p<0.01\")\n",
    "\n",
    "print(\"Academic regression table generator (firm-clustered SE version)\")\n",
    "print(\"=\" * 65)\n",
    "print(\"This is the final step. It takes the master 'final_reg_data' and runs all regressions\")\n",
    "print(\"It now uses firm-clustered standard errors for valid inference in panel data\")\n",
    "\n",
    "# 1. define the list of individual, averaged ai factors\n",
    "ai_factors = [\n",
    "    'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking',\n",
    "    'Strategic_Depth', 'Talent_Investment', 'Risk_External',\n",
    "    'Risk_Non_Adoption', 'Risk_Own_Adoption'\n",
    "]\n",
    "\n",
    "# 2. run the complete analysis using the master dataframe ('final_reg_data')\n",
    "all_tables, final_data_for_reg = run_all_academic_tables(final_reg_data, ai_factors, cluster_col='Ticker')\n",
    "\n",
    "# 3. display a sample table to check the results for the main hypothesis\n",
    "display_sample_table(all_tables, ai_factor='AI_Cumulative_Score', horizon='t12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62920fad-589b-49b7-a92c-bcf3640cf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# function definitions\n",
    "# Functions are adapted from the Fama-French script to handle book controls.\n",
    "\n",
    "def run_book_controls_regression_table(reg_data, ai_factor, dependent_var, sector_col, cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    Runs regressions for a progressive buildup of book control variables.\n",
    "    This function is analogous to 'run_academic_regression_table' but for book controls.\n",
    "    It uses firm-clustered standard errors and handles missing data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the progressive models, adding one control at a time\n",
    "    control_models = {\n",
    "        'Base': [],\n",
    "        'Base + Size': ['calc_log_market_cap'],\n",
    "        'Base + Size + Value': ['calc_log_market_cap', 'calc_price_to_book'],\n",
    "        'Full Controls': ['calc_log_market_cap', 'calc_price_to_book', 'calc_roa']\n",
    "    }\n",
    "    \n",
    "    # Base formula includes the AI factor and fixed effects (Sector, Year)\n",
    "    base_formula = f\"{dependent_var} ~ {ai_factor}\"\n",
    "    if sector_col and sector_col in reg_data.columns:\n",
    "        base_formula += f\" + C({sector_col})\"\n",
    "    if 'Year' in reg_data.columns:\n",
    "        base_formula += f\" + C(Year)\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if not cluster_col or cluster_col not in reg_data.columns:\n",
    "        print(f\"Error: Cluster column '{cluster_col}' not found. Aborting.\")\n",
    "        return None\n",
    "\n",
    "    for model_name, controls in control_models.items():\n",
    "        formula = base_formula\n",
    "        available_controls = [c for c in controls if c in reg_data.columns]\n",
    "        if available_controls:\n",
    "            formula += \" + \" + \" + \".join(available_controls)\n",
    "\n",
    "        # Manually prepare data to handle missing values before fitting\n",
    "        all_vars_in_formula = [dependent_var, ai_factor, sector_col, 'Year', cluster_col] + available_controls\n",
    "        # Filter for columns that actually exist in the dataframe\n",
    "        existing_vars = [v for v in all_vars_in_formula if v is not None and v in reg_data.columns]\n",
    "        \n",
    "        clean_data = reg_data[existing_vars].dropna()\n",
    "\n",
    "        if len(clean_data) < 20: # Check for sufficient data\n",
    "            print(f\"Skipping model '{model_name}' due to insufficient data ({len(clean_data)} obs).\")\n",
    "            results[model_name] = None\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Run the model on clean data with firm-clustered standard errors\n",
    "            model = smf.ols(formula, data=clean_data).fit(\n",
    "                cov_type='cluster', \n",
    "                cov_kwds={'groups': clean_data[cluster_col]}\n",
    "            )\n",
    "            \n",
    "            # Store results for the AI factor\n",
    "            ai_coef = model.params.get(ai_factor, np.nan)\n",
    "            ai_stderr = model.bse.get(ai_factor, np.nan)\n",
    "            ai_tstat = model.tvalues.get(ai_factor, np.nan)\n",
    "            ai_pvalue = model.pvalues.get(ai_factor, np.nan)\n",
    "            \n",
    "            # Store results for control variables\n",
    "            control_stats = {}\n",
    "            for control in available_controls:\n",
    "                control_stats[control] = {\n",
    "                    'coef': model.params.get(control, np.nan),\n",
    "                    'stderr': model.bse.get(control, np.nan),\n",
    "                    'tstat': model.tvalues.get(control, np.nan),\n",
    "                    'pvalue': model.pvalues.get(control, np.nan)\n",
    "                }\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'model': model, 'ai_coefficient': ai_coef, 'ai_stderr': ai_stderr,\n",
    "                'ai_tstat': ai_tstat, 'ai_pvalue': ai_pvalue, 'control_stats': control_stats,\n",
    "                'r_squared': model.rsquared, 'adj_r_squared': model.rsquared_adj,\n",
    "                'n_obs': model.nobs, 'available_controls': available_controls\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error running regression for model '{model_name}'. Formula: '{formula}'. Error: {e}\")\n",
    "            results[model_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def format_coefficient_academic(coef, stderr, pvalue):\n",
    "    \"\"\"\n",
    "    Formats a coefficient, its standard error, and adds significance stars.\n",
    "    \"\"\"\n",
    "    if pd.isna(coef) or pd.isna(stderr):\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    if pvalue < 0.01: stars = \"***\"\n",
    "    elif pvalue < 0.05: stars = \"**\"\n",
    "    elif pvalue < 0.1: stars = \"*\"\n",
    "    else: stars = \"\"\n",
    "    \n",
    "    coef_str = f\"{coef:.4f}{stars}\"\n",
    "    stderr_str = f\"({stderr:.4f})\"\n",
    "    \n",
    "    return coef_str, stderr_str\n",
    "\n",
    "def create_book_controls_table_single_factor(reg_data, ai_factor, dependent_var, sector_col='Sector', cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    Assembles a full academic-style regression table for one AI factor with book controls.\n",
    "    \"\"\"\n",
    "    results = run_book_controls_regression_table(reg_data, ai_factor, dependent_var, sector_col, cluster_col)\n",
    "    \n",
    "    if results is None:\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "    # Define the order and display names of variables in the table\n",
    "    variable_order = [\n",
    "        ai_factor, 'calc_log_market_cap', 'calc_price_to_book', 'calc_roa'\n",
    "    ]\n",
    "    variable_names = {\n",
    "        ai_factor: ai_factor.replace('_', ' ').title(),\n",
    "        'calc_log_market_cap': 'Log(Market Cap)',\n",
    "        'calc_price_to_book': 'Price-to-Book',\n",
    "        'calc_roa': 'Return on Assets (ROA)'\n",
    "    }\n",
    "    \n",
    "    model_keys = ['Base', 'Base + Size', 'Base + Size + Value', 'Full Controls']\n",
    "    \n",
    "    table_data = []\n",
    "    for var in variable_order:\n",
    "        row_data = {'Variable': variable_names.get(var, var)}\n",
    "        for model_name in model_keys:\n",
    "            if results.get(model_name) is None:\n",
    "                row_data[f'{model_name}_coef'], row_data[f'{model_name}_stderr'] = \"\", \"\"\n",
    "                continue\n",
    "                \n",
    "            if var == ai_factor:\n",
    "                coef, stderr, pvalue = results[model_name]['ai_coefficient'], results[model_name]['ai_stderr'], results[model_name]['ai_pvalue']\n",
    "            else:\n",
    "                control_stats = results[model_name]['control_stats']\n",
    "                if var in control_stats:\n",
    "                    coef, stderr, pvalue = control_stats[var]['coef'], control_stats[var]['stderr'], control_stats[var]['pvalue']\n",
    "                else:\n",
    "                    coef = stderr = pvalue = np.nan\n",
    "            \n",
    "            coef_str, stderr_str = format_coefficient_academic(coef, stderr, pvalue)\n",
    "            row_data[f'{model_name}_coef'] = coef_str\n",
    "            row_data[f'{model_name}_stderr'] = stderr_str\n",
    "            \n",
    "        table_data.append(row_data)\n",
    "    \n",
    "    # Add summary statistics (R-squared, N Obs, etc.)\n",
    "    summary_rows = []\n",
    "    for stat_name in ['R', 'Adj. R', 'Observations']:\n",
    "        row = {'Variable': stat_name}\n",
    "        for model_name in model_keys:\n",
    "            if results.get(model_name) is not None:\n",
    "                if stat_name == 'R': value = f\"{results[model_name]['r_squared']:.4f}\"\n",
    "                elif stat_name == 'Adj. R': value = f\"{results[model_name]['adj_r_squared']:.4f}\"\n",
    "                else: value = f\"{int(results[model_name]['n_obs']):,}\"\n",
    "                row[f'{model_name}_coef'] = value\n",
    "                row[f'{model_name}_stderr'] = \"\"\n",
    "            else:\n",
    "                row[f'{model_name}_coef'], row[f'{model_name}_stderr'] = \"\", \"\"\n",
    "        summary_rows.append(row)\n",
    "        \n",
    "    return pd.DataFrame(table_data + summary_rows), results\n",
    "\n",
    "def export_book_controls_tables(all_tables):\n",
    "    \"\"\"\n",
    "    Exports book control regression results into a detailed summary and individual tables.\n",
    "    \"\"\"\n",
    "    print(f\"\\nExporting book control regression results\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # 1. Export a detailed summary CSV\n",
    "    summary_data = []\n",
    "    for ai_factor, factor_tables in all_tables.items():\n",
    "        for horizon, table_info in factor_tables.items():\n",
    "            for model_name, result in table_info['results'].items():\n",
    "                if result:\n",
    "                    summary_data.append({\n",
    "                        'AI_Factor': ai_factor, 'Horizon': horizon,\n",
    "                        'Dependent_Variable': table_info['dependent_var'], 'Model': model_name,\n",
    "                        'AI_Coefficient': result['ai_coefficient'], 'AI_Std_Error': result['ai_stderr'],\n",
    "                        'AI_t_stat': result['ai_tstat'], 'AI_p_value': result['ai_pvalue'],\n",
    "                        'R_Squared': result['r_squared'], 'Adj_R_Squared': result['adj_r_squared'],\n",
    "                        'N_Obs': result['n_obs']\n",
    "                    })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv('book_controls_regression_summary_CLUSTERED.csv', index=False)\n",
    "    print(f\"Detailed summary exported to: book_controls_regression_summary_CLUSTERED.csv\")\n",
    "\n",
    "    # 2. Export individual, publication-ready tables\n",
    "    print(\"\\n2. Exporting individual, publication-ready tables...\")\n",
    "    exported_count = 0\n",
    "    model_keys = ['Base', 'Base + Size', 'Base + Size + Value', 'Full Controls']\n",
    "    model_headers = ['Base Model', '+ Size', '+ Value', '+ ROA']\n",
    "\n",
    "    for ai_factor, factor_tables in all_tables.items():\n",
    "        for horizon, table_info in factor_tables.items():\n",
    "            table_df = table_info['table_df']\n",
    "            \n",
    "            output_rows = [['Variable'] + model_headers]\n",
    "            for _, row in table_df.iterrows():\n",
    "                # Coefficient row\n",
    "                output_rows.append([row['Variable']] + [row[f'{m}_coef'] for m in model_keys])\n",
    "                # Standard error row (if it contains data)\n",
    "                if any(row[f'{m}_stderr'] for m in model_keys):\n",
    "                    output_rows.append([''] + [row[f'{m}_stderr'] for m in model_keys])\n",
    "\n",
    "            export_df = pd.DataFrame(output_rows[1:], columns=output_rows[0])\n",
    "            export_df.to_csv(f\"table_book_controls_{ai_factor}_{horizon}_CLUSTERED.csv\", index=False)\n",
    "            exported_count += 1\n",
    "            \n",
    "    print(f\"All {exported_count} individual tables exported (e.g., 'table_book_controls_AI_Cumulative_Score_t12_CLUSTERED.csv').\")\n",
    "\n",
    "\n",
    "def display_sample_book_controls_table(all_tables, ai_factor='AI_Cumulative_Score', horizon='t12'):\n",
    "    \"\"\"\n",
    "    Displays a single, formatted sample table in the console for a quick check.\n",
    "    \"\"\"\n",
    "    if ai_factor in all_tables and horizon in all_tables[ai_factor]:\n",
    "        table_info = all_tables[ai_factor][horizon]\n",
    "        table_df = table_info['table_df']\n",
    "        \n",
    "        print(f\"\\n\\nSample table (book controls buildup with firm-clustered SE):\")\n",
    "        print(f\"AI Factor: {ai_factor.replace('_', ' ').title()} ({horizon}) | Dependent Var: {table_info['dependent_var']}\")\n",
    "        print(\"=\" * 90)\n",
    "        \n",
    "        model_keys = ['Base', 'Base + Size', 'Base + Size + Value', 'Full Controls']\n",
    "        headers = ['Base Model', '+ Size', '+ Value', '+ ROA']\n",
    "        print(f\"{'Variable':<25} {'':<15} {'':<15} {'':<15} {'':<15}\".format(*headers))\n",
    "        print(f\"{'':<25} {headers[0]:<15} {headers[1]:<15} {headers[2]:<15} {headers[3]:<15}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        for _, row in table_df.iterrows():\n",
    "            var_name = row['Variable'][:24]\n",
    "            print(f\"{var_name:<25} {row[f'{model_keys[0]}_coef']:<15} {row[f'{model_keys[1]}_coef']:<15} {row[f'{model_keys[2]}_coef']:<15} {row[f'{model_keys[3]}_coef']:<15}\")\n",
    "            # Print standard error row if it's not empty\n",
    "            if any(row[f'{m}_stderr'] for m in model_keys):\n",
    "                print(f\"{'':<25} {row[f'{model_keys[0]}_stderr']:<15} {row[f'{model_keys[1]}_stderr']:<15} {row[f'{model_keys[2]}_stderr']:<15} {row[f'{model_keys[3]}_stderr']:<15}\")\n",
    "        \n",
    "        print(\"\\nNotes: Firm-clustered standard errors (by Ticker) in parentheses. *p<0.1, **p<0.05, ***p<0.01.\")\n",
    "\n",
    "\n",
    "def run_all_book_controls_tables(reg_data, ai_factors, export_tables=True, cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    Main orchestrator function. Runs regression analysis for book controls and exports results.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating book control regression tables with firm-clustered standard errors\")\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"Clustering standard errors by firm identifier: '{cluster_col}'\")\n",
    "    \n",
    "    # Also test the cumulative score\n",
    "    all_ai_factors_to_test = ai_factors + ['AI_Cumulative_Score']\n",
    "    \n",
    "    # Define return horizons and corresponding dependent variable names\n",
    "    horizons = {\n",
    "        't3': 'excess_return_3mo', 't6': 'excess_return_6mo',\n",
    "        't9': 'excess_return_9mo', 't12': 'excess_return_12mo'\n",
    "    }\n",
    "    \n",
    "    all_tables = {}\n",
    "    \n",
    "    for ai_factor in all_ai_factors_to_test:\n",
    "        if ai_factor not in reg_data.columns:\n",
    "            print(f\"'{ai_factor}' not found in data, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nCreating tables for: {ai_factor.replace('_', ' ').title()}\")\n",
    "        factor_tables = {}\n",
    "        \n",
    "        for horizon, dependent_var in horizons.items():\n",
    "            if dependent_var not in reg_data.columns:\n",
    "                print(f\"  Dependent var '{dependent_var}' not found, skipping {horizon}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Running for {horizon} horizon ({dependent_var})\")\n",
    "            \n",
    "            # This is the core function call to generate the table data for one factor/horizon pair\n",
    "            table_df, results = create_book_controls_table_single_factor(\n",
    "                reg_data, ai_factor, dependent_var, cluster_col=cluster_col\n",
    "            )\n",
    "            \n",
    "            if results: \n",
    "                factor_tables[horizon] = {\n",
    "                    'table_df': table_df, 'results': results, 'dependent_var': dependent_var\n",
    "                }\n",
    "            \n",
    "        all_tables[ai_factor] = factor_tables\n",
    "    \n",
    "    if export_tables:\n",
    "        export_book_controls_tables(all_tables)\n",
    "    \n",
    "    return all_tables, reg_data\n",
    "\n",
    "# example execution\n",
    "\n",
    "print(\"\\n\\nBook controls regression table generator (firm-clustered SE version)\")\n",
    "print(\"=\" * 75)\n",
    "print(\"This script takes the master 'final_reg_data' and runs all regressions.\")\n",
    "print(\"It builds up models by progressively adding book control variables.\")\n",
    "print(\"It uses firm-clustered standard errors for valid inference in panel data.\")\n",
    "\n",
    "# 1. Define the list of individual, averaged AI factors\n",
    "ai_factors = [\n",
    "    'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking',\n",
    "    'Strategic_Depth', 'Talent_Investment', 'Risk_External',\n",
    "    'Risk_Non_Adoption', 'Risk_Own_Adoption'\n",
    "]\n",
    "\n",
    "# 2. Run the complete analysis using the master dataframe ('final_reg_data')\n",
    "all_book_controls_tables, final_data_for_reg = run_all_book_controls_tables(\n",
    "    final_reg_data, \n",
    "    ai_factors, \n",
    "    cluster_col='Ticker'\n",
    ")\n",
    "\n",
    "# 3. Display a sample table to check the results for the main cumulative score\n",
    "display_sample_book_controls_table(\n",
    "    all_book_controls_tables, \n",
    "    ai_factor='AI_Cumulative_Score', \n",
    "    horizon='t12'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b4a4a-c8d6-425e-9711-a0e6f44c2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# function definitions\n",
    "\n",
    "def run_single_regression(reg_data, formula, cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    helper function: Runs a single regression with robust error handling for missing data.\n",
    "    \"\"\"\n",
    "    all_vars_in_formula = [v.strip() for v in formula.replace('~', '+').replace('*', '+').split('+')]\n",
    "    cleaned_vars = []\n",
    "    for var in all_vars_in_formula:\n",
    "        if 'C(' in var:\n",
    "            cleaned_vars.append(var[var.find('(')+1:var.find(')')])\n",
    "        else:\n",
    "            cleaned_vars.append(var)\n",
    "    \n",
    "    final_vars_set = set(cleaned_vars + [cluster_col])\n",
    "    final_vars = [v for v in final_vars_set if v in reg_data.columns]\n",
    "    \n",
    "    clean_data = reg_data[final_vars].dropna()\n",
    "\n",
    "    if len(clean_data) < 20:\n",
    "        print(f\"Skipping model due to insufficient data ({len(clean_data)} obs). Formula: {formula}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        model = smf.ols(formula, data=clean_data).fit(\n",
    "            cov_type='cluster', \n",
    "            cov_kwds={'groups': clean_data[cluster_col]}\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error running regression. Formula: '{formula}'. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_full_control_regression(reg_data, ai_factor, dependent_var, sector_col='Sector', cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    Run full control variables regression for a single AI factor using firm-clustered SEs.\n",
    "    \"\"\"\n",
    "    control_vars = ['calc_log_market_cap', 'calc_price_to_book', 'calc_roa']\n",
    "    available_controls = [var for var in control_vars if var in reg_data.columns]\n",
    "    \n",
    "    formula = f\"{dependent_var} ~ {ai_factor}\"\n",
    "    if sector_col and sector_col in reg_data.columns:\n",
    "        formula += f\" + C({sector_col})\"\n",
    "    if 'Year' in reg_data.columns:\n",
    "        formula += f\" + C(Year)\"\n",
    "    if available_controls:\n",
    "        formula += \" + \" + \" + \".join(available_controls)\n",
    "    \n",
    "    model = run_single_regression(reg_data, formula, cluster_col)\n",
    "    \n",
    "    if model is None:\n",
    "        return None\n",
    "        \n",
    "    return {\n",
    "        'model': model,\n",
    "        'ai_coefficient': model.params.get(ai_factor, np.nan),\n",
    "        'ai_stderr': model.bse.get(ai_factor, np.nan),\n",
    "        'ai_pvalue': model.pvalues.get(ai_factor, np.nan),\n",
    "        'r_squared': model.rsquared, 'adj_r_squared': model.rsquared_adj,\n",
    "        'n_obs': model.nobs\n",
    "    }\n",
    "\n",
    "def run_full_ff6_regression(reg_data, ai_factor, dependent_var, beta_horizon='t12', sector_col='Sector', cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    Run full FF6 regression for a single AI factor using firm-clustered SEs.\n",
    "    \"\"\"\n",
    "    ff6_factors = [f'beta_mktrf_{beta_horizon}', f'beta_smb_{beta_horizon}', f'beta_hml_{beta_horizon}',\n",
    "                   f'beta_rmw_{beta_horizon}', f'beta_cma_{beta_horizon}', f'beta_umd_{beta_horizon}']\n",
    "    available_ff_factors = [var for var in ff6_factors if var in reg_data.columns]\n",
    "    \n",
    "    formula = f\"{dependent_var} ~ {ai_factor}\"\n",
    "    if sector_col and sector_col in reg_data.columns:\n",
    "        formula += f\" + C({sector_col})\"\n",
    "    if 'Year' in reg_data.columns:\n",
    "        formula += f\" + C(Year)\"\n",
    "    if available_ff_factors:\n",
    "        formula += \" + \" + \" + \".join(available_ff_factors)\n",
    "        \n",
    "    model = run_single_regression(reg_data, formula, cluster_col)\n",
    "    \n",
    "    if model is None:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'ai_coefficient': model.params.get(ai_factor, np.nan),\n",
    "        'ai_stderr': model.bse.get(ai_factor, np.nan),\n",
    "        'ai_pvalue': model.pvalues.get(ai_factor, np.nan),\n",
    "        'r_squared': model.rsquared, 'adj_r_squared': model.rsquared_adj,\n",
    "        'n_obs': model.nobs\n",
    "    }\n",
    "\n",
    "def format_coefficient_for_comparison_table(coef, stderr, pvalue):\n",
    "    \"\"\"\n",
    "    Format coefficient for comparison table (coefficient with stars and stderr in parentheses).\n",
    "    \"\"\"\n",
    "    if pd.isna(coef) or pd.isna(stderr):\n",
    "        return \"\"\n",
    "    \n",
    "    if pvalue < 0.01: stars = \"***\"\n",
    "    elif pvalue < 0.05: stars = \"**\"\n",
    "    elif pvalue < 0.1: stars = \"*\"\n",
    "    else: stars = \"\"\n",
    "    \n",
    "    return f\"{coef:.4f}{stars}\\n({stderr:.4f})\"\n",
    "\n",
    "def create_ai_factors_comparison_table(reg_data, ai_factors, cluster_col='Ticker', export_tables=True):\n",
    "    \"\"\"\n",
    "    Create cleaner comparison tables without extra empty rows.\n",
    "    \"\"\"\n",
    "    print(\"Creating AI factors comparison tables (controls vs. FF6)\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    horizons = {'t3': 'excess_return_3mo', 't6': 'excess_return_6mo', 't9': 'excess_return_9mo', 't12': 'excess_return_12mo'}\n",
    "    all_stacked_rows = [[\"AI FACTOR COMPARISON: CONTROLS vs. FAMA-FRENCH 6-FACTOR\", \"\", \"\"]]\n",
    "    \n",
    "    for horizon, dependent_var in horizons.items():\n",
    "        if dependent_var not in reg_data.columns:\n",
    "            print(f\"Skipping {horizon} horizon, dependent variable '{dependent_var}' not found.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing {horizon.upper()} Horizon (Dependent Var: {dependent_var})\")\n",
    "        \n",
    "        horizon_results = {}\n",
    "        all_ai_to_test = ai_factors + ['AI_Cumulative_Score']\n",
    "        for ai_factor in all_ai_to_test:\n",
    "            if ai_factor in reg_data.columns:\n",
    "                control_res = run_full_control_regression(reg_data, ai_factor, dependent_var, cluster_col=cluster_col)\n",
    "                ff6_res = run_full_ff6_regression(reg_data, ai_factor, dependent_var, horizon, cluster_col=cluster_col)\n",
    "                horizon_results[ai_factor] = {'control_result': control_res, 'ff6_result': ff6_res}\n",
    "\n",
    "        # Format the table for this horizon\n",
    "        table_rows = [[f\"Time Horizon: {horizon.upper()}\", \"\", \"\"], [f\"Standard Errors: Clustered by {cluster_col}\", \"\", \"\"],\n",
    "                      [\"AI Factor\", \"Control Variables Model\", \"Fama-French 6-Factor Model\"],\n",
    "                      [\"\", \"(FE + Size, Value, ROA)\", \"(FE + FF6 Factors)\"]]\n",
    "\n",
    "        for ai_factor, results in horizon_results.items():\n",
    "            control_formatted = format_coefficient_for_comparison_table(results['control_result']['ai_coefficient'], results['control_result']['ai_stderr'], results['control_result']['ai_pvalue']) if results.get('control_result') else \"Error\"\n",
    "            ff6_formatted = format_coefficient_for_comparison_table(results['ff6_result']['ai_coefficient'], results['ff6_result']['ai_stderr'], results['ff6_result']['ai_pvalue']) if results.get('ff6_result') else \"Error\"\n",
    "            table_rows.append([ai_factor.replace('_', ' ').title(), control_formatted, ff6_formatted])\n",
    "        \n",
    "        # Add summary rows for R-squared and Observations without extra spacing\n",
    "        control_r2s = [res['control_result']['adj_r_squared'] for res in horizon_results.values() if res.get('control_result')]\n",
    "        ff6_r2s = [res['ff6_result']['adj_r_squared'] for res in horizon_results.values() if res.get('ff6_result')]\n",
    "        avg_control_r2 = np.mean(control_r2s) if control_r2s else np.nan\n",
    "        avg_ff6_r2 = np.mean(ff6_r2s) if ff6_r2s else np.nan\n",
    "        table_rows.append([\"Adj. R-squared (Avg)\", f\"{avg_control_r2:.4f}\", f\"{avg_ff6_r2:.4f}\"])\n",
    "\n",
    "        control_n = [res['control_result']['n_obs'] for res in horizon_results.values() if res.get('control_result')]\n",
    "        ff6_n = [res['ff6_result']['n_obs'] for res in horizon_results.values() if res.get('ff6_result')]\n",
    "        avg_control_n = np.mean(control_n) if control_n else np.nan\n",
    "        avg_ff6_n = np.mean(ff6_n) if ff6_n else np.nan\n",
    "        table_rows.append([\"Observations (Avg)\", f\"{avg_control_n:,.0f}\", f\"{avg_ff6_n:,.0f}\"])\n",
    "            \n",
    "        all_stacked_rows.extend(table_rows)\n",
    "\n",
    "    if export_tables:\n",
    "        filename = f\"ai_factors_comparison_controls_vs_ff6_CLUSTERED.csv\"\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            import csv\n",
    "            csv.writer(f).writerows(all_stacked_rows)\n",
    "        print(f\"\\n\\nFull comparison table exported to: {filename}\")\n",
    "\n",
    "    return all_stacked_rows\n",
    "\n",
    "# run the analysis\n",
    "\n",
    "print(\"AI factors comparison: controls vs Fama-French (firm-clustered SE version)\")\n",
    "print(\"=\" * 75)\n",
    "print(\"This script runs regressions comparing a simple control model to a full FF6 model.\")\n",
    "print(\"It uses firm-clustered standard errors for valid inference in panel data.\")\n",
    "print(\"It now includes Adj. R-squared and N observations in the final table.\")\n",
    "\n",
    "# 1. Define the list of individual, averaged AI factors\n",
    "ai_factors = [\n",
    "    'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking',\n",
    "    'Strategic_Depth', 'Talent_Investment', 'Risk_External',\n",
    "    'Risk_Non_Adoption', 'Risk_Own_Adoption'\n",
    "]\n",
    "\n",
    "# 2. Run the complete analysis using the master dataframe ('final_reg_data')\n",
    "comparison_table_rows = create_ai_factors_comparison_table(\n",
    "    reg_data=final_reg_data, \n",
    "    ai_factors=ai_factors, \n",
    "    cluster_col='Ticker'\n",
    ")\n",
    "\n",
    "print(\"\\n\\nAI Factor Comparison Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e6f93-10fd-4f49-a444-02560ebc7230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d43c62e-a5df-4037-b03f-ae6ff5428f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FINAL SECTOR ANALYSIS (FIRM-CLUSTERED SEs)\n",
      "==================================================\n",
      "\n",
      " ANALYZING T12 HORIZON\n",
      "\n",
      "   INTERACTION MODEL for: AI_Washing\n",
      "     Combined Interaction Model: R=0.052, N=3,028.0\n",
      "\n",
      "   INDIVIDUAL SECTOR REGRESSIONS for: AI_Washing\n",
      "\n",
      "   INTERACTION MODEL for: Disclosure_Sentiment\n",
      "     Combined Interaction Model: R=0.050, N=3,028.0\n",
      "\n",
      "   INDIVIDUAL SECTOR REGRESSIONS for: Disclosure_Sentiment\n",
      "\n",
      "   INTERACTION MODEL for: Forward_Looking\n",
      "     Combined Interaction Model: R=0.052, N=3,028.0\n",
      "\n",
      "   INDIVIDUAL SECTOR REGRESSIONS for: Forward_Looking\n",
      "\n",
      "   INTERACTION MODEL for: Strategic_Depth\n",
      "     Combined Interaction Model: R=0.055, N=3,028.0\n",
      "\n",
      "   INDIVIDUAL SECTOR REGRESSIONS for: Strategic_Depth\n",
      "\n",
      "   INTERACTION MODEL for: Talent_Investment\n",
      "     Combined Interaction Model: R=0.051, N=3,028.0\n",
      "\n",
      "   INDIVIDUAL SECTOR REGRESSIONS for: Talent_Investment\n",
      "\n",
      "   INTERACTION MODEL for: Risk_External\n",
      "     Combined Interaction Model: R=0.041, N=3,028.0\n",
      "\n",
      "   INDIVIDUAL SECTOR REGRESSIONS for: Risk_External\n",
      "\n",
      "   INTERACTION MODEL for: Risk_Non_Adoption\n",
      "     Combined Interaction Model: R=0.048, N=3,028.0\n",
      "\n",
      "   INDIVIDUAL SECTOR REGRESSIONS for: Risk_Non_Adoption\n",
      "\n",
      "   INTERACTION MODEL for: Risk_Own_Adoption\n",
      "     Combined Interaction Model: R=0.050, N=3,028.0\n",
      "\n",
      "   INDIVIDUAL SECTOR REGRESSIONS for: Risk_Own_Adoption\n",
      "\n",
      "   INTERACTION MODEL for: AI_Cumulative_Score\n",
      "     Combined Interaction Model: R=0.055, N=3,028.0\n",
      "\n",
      "   INDIVIDUAL SECTOR REGRESSIONS for: AI_Cumulative_Score\n",
      "\n",
      "\n",
      " EXPORTING FINAL SECTOR ANALYSIS\n",
      "========================================\n",
      " Comprehensive sector analysis summary exported to: sector_analysis_summary_CLUSTERED.csv\n",
      "\n",
      " ANALYSIS COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#================================================================================\n",
    "#                        1. HELPER FUNCTION\n",
    "#================================================================================\n",
    "\n",
    "def run_single_regression(reg_data, formula, cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    **HELPER FUNCTION**: Runs a single regression with robust error handling for missing data\n",
    "    and uses firm-clustered standard errors.\n",
    "    \"\"\"\n",
    "    # Identify all variables needed for this specific formula\n",
    "    all_vars_in_formula = [v.strip() for v in formula.replace('~', '+').replace('*', '+').replace(':', '+').split('+')]\n",
    "    cleaned_vars = []\n",
    "    for var in all_vars_in_formula:\n",
    "        if 'C(' in var:\n",
    "            cleaned_vars.append(var[var.find('(')+1:var.find(')')])\n",
    "        else:\n",
    "            cleaned_vars.append(var)\n",
    "    \n",
    "    final_vars_set = set(cleaned_vars + [cluster_col])\n",
    "    final_vars = [v for v in final_vars_set if v in reg_data.columns]\n",
    "    \n",
    "    clean_data = reg_data[final_vars].dropna()\n",
    "\n",
    "    if len(clean_data) < 20:\n",
    "        # print(f\"Skipping model due to insufficient data ({len(clean_data)} obs).\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        model = smf.ols(formula, data=clean_data).fit(\n",
    "            cov_type='cluster', \n",
    "            cov_kwds={'groups': clean_data[cluster_col]}\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in formula '{formula}': {e}\")\n",
    "        return None\n",
    "\n",
    "#================================================================================\n",
    "#                        2. MAIN ANALYSIS FUNCTIONS\n",
    "#================================================================================\n",
    "\n",
    "def run_sector_interactions(data, ai_factor, dependent_var, ff_factors, controls, sectors, cluster_col):\n",
    "    \"\"\"\n",
    "    Run sector interaction models for a single AI factor.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  INTERACTION MODEL for: {ai_factor}\")\n",
    "    factor_models = {}\n",
    "    \n",
    "    # Model: Combined (Controls + FF6 + Interactions)\n",
    "    control_terms = \" + \".join(controls)\n",
    "    ff_terms = \" + \".join(ff_factors)\n",
    "    formula = f\"{dependent_var} ~ {ai_factor}*C(Sector) + {control_terms} + {ff_terms} + C(Year)\"\n",
    "    \n",
    "    model = run_single_regression(data, formula, cluster_col)\n",
    "    \n",
    "    if model:\n",
    "        factor_models['combined'] = model\n",
    "        factor_models['sector_effects'] = extract_sector_effects(model, ai_factor, sectors)\n",
    "        print(f\"    Combined Interaction Model: R={model.rsquared:.3f}, N={model.nobs:,}\")\n",
    "    else:\n",
    "        print(f\"    Combined Interaction Model failed.\")\n",
    "        \n",
    "    return factor_models\n",
    "\n",
    "def run_individual_sectors(data, ai_factor, dependent_var, ff_factors, controls, sectors, cluster_col):\n",
    "    \"\"\"\n",
    "    Run regressions for each sector individually.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  INDIVIDUAL SECTOR REGRESSIONS for: {ai_factor}\")\n",
    "    results = {}\n",
    "    \n",
    "    for sector in sectors:\n",
    "        sector_data = data[data['Sector'] == sector]\n",
    "        if len(sector_data) < 20: continue\n",
    "\n",
    "        # Combined Model: AI Factor + Controls + FF6 + Year FE\n",
    "        formula = f\"{dependent_var} ~ {ai_factor} + {' + '.join(controls)} + {' + '.join(ff_factors)} + C(Year)\"\n",
    "        model = run_single_regression(sector_data, formula, cluster_col)\n",
    "        \n",
    "        if model:\n",
    "            if sector not in results: results[sector] = {}\n",
    "            results[sector][ai_factor] = model\n",
    "            \n",
    "    return results\n",
    "\n",
    "def extract_sector_effects(model, ai_factor, sectors):\n",
    "    \"\"\"\n",
    "    Extract sector-specific AI effects from an interaction model.\n",
    "    \"\"\"\n",
    "    effects = {}\n",
    "    # Base effect (for the reference sector)\n",
    "    base_coef = model.params.get(ai_factor, 0)\n",
    "    base_se = model.bse.get(ai_factor, np.nan)\n",
    "    base_pval = model.pvalues.get(ai_factor, np.nan)\n",
    "    \n",
    "    # The reference sector is the first one alphabetically that patsy chooses\n",
    "    ref_sector = [s for s in sectors if f\"C(Sector)[T.{s}]\" not in model.params.index][0]\n",
    "    effects[ref_sector] = {'coef': base_coef, 'se': base_se, 'pvalue': base_pval}\n",
    "\n",
    "    # Interaction effects for other sectors\n",
    "    for sector in sectors:\n",
    "        if sector == ref_sector: continue\n",
    "        interaction_term = f\"{ai_factor}:C(Sector)[T.{sector}]\"\n",
    "        if interaction_term in model.params:\n",
    "            # The total effect for an interacted sector is (base_coef + interaction_coef)\n",
    "            # We test the significance of the total effect using a Wald test.\n",
    "            total_effect_test = model.t_test(f\"{ai_factor} + {interaction_term}\")\n",
    "            effects[sector] = {\n",
    "                'coef': total_effect_test.effect[0],\n",
    "                'se': total_effect_test.sd[0],\n",
    "                'pvalue': total_effect_test.pvalue\n",
    "            }\n",
    "    return effects\n",
    "\n",
    "#================================================================================\n",
    "#                        3. EXPORT & FORMATTING FUNCTIONS\n",
    "#================================================================================\n",
    "\n",
    "def export_sector_analysis_results(all_results, ai_factors, sectors, cluster_col):\n",
    "    \"\"\"\n",
    "    **NEW**: Creates a single, comprehensive CSV file summarizing all sector results.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n EXPORTING FINAL SECTOR ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    output_rows = []\n",
    "    header = ['Time Horizon', 'Analysis Type', 'Sector/Model', 'AI Factor', 'Coefficient', 'Std. Error', 'P-Value', 'Significant (5%)', 'N Obs', 'Adj. R-squared']\n",
    "    output_rows.append(header)\n",
    "\n",
    "    for horizon, horizon_results in all_results.items():\n",
    "        # Interaction Model Results\n",
    "        for ai_factor in ai_factors:\n",
    "            if 'interactions' in horizon_results and ai_factor in horizon_results['interactions'] and 'sector_effects' in horizon_results['interactions'][ai_factor]:\n",
    "                model = horizon_results['interactions'][ai_factor].get('combined')\n",
    "                if not model: continue\n",
    "                \n",
    "                for sector, effects in horizon_results['interactions'][ai_factor]['sector_effects'].items():\n",
    "                    output_rows.append([\n",
    "                        horizon.upper(), 'Interaction', sector, ai_factor.replace('_', ' ').title(),\n",
    "                        f\"{effects.get('coef', 0):.4f}\", f\"{effects.get('se', 0):.4f}\", f\"{effects.get('pvalue', 1):.4f}\",\n",
    "                        effects.get('pvalue', 1) < 0.05, model.nobs, f\"{model.rsquared_adj:.4f}\"\n",
    "                    ])\n",
    "\n",
    "        # Individual Sector Regression Results\n",
    "        if 'individual' in horizon_results:\n",
    "            for sector, sector_results in horizon_results['individual'].items():\n",
    "                for ai_factor, model in sector_results.items():\n",
    "                    output_rows.append([\n",
    "                        horizon.upper(), 'Individual', sector, ai_factor.replace('_', ' ').title(),\n",
    "                        f\"{model.params.get(ai_factor, 0):.4f}\", f\"{model.bse.get(ai_factor, 0):.4f}\", f\"{model.pvalues.get(ai_factor, 1):.4f}\",\n",
    "                        model.pvalues.get(ai_factor, 1) < 0.05, model.nobs, f\"{model.rsquared_adj:.4f}\"\n",
    "                    ])\n",
    "    \n",
    "    filename = f\"sector_analysis_summary_CLUSTERED.csv\"\n",
    "    pd.DataFrame(output_rows[1:], columns=output_rows[0]).to_csv(filename, index=False)\n",
    "    print(f\" Comprehensive sector analysis summary exported to: {filename}\")\n",
    "\n",
    "#================================================================================\n",
    "#                        4. MAIN ORCHESTRATOR\n",
    "#================================================================================\n",
    "\n",
    "def run_final_sector_analysis(final_reg_data, ai_factors, time_horizons=['t12'], cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    Complete sector analysis using final_reg_data with firm-clustered SEs.\n",
    "    \"\"\"\n",
    "    print(\" FINAL SECTOR ANALYSIS (FIRM-CLUSTERED SEs)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    dependent_vars = {'t3': 'excess_return_3mo', 't6': 'excess_return_6mo', 't9': 'excess_return_9mo', 't12': 'excess_return_12mo'}\n",
    "    sectors = sorted([s for s in final_reg_data['Sector'].unique() if pd.notna(s)])\n",
    "    controls = ['calc_log_market_cap', 'calc_price_to_book', 'calc_roa']\n",
    "    all_ai_factors = ai_factors + ['AI_Cumulative_Score']\n",
    "\n",
    "    all_results = {}\n",
    "    for horizon in time_horizons:\n",
    "        if dependent_vars[horizon] not in final_reg_data.columns:\n",
    "            print(f\"\\n Skipping {horizon} horizon: Dependent variable not available.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n ANALYZING {horizon.upper()} HORIZON\")\n",
    "        ff_factors = [f'beta_mktrf_{horizon}', f'beta_smb_{horizon}', f'beta_hml_{horizon}',\n",
    "                      f'beta_rmw_{horizon}', f'beta_cma_{horizon}', f'beta_umd_{horizon}']\n",
    "        \n",
    "        #  only need to run one loop for each AI factor now\n",
    "        interaction_results_all = {}\n",
    "        individual_results_all = {}\n",
    "\n",
    "        for ai_factor in all_ai_factors:\n",
    "            interaction_results_all.update(run_sector_interactions(final_reg_data, ai_factor, dependent_vars[horizon], ff_factors, controls, sectors, cluster_col))\n",
    "            individual_results_all.update(run_individual_sectors(final_reg_data, ai_factor, dependent_vars[horizon], ff_factors, controls, sectors, cluster_col))\n",
    "\n",
    "        all_results[horizon] = {\n",
    "            'interactions': interaction_results_all,\n",
    "            'individual': individual_results_all\n",
    "        }\n",
    "        \n",
    "    export_sector_analysis_results(all_results, all_ai_factors, sectors, cluster_col)\n",
    "    \n",
    "    print(f\"\\n ANALYSIS COMPLETE!\")\n",
    "    return all_results\n",
    "\n",
    "#================================================================================\n",
    "#                        5. RUN THE ANALYSIS\n",
    "#================================================================================\n",
    "\n",
    "ai_factors = [\n",
    "    'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking',\n",
    "    'Strategic_Depth', 'Talent_Investment', 'Risk_External',\n",
    "    'Risk_Non_Adoption', 'Risk_Own_Adoption'\n",
    "]\n",
    "\n",
    "# Run for the 12-month horizon as a sample\n",
    "sector_analysis_results = run_final_sector_analysis(final_reg_data, ai_factors, time_horizons=['t12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "840b28c8-03e3-4691-9d46-1ea153a4058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FINAL SECTOR ANALYSIS (FIRM-CLUSTERED SEs & INDIVIDUAL TABLE EXPORTS)\n",
      "===========================================================================\n",
      "\n",
      " ANALYZING T3 HORIZON...\n",
      "   Running Individual Sector Regressions (Controls vs. FF6)...\n",
      "\n",
      "   Exporting individual sector tables to: 'sector_analysis/t3/'\n",
      "     Successfully exported 11 sector tables.\n",
      "\n",
      " ANALYZING T6 HORIZON...\n",
      "   Running Individual Sector Regressions (Controls vs. FF6)...\n",
      "\n",
      "   Exporting individual sector tables to: 'sector_analysis/t6/'\n",
      "     Successfully exported 11 sector tables.\n",
      "\n",
      " ANALYZING T9 HORIZON...\n",
      "   Running Individual Sector Regressions (Controls vs. FF6)...\n",
      "\n",
      "   Exporting individual sector tables to: 'sector_analysis/t9/'\n",
      "     Successfully exported 11 sector tables.\n",
      "\n",
      " ANALYZING T12 HORIZON...\n",
      "   Running Individual Sector Regressions (Controls vs. FF6)...\n",
      "\n",
      "   Exporting individual sector tables to: 'sector_analysis/t12/'\n",
      "     Successfully exported 11 sector tables.\n",
      "\n",
      "\n",
      " All Sector Analysis and Exports Complete! \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import os\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#================================================================================\n",
    "#                        1. HELPER & REGRESSION FUNCTIONS\n",
    "#================================================================================\n",
    "\n",
    "def run_single_regression(reg_data, formula, cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    **HELPER FUNCTION**: Runs a single regression with robust error handling for missing data\n",
    "    and uses firm-clustered standard errors.\n",
    "    \"\"\"\n",
    "    # Identify all variables needed for this specific formula\n",
    "    all_vars_in_formula = [v.strip() for v in formula.replace('~', '+').replace('*', '+').replace(':', '+').split('+')]\n",
    "    cleaned_vars = []\n",
    "    for var in all_vars_in_formula:\n",
    "        if 'C(' in var:\n",
    "            cleaned_vars.append(var[var.find('(')+1:var.find(')')])\n",
    "        else:\n",
    "            cleaned_vars.append(var)\n",
    "    \n",
    "    final_vars_set = set(cleaned_vars + [cluster_col])\n",
    "    final_vars = [v for v in final_vars_set if v in reg_data.columns]\n",
    "    \n",
    "    clean_data = reg_data[final_vars].dropna()\n",
    "\n",
    "    if len(clean_data) < 20:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        model = smf.ols(formula, data=clean_data).fit(\n",
    "            cov_type='cluster', \n",
    "            cov_kwds={'groups': clean_data[cluster_col]}\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        # This will now only print an error if one actually occurs\n",
    "        # print(f\"Error in formula '{formula}': {e}\")\n",
    "        return None\n",
    "\n",
    "def format_coefficient_academic(coef, stderr, pvalue):\n",
    "    \"\"\"\n",
    "    **FIXED**: Formats a coefficient, SE, and p-value into a standard academic string\n",
    "    with standard errors in parentheses on a new line.\n",
    "    \"\"\"\n",
    "    if pd.isna(coef) or pd.isna(stderr):\n",
    "        return \"\"\n",
    "    if pvalue < 0.01: stars = \"***\"\n",
    "    elif pvalue < 0.05: stars = \"**\"\n",
    "    elif pvalue < 0.1: stars = \"*\"\n",
    "    else: stars = \"\"\n",
    "    \n",
    "    # This format creates \"COEF***\\n(SE)\" which displays well in Excel/Sheets\n",
    "    return f\"{coef:.4f}{stars}\\n({stderr:.4f})\"\n",
    "\n",
    "#================================================================================\n",
    "#                        2. SECTOR ANALYSIS FUNCTIONS\n",
    "#================================================================================\n",
    "\n",
    "def run_individual_sector_regressions(data, ai_factors, dependent_var, ff_factors, controls, sectors, cluster_col):\n",
    "    \"\"\"\n",
    "    **MODIFIED**: Runs regressions for each AI factor within each individual sector,\n",
    "    but only for the 'Controls' and 'FF6' models (removes 'Combined').\n",
    "    \"\"\"\n",
    "    print(\"   Running Individual Sector Regressions (Controls vs. FF6)...\")\n",
    "    sector_results = {}\n",
    "    \n",
    "    for sector in sectors:\n",
    "        sector_data = data[data['Sector'] == sector]\n",
    "        if len(sector_data) < 20: continue\n",
    "\n",
    "        models_for_this_sector = {}\n",
    "        for ai_factor in ai_factors:\n",
    "            if ai_factor not in sector_data.columns: continue\n",
    "\n",
    "            # Define the two models to run for each AI factor\n",
    "            control_formula = f\"{dependent_var} ~ {ai_factor} + {' + '.join(controls)} + C(Year)\"\n",
    "            ff6_formula = f\"{dependent_var} ~ {ai_factor} + {' + '.join(ff_factors)} + C(Year)\"\n",
    "            \n",
    "            models_for_this_sector[ai_factor] = {\n",
    "                'Controls': run_single_regression(sector_data, control_formula, cluster_col),\n",
    "                'FF6': run_single_regression(sector_data, ff6_formula, cluster_col)\n",
    "            }\n",
    "        sector_results[sector] = models_for_this_sector\n",
    "        \n",
    "    return sector_results\n",
    "\n",
    "#================================================================================\n",
    "#                        3. EXPORT & FORMATTING FUNCTIONS\n",
    "#================================================================================\n",
    "\n",
    "def create_and_export_sector_tables(sector_results, horizon, cluster_col):\n",
    "    \"\"\"\n",
    "    **MODIFIED**: Creates and exports publication-ready academic tables for each sector,\n",
    "    with a cleaner format and added summary statistics.\n",
    "    \"\"\"\n",
    "    output_dir = f\"sector_analysis/{horizon}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\n   Exporting individual sector tables to: '{output_dir}/'\")\n",
    "\n",
    "    for sector, ai_factor_models in sector_results.items():\n",
    "        table_rows = []\n",
    "        # **FIX**: Simplified header\n",
    "        header = [\"AI Factor\", \"Controls Model\", \"Fama-French 6-Factor Model\"]\n",
    "        table_rows.append(header)\n",
    "\n",
    "        # Temp storage for R-squared and N values to average them later\n",
    "        r2_controls, r2_ff6 = [], []\n",
    "        n_controls, n_ff6 = [], []\n",
    "\n",
    "        for ai_factor, models in ai_factor_models.items():\n",
    "            row_data = [ai_factor.replace('_', ' ').title()]\n",
    "            \n",
    "            # Process 'Controls' model\n",
    "            model_controls = models.get('Controls')\n",
    "            if model_controls:\n",
    "                coef, se, pval = model_controls.params.get(ai_factor), model_controls.bse.get(ai_factor), model_controls.pvalues.get(ai_factor)\n",
    "                row_data.append(format_coefficient_academic(coef, se, pval))\n",
    "                r2_controls.append(model_controls.rsquared_adj)\n",
    "                n_controls.append(model_controls.nobs)\n",
    "            else:\n",
    "                row_data.append(\"N/A\")\n",
    "\n",
    "            # Process 'FF6' model\n",
    "            model_ff6 = models.get('FF6')\n",
    "            if model_ff6:\n",
    "                coef, se, pval = model_ff6.params.get(ai_factor), model_ff6.bse.get(ai_factor), model_ff6.pvalues.get(ai_factor)\n",
    "                row_data.append(format_coefficient_academic(coef, se, pval))\n",
    "                r2_ff6.append(model_ff6.rsquared_adj)\n",
    "                n_ff6.append(model_ff6.nobs)\n",
    "            else:\n",
    "                row_data.append(\"N/A\")\n",
    "                \n",
    "            table_rows.append(row_data)\n",
    "            \n",
    "        # **FIX**: Add summary stats (Avg R-squared, Avg N) directly to the table footer\n",
    "        table_rows.append([\"Adj. R-squared (Avg)\", f\"{np.mean(r2_controls):.4f}\" if r2_controls else \"N/A\", f\"{np.mean(r2_ff6):.4f}\" if r2_ff6 else \"N/A\"])\n",
    "        table_rows.append([\"Observations (Avg)\", f\"{np.mean(n_controls):,.0f}\" if n_controls else \"N/A\", f\"{np.mean(n_ff6):,.0f}\" if n_ff6 else \"N/A\"])\n",
    "        \n",
    "        # Create and save the DataFrame\n",
    "        table_df = pd.DataFrame(table_rows[1:], columns=table_rows[0])\n",
    "        filename = f\"{output_dir}/{sector.replace(' ', '_').replace('/', '_')}.csv\"\n",
    "        table_df.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"     Successfully exported {len(sector_results)} sector tables.\")\n",
    "\n",
    "#================================================================================\n",
    "#                        4. MAIN ORCHESTRATOR\n",
    "#================================================================================\n",
    "\n",
    "def run_final_sector_analysis(final_reg_data, ai_factors, time_horizons=['t12'], cluster_col='Ticker'):\n",
    "    \"\"\"\n",
    "    **MODIFIED**: Main function to run the simplified (Controls vs. FF6) sector analysis.\n",
    "    \"\"\"\n",
    "    print(\" FINAL SECTOR ANALYSIS (FIRM-CLUSTERED SEs & INDIVIDUAL TABLE EXPORTS)\")\n",
    "    print(\"=\" * 75)\n",
    "    \n",
    "    dependent_vars = {'t3': 'excess_return_3mo', 't6': 'excess_return_6mo', 't9': 'excess_return_9mo', 't12': 'excess_return_12mo'}\n",
    "    sectors = sorted([s for s in final_reg_data['Sector'].unique() if pd.notna(s)])\n",
    "    controls = ['calc_log_market_cap', 'calc_price_to_book', 'calc_roa']\n",
    "    all_ai_factors = ai_factors + ['AI_Cumulative_Score']\n",
    "\n",
    "    for horizon in time_horizons:\n",
    "        if dependent_vars.get(horizon) not in final_reg_data.columns:\n",
    "            print(f\"\\n Skipping {horizon.upper()} horizon: Dependent variable not available.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n ANALYZING {horizon.upper()} HORIZON...\")\n",
    "        \n",
    "        ff_factors = [f'beta_mktrf_{horizon}', f'beta_smb_{horizon}', f'beta_hml_{horizon}',\n",
    "                      f'beta_rmw_{horizon}', f'beta_cma_{horizon}', f'beta_umd_{horizon}']\n",
    "        \n",
    "        individual_sector_results = run_individual_sector_regressions(\n",
    "            final_reg_data, all_ai_factors, dependent_vars[horizon], \n",
    "            ff_factors, controls, sectors, cluster_col\n",
    "        )\n",
    "        \n",
    "        create_and_export_sector_tables(individual_sector_results, horizon, cluster_col)\n",
    "        \n",
    "    print(\"\\n\\n All Sector Analysis and Exports Complete\")\n",
    "\n",
    "#================================================================================\n",
    "#                        5. RUN THE ANALYSIS\n",
    "#================================================================================\n",
    "\n",
    "# Define the list of your individual, averaged AI factors\n",
    "ai_factors = [\n",
    "    'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking',\n",
    "    'Strategic_Depth', 'Talent_Investment', 'Risk_External',\n",
    "    'Risk_Non_Adoption', 'Risk_Own_Adoption'\n",
    "]\n",
    "\n",
    "# Run the full analysis for all time horizons\n",
    "# This will create the 'sector_analysis' folder with subfolders for each horizon.\n",
    "run_final_sector_analysis(final_reg_data, ai_factors, time_horizons=['t3', 't6', 't9', 't12'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cf01f-ea14-44ba-85a7-94c6cbbec6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def find_interesting_cases(reg_data, sector_name='Consumer Discretionary', return_col='excess_return_12mo'):\n",
    "    \"\"\"\n",
    "    identifies and analyzes firms within a specific sector that have high ai scores\n",
    "    but subsequently underperform, providing concrete examples for case studies\n",
    "    \"\"\"\n",
    "    print(\"Case study explorer\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Objective: find firms in '{sector_name}' with high AI scores and low subsequent returns\")\n",
    "    \n",
    "    # 1. filter the dataset to the specified sector\n",
    "    sector_df = reg_data[reg_data['Sector'] == sector_name].copy()\n",
    "    \n",
    "    if sector_df.empty:\n",
    "        print(f\"\\nError: no data found for the '{sector_name}' sector. Please check the sector name\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\n1. Filtering to '{sector_name}': found {len(sector_df):,} observations\")\n",
    "    \n",
    "    # 2. define the thresholds for \"high\" and \"low\" using quartiles\n",
    "    high_ai_threshold = sector_df['AI_Cumulative_Score'].dropna().quantile(0.75)\n",
    "    low_return_threshold = sector_df[return_col].dropna().quantile(0.25)\n",
    "    \n",
    "    print(f\"2. Defining thresholds:\")\n",
    "    print(f\"   - 'high AI score' is defined as > {high_ai_threshold:.2f} (top 25%)\")\n",
    "    print(f\"   - 'low return' is defined as < {low_return_threshold:.2%} (bottom 25%)\")\n",
    "    \n",
    "    # 3. identify the firms that meet both criteria\n",
    "    interesting_cases = sector_df[\n",
    "        (sector_df['AI_Cumulative_Score'] > high_ai_threshold) &\n",
    "        (sector_df[return_col] < low_return_threshold)\n",
    "    ]\n",
    "    \n",
    "    if interesting_cases.empty:\n",
    "        print(\"\\nNo specific firms were found that met both the 'high AI' and 'low return' criteria\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\n3. Identifying cases: found {len(interesting_cases)} observations meeting both criteria\")\n",
    "    \n",
    "    # 4. prepare a clean table for analysis\n",
    "    display_cols = [\n",
    "        'Ticker', 'Company Name', 'Year', 'AI_Cumulative_Score', return_col\n",
    "    ]\n",
    "    \n",
    "    # find out which individual ai factor was highest for each case\n",
    "    ai_factor_cols = [\n",
    "        'AI_Washing', 'Disclosure_Sentiment', 'Forward_Looking',\n",
    "        'Strategic_Depth', 'Talent_Investment', 'Risk_External',\n",
    "        'Risk_Non_Adoption', 'Risk_Own_Adoption'\n",
    "    ]\n",
    "    # ensure only factors present in the data are used\n",
    "    available_ai_factors = [f for f in ai_factor_cols if f in interesting_cases.columns]\n",
    "    \n",
    "    if available_ai_factors:\n",
    "        interesting_cases['Top_AI_Factor'] = interesting_cases[available_ai_factors].idxmax(axis=1)\n",
    "        display_cols.append('Top_AI_Factor')\n",
    "\n",
    "    report_table = interesting_cases[display_cols].sort_values(by=[return_col], ascending=True)\n",
    "    \n",
    "    # format the return column as a percentage\n",
    "    report_table[return_col] = report_table[return_col].apply(lambda x: f\"{x:.2%}\")\n",
    "\n",
    "    print(\"\\n\\nTable of interesting cases\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------\")\n",
    "    print(\"The following companies had AI scores in the top quartile but returns in the bottom quartile\")\n",
    "    print(\"These are excellent candidates for a qualitative case study in the thesis\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------\")\n",
    "    print(report_table.to_string(index=False))\n",
    "    \n",
    "    return report_table\n",
    "\n",
    "# make sure 'final_reg_data' is loaded and available in the notebook\n",
    "if 'final_reg_data' in locals():\n",
    "    interesting_cases_df = find_interesting_cases(final_reg_data)\n",
    "else:\n",
    "    print(\"\\n\\nError: the 'final_reg_data' dataframe was not found\")\n",
    "    print(\"Please ensure the preceding cells in the notebook have been run to create it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e24de05-95ef-4bc0-8583-f1b38c339ca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SECTOR OUTLIER & QUADRANT ANALYSIS\n",
      "============================================================\n",
      " Objective: For each sector, find firms in all four quadrants of AI Score vs. Return.\n",
      "   (Using top/bottom 25% for high/low thresholds)\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: COMMUNICATION\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 3 firms.\n",
      "Ticker            Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  SIRI SIRIUS XM HOLDINGS INC.  2023            23.666667            167.17%\n",
      " FWONK      Liberty Media Corp  2023            27.000000             36.78%\n",
      "   WLY JOHN WILEY & SONS, INC.  2023            24.000000             21.98%\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 3 firms.\n",
      "Ticker                         Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   TTD                     Trade Desk, Inc.  2020            29.333333            -72.99%\n",
      "  PINS                      PINTEREST, INC.  2020            30.333333            -66.75%\n",
      "   CCO Clear Channel Outdoor Holdings, Inc.  2023            26.000000            -39.96%\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 8 firms.\n",
      "Ticker                    Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  SPOK              Spok Holdings, Inc  2022            18.000000             82.90%\n",
      "   IDT                        IDT CORP  2023            18.333333             57.60%\n",
      "   LYV Live Nation Entertainment, Inc.  2023            17.666667             54.70%\n",
      " FWONK              Liberty Media Corp  2020            17.000000             48.41%\n",
      "   WOW              WideOpenWest, Inc.  2020            17.666667             30.16%\n",
      "... and 3 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 11 firms.\n",
      "Ticker                         Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   WOW                   WideOpenWest, Inc.  2022            18.000000            -67.14%\n",
      "  ANGI                            Angi Inc.  2021            18.000000            -66.71%\n",
      "   CCO Clear Channel Outdoor Holdings, Inc.  2021            17.666667            -62.91%\n",
      "  ANGI                            Angi Inc.  2020            17.666667            -56.60%\n",
      "  AMCX                    AMC Networks Inc.  2022            16.333333            -55.97%\n",
      "... and 6 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: CONSUMER DISCRETIONARY\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 11 firms.\n",
      "Ticker                Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  REAL           TheRealReal, Inc.  2023            28.000000            142.04%\n",
      "     M                Macy's, Inc.  2020            21.000000             67.61%\n",
      "  SFIX            Stitch Fix, Inc.  2020            28.000000             66.65%\n",
      "  AMZN              AMAZON COM INC  2022            32.000000             61.14%\n",
      "   DKS DICK'S SPORTING GOODS, INC.  2022            25.333333             53.01%\n",
      "... and 6 more.\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 41 firms.\n",
      "Ticker           Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  REAL      TheRealReal, Inc.  2021            26.666667            -72.99%\n",
      "  CHGG             CHEGG, INC  2023            36.333333            -72.99%\n",
      "  TSCO TRACTOR SUPPLY CO /DE/  2023            25.000000            -72.99%\n",
      "  SFIX       Stitch Fix, Inc.  2021            28.333333            -72.99%\n",
      "  IRBT            IROBOT CORP  2022            31.333333            -72.99%\n",
      "... and 36 more.\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 51 firms.\n",
      "Ticker                   Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "    TH       Target Hospitality Corp.  2021            16.333333            167.17%\n",
      "   BZH           BEAZER HOMES USA INC  2022            16.666667            127.27%\n",
      "  RICK RCI HOSPITALITY HOLDINGS, INC.  2020            16.000000            123.42%\n",
      "  DECK           DECKERS OUTDOOR CORP  2022            16.000000            116.69%\n",
      "   TOL            Toll Brothers, Inc.  2022            16.000000            101.41%\n",
      "... and 46 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 33 firms.\n",
      "Ticker                  Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   CPS Cooper-Standard Holdings Inc.  2020            17.000000            -65.60%\n",
      "   HBI              Hanesbrands Inc.  2021            16.000000            -65.45%\n",
      "  HELE             HELEN OF TROY LTD  2021            17.000000            -57.19%\n",
      "    TH      Target Hospitality Corp.  2022            17.000000            -54.05%\n",
      "  POWW            Outdoor Holding Co  2020            16.666667            -53.78%\n",
      "... and 28 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: CONSUMER STAPLES\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 7 firms.\n",
      "Ticker               Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   TPB Turning Point Brands, Inc.  2023            20.666667            167.17%\n",
      "    KR                  KROGER CO  2020            25.333333             48.03%\n",
      "   HSY                 HERSHEY CO  2020            24.666667             33.54%\n",
      "   TSN          TYSON FOODS, INC.  2023            26.000000             33.38%\n",
      "   TGT                TARGET CORP  2020            22.333333             20.27%\n",
      "... and 2 more.\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 16 firms.\n",
      "Ticker               Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   ELF        e.l.f. Beauty, Inc.  2023            21.000000            -58.73%\n",
      "    LW Lamb Weston Holdings, Inc.  2023            20.666667            -54.97%\n",
      "    EL ESTEE LAUDER COMPANIES INC  2023            23.333333            -46.18%\n",
      "   TGT                TARGET CORP  2023            21.666667            -42.34%\n",
      "   ADM  Archer-Daniels-Midland Co  2022            24.000000            -39.94%\n",
      "... and 11 more.\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 16 firms.\n",
      "Ticker         Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  SMPL Simply Good Foods Co  2020            16.000000            111.54%\n",
      "   ELF  e.l.f. Beauty, Inc.  2022            16.666667             78.42%\n",
      "  FRPT       Freshpet, Inc.  2022            16.000000             71.04%\n",
      "  ANDE      Andersons, Inc.  2020            16.666667             61.92%\n",
      "  CALM  CAL-MAINE FOODS INC  2023            16.000000             52.19%\n",
      "... and 11 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 12 firms.\n",
      "Ticker               Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "    EL ESTEE LAUDER COMPANIES INC  2022            16.666667            -47.34%\n",
      "   BGS            B&G Foods, Inc.  2021            16.333333            -46.57%\n",
      "   BGS            B&G Foods, Inc.  2023            16.666667            -46.51%\n",
      "  FRPT             Freshpet, Inc.  2021            16.000000            -43.93%\n",
      "   TPB Turning Point Brands, Inc.  2020            16.000000            -33.62%\n",
      "... and 7 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: ENERGY\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 12 firms.\n",
      "Ticker                        Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  NEXT                    NextDecade Corp.  2020            19.000000            167.17%\n",
      "   SEI Solaris Energy Infrastructure, Inc.  2023            21.666667            167.17%\n",
      "  WFRD       Weatherford International plc  2020            23.666667            167.17%\n",
      "   FTI                      TechnipFMC plc  2021            22.666667            121.11%\n",
      "  AROC                      Archrock, Inc.  2022            19.000000             89.08%\n",
      "... and 7 more.\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 10 firms.\n",
      "Ticker             Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   NBR    NABORS INDUSTRIES LTD  2022            27.333333            -59.43%\n",
      "  BOOM          DMC Global Inc.  2023            22.666667            -56.08%\n",
      "   NBR    NABORS INDUSTRIES LTD  2023            27.666667            -37.36%\n",
      "  PTEN PATTERSON UTI ENERGY INC  2022            23.000000            -36.69%\n",
      "  PTEN PATTERSON UTI ENERGY INC  2023            23.666667            -35.38%\n",
      "... and 5 more.\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 12 firms.\n",
      "Ticker                     Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "    SM                     SM Energy Co  2020            16.666667            167.17%\n",
      "   LEU              CENTRUS ENERGY CORP  2023            17.000000            145.24%\n",
      "   HLX HELIX ENERGY SOLUTIONS GROUP INC  2021            17.000000            104.22%\n",
      "  GPOR             GULFPORT ENERGY CORP  2022            16.000000             81.68%\n",
      "   EXE               EXPAND ENERGY Corp  2020            16.333333             77.46%\n",
      "... and 7 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 11 firms.\n",
      "Ticker                Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   REX REX AMERICAN RESOURCES Corp  2021            17.000000            -72.06%\n",
      "  KNTK       Kinetik Holdings Inc.  2021            16.666667            -56.32%\n",
      "  BOOM             DMC Global Inc.  2020            17.000000            -49.73%\n",
      "   REX REX AMERICAN RESOURCES Corp  2023            16.666667            -40.40%\n",
      "   MUR             MURPHY OIL CORP  2023            16.666667            -34.73%\n",
      "... and 6 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: FINANCIALS\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 27 firms.\n",
      "Ticker                Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  CTLP            CANTALOUPE, INC.  2022            27.000000            129.02%\n",
      "  PAYS               Paysign, Inc.  2021            20.666667             72.39%\n",
      "  SLQT           SelectQuote, Inc.  2023            27.333333             71.70%\n",
      "  KNSL Kinsale Capital Group, Inc.  2022            20.666667             55.44%\n",
      "  UNTY      UNITY BANCORP INC /NJ/  2023            21.666667             49.88%\n",
      "... and 22 more.\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 14 firms.\n",
      "Ticker       Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  SLQT  SelectQuote, Inc.  2021            28.000000            -72.99%\n",
      "   RLI           RLI CORP  2023            21.333333            -54.03%\n",
      "  PAYS      Paysign, Inc.  2023            20.666667            -51.23%\n",
      "  MVBF MVB FINANCIAL CORP  2021            22.000000            -42.50%\n",
      "  SLQT  SelectQuote, Inc.  2020            26.000000            -41.76%\n",
      "... and 9 more.\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 32 firms.\n",
      "Ticker                    Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  ACIC AMERICAN COASTAL INSURANCE Corp  2022            17.333333            167.17%\n",
      "   ARR   Armour Residential REIT, Inc.  2022            16.000000            167.17%\n",
      "  TRST           TRUSTCO BANK CORP N Y  2020            16.333333            167.17%\n",
      "   MFA             MFA FINANCIAL, INC.  2021            16.333333            167.17%\n",
      "   MCB Metropolitan Bank Holding Corp.  2020            16.666667             86.95%\n",
      "... and 27 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 38 firms.\n",
      "Ticker                         Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  DCOM Dime Community Bancshares, Inc. /NY/  2022            16.333333            -45.09%\n",
      "  CFFN      Capitol Federal Financial, Inc.  2022            17.000000            -42.02%\n",
      "  KREF   KKR Real Estate Finance Trust Inc.  2022            16.000000            -41.17%\n",
      "   MCY                 MERCURY GENERAL CORP  2021            17.000000            -36.81%\n",
      "   RBB                          RBB Bancorp  2021            17.333333            -35.17%\n",
      "... and 33 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: HEALTH CARE\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 9 firms.\n",
      "Ticker        Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  PODD        INSULET CORP  2023            23.333333             48.81%\n",
      "  OMCL      OMNICELL, INC.  2023            28.000000             40.44%\n",
      "   CNC        CENTENE CORP  2020            27.000000             39.55%\n",
      "  CRVL         CORVEL CORP  2022            23.000000             35.91%\n",
      "   IQV IQVIA HOLDINGS INC.  2020            28.666667             26.79%\n",
      "... and 4 more.\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 14 firms.\n",
      "Ticker    Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  CRVL     CORVEL CORP  2023            24.666667            -64.23%\n",
      "  OPRX OptimizeRx Corp  2021            27.333333            -62.69%\n",
      "  OMCL  OMNICELL, INC.  2021            30.333333            -60.14%\n",
      "   NEO NEOGENOMICS INC  2020            26.000000            -59.20%\n",
      "  OMCL  OMNICELL, INC.  2022            29.333333            -59.00%\n",
      "... and 9 more.\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 28 firms.\n",
      "Ticker                   Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  MDGL MADRIGAL PHARMACEUTICALS, INC.  2021            16.666667            167.17%\n",
      "  VKTX      Viking Therapeutics, Inc.  2022            16.000000            167.17%\n",
      "  VKTX      Viking Therapeutics, Inc.  2021            16.666667            167.17%\n",
      "  CORT       CORCEPT THERAPEUTICS INC  2023            16.666667            167.17%\n",
      "  ENTA     ENANTA PHARMACEUTICALS INC  2020            16.666667            102.35%\n",
      "... and 23 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 23 firms.\n",
      "Ticker                          Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  ZYXI                             ZYNEX INC  2023            17.000000            -72.99%\n",
      "  ENTA            ENANTA PHARMACEUTICALS INC  2022            17.000000            -72.99%\n",
      "  CRBP Corbus Pharmaceuticals Holdings, Inc.  2023            16.666667            -72.99%\n",
      "  CRBP Corbus Pharmaceuticals Holdings, Inc.  2020            16.666667            -72.99%\n",
      "  KURA                   Kura Oncology, Inc.  2023            16.666667            -69.95%\n",
      "... and 18 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: INDUSTRIALS\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 22 firms.\n",
      "Ticker           Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  AXON  AXON ENTERPRISE, INC.  2023            25.333333             88.20%\n",
      "  CMPR           CIMPRESS plc  2022            23.333333             87.23%\n",
      "  PCTY Paylocity Holding Corp  2020            23.666667             82.08%\n",
      "  VICR             VICOR CORP  2023            28.333333             66.66%\n",
      "   RHI       ROBERT HALF INC.  2020            27.333333             63.89%\n",
      "... and 17 more.\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 51 firms.\n",
      "Ticker        Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  TTEC TTEC Holdings, Inc.  2023            31.333333            -72.99%\n",
      "  PRLB      Proto Labs Inc  2020            26.333333            -67.88%\n",
      "  TTEC TTEC Holdings, Inc.  2022            30.666667            -62.06%\n",
      "   DDD     3D SYSTEMS CORP  2022            29.666667            -62.04%\n",
      "  SHYF   SHYFT GROUP, INC.  2022            26.333333            -61.55%\n",
      "... and 46 more.\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 58 firms.\n",
      "Ticker                Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  WLFC   WILLIS LEASE FINANCE CORP  2023            17.333333            167.17%\n",
      "  ATKR                 Atkore Inc.  2020            17.000000            167.17%\n",
      "  HDSN HUDSON TECHNOLOGIES INC /NY  2020            16.666667            167.17%\n",
      "   CAR     AVIS BUDGET GROUP, INC.  2020            17.333333            167.17%\n",
      "  IESC          IES Holdings, Inc.  2023            16.666667            167.17%\n",
      "... and 53 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 47 firms.\n",
      "Ticker                   Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  SPCE  Virgin Galactic Holdings, Inc  2022            16.666667            -72.99%\n",
      "   EAF     GRAFTECH INTERNATIONAL LTD  2022            17.333333            -72.99%\n",
      "  CVGI Commercial Vehicle Group, Inc.  2023            17.333333            -72.99%\n",
      "  SPCE  Virgin Galactic Holdings, Inc  2020            17.333333            -67.42%\n",
      "  HDSN    HUDSON TECHNOLOGIES INC /NY  2023            17.333333            -52.83%\n",
      "... and 42 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: INFORMATION TECHNOLOGY\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 22 firms.\n",
      "Ticker               Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  NVDA                NVIDIA CORP  2022            32.000000            167.17%\n",
      "  FICO            FAIR ISAAC CORP  2023            30.000000            142.63%\n",
      "   AMD ADVANCED MICRO DEVICES INC  2022            29.333333            120.87%\n",
      "  CIEN                 CIENA CORP  2023            28.000000            104.29%\n",
      "  PANW     Palo Alto Networks Inc  2020            32.000000             98.69%\n",
      "... and 17 more.\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 21 firms.\n",
      "Ticker           Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  NVDA            NVIDIA CORP  2023            35.666667            -72.99%\n",
      "  PANW Palo Alto Networks Inc  2022            32.000000            -58.01%\n",
      "  NVDA            NVIDIA CORP  2020            33.333333            -55.96%\n",
      "   MDB          MongoDB, Inc.  2023            30.000000            -52.86%\n",
      "   PAR    PAR TECHNOLOGY CORP  2020            27.000000            -50.53%\n",
      "... and 16 more.\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 36 firms.\n",
      "Ticker                    Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  AAOI   APPLIED OPTOELECTRONICS, INC.  2022            17.000000            167.17%\n",
      "  COMM CommScope Holding Company, Inc.  2023            18.000000            167.17%\n",
      "  SYNA                   SYNAPTICS Inc  2020            18.333333            106.79%\n",
      " BELFB                BEL FUSE INC /NJ  2022            17.666667             90.46%\n",
      "   JBL                       JABIL INC  2022            17.333333             83.56%\n",
      "... and 31 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 22 firms.\n",
      "Ticker                    Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  WOLF                 WOLFSPEED, INC.  2023            19.333333            -72.99%\n",
      "   MEI         METHODE ELECTRONICS INC  2023            18.000000            -72.99%\n",
      "  COMM CommScope Holding Company, Inc.  2022            18.333333            -72.99%\n",
      "  AAOI   APPLIED OPTOELECTRONICS, INC.  2020            17.666667            -69.11%\n",
      "  KODK                EASTMAN KODAK CO  2020            18.000000            -49.03%\n",
      "... and 17 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: MATERIALS\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 9 firms.\n",
      "Ticker                     Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  RYAM RAYONIER ADVANCED MATERIALS INC.  2023            21.000000            122.06%\n",
      "   NEU                   NEWMARKET CORP  2022            19.333333             69.03%\n",
      "   SEE               SEALED AIR CORP/DE  2020            18.333333             62.19%\n",
      "    OI             O-I Glass, Inc. /DE/  2021            18.333333             56.68%\n",
      "   CRS        CARPENTER TECHNOLOGY CORP  2020            18.333333             54.47%\n",
      "... and 4 more.\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 10 firms.\n",
      "Ticker         Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   FMC             FMC CORP  2022            23.333333            -64.43%\n",
      "   SHW  SHERWIN WILLIAMS CO  2020            18.333333            -62.61%\n",
      "   FMC             FMC CORP  2023            21.000000            -33.68%\n",
      "   EMN  EASTMAN CHEMICAL CO  2021            24.666667            -32.66%\n",
      "    OI O-I Glass, Inc. /DE/  2022            18.666667            -32.53%\n",
      "... and 5 more.\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 21 firms.\n",
      "Ticker              Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      " METCB    Ramaco Resources, Inc.  2020            16.666667            167.17%\n",
      "   CRS CARPENTER TECHNOLOGY CORP  2023            16.666667            136.54%\n",
      "  HWKN               HAWKINS INC  2022            16.666667             92.62%\n",
      "  STLD        STEEL DYNAMICS INC  2020            16.666667             63.99%\n",
      "   OLN                 OLIN Corp  2020            16.666667             56.08%\n",
      "... and 16 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 12 firms.\n",
      "Ticker                     Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   OLN                        OLIN Corp  2023            16.666667            -53.76%\n",
      " METCB           Ramaco Resources, Inc.  2021            16.666667            -46.22%\n",
      "  RYAM RAYONIER ADVANCED MATERIALS INC.  2020            16.000000            -46.17%\n",
      "   MOS                        MOSAIC CO  2022            16.000000            -43.97%\n",
      "   KWR             QUAKER CHEMICAL CORP  2020            16.000000            -39.02%\n",
      "... and 7 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: REAL ESTATE\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 18 firms.\n",
      "Ticker                           Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   EXR               Extra Space Storage Inc.  2020            19.000000             54.48%\n",
      "  CBRE                       CBRE GROUP, INC.  2023            18.666667             50.60%\n",
      "   CWK                Cushman & Wakefield plc  2020            19.333333             49.35%\n",
      "   MAA MID AMERICA APARTMENT COMMUNITIES INC.  2020            23.000000             48.95%\n",
      "   PSA                         Public Storage  2020            18.333333             44.94%\n",
      "... and 13 more.\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 14 firms.\n",
      "Ticker                           Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  RMAX                  RE/MAX Holdings, Inc.  2022            19.000000            -60.98%\n",
      "   CWK                Cushman & Wakefield plc  2021            18.666667            -45.92%\n",
      "  RMAX                  RE/MAX Holdings, Inc.  2020            26.000000            -40.11%\n",
      "  RMAX                  RE/MAX Holdings, Inc.  2021            21.333333            -34.65%\n",
      "   MAA MID AMERICA APARTMENT COMMUNITIES INC.  2022            18.000000            -31.87%\n",
      "... and 9 more.\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 25 firms.\n",
      "Ticker                      Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   AHR    American Healthcare REIT, Inc.  2023            16.000000            167.17%\n",
      "   FOR               Forestar Group Inc.  2022            16.000000            120.80%\n",
      "  PLYM    Plymouth Industrial REIT, Inc.  2020            16.000000             78.82%\n",
      "   NSA National Storage Affiliates Trust  2020            16.333333             54.60%\n",
      "   SKT                       TANGER INC.  2022            16.333333             47.97%\n",
      "... and 20 more.\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 20 firms.\n",
      "Ticker                          Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  ILPT Industrial Logistics Properties Trust  2021            16.000000            -72.99%\n",
      "   AHR        American Healthcare REIT, Inc.  2022            16.000000            -49.83%\n",
      "  CHCT        Community Healthcare Trust Inc  2022            16.000000            -38.00%\n",
      "    HR           Healthcare Realty Trust Inc  2022            16.333333            -33.40%\n",
      "   CUZ                COUSINS PROPERTIES INC  2021            16.000000            -31.39%\n",
      "... and 15 more.\n",
      "\n",
      "\n",
      "ANALYZING SECTOR: UTILITIES\n",
      "--------------------------------------------------\n",
      "\n",
      " High AI / High Return (Champions): Found 5 firms.\n",
      "Ticker               Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   PNW PINNACLE WEST CAPITAL CORP  2023                 24.0             26.55%\n",
      "    SO                SOUTHERN CO  2023                 19.0             22.94%\n",
      "   LNT        ALLIANT ENERGY CORP  2023                 19.0             21.75%\n",
      "   OGS              ONE Gas, Inc.  2020                 19.0             21.26%\n",
      "   LNT        ALLIANT ENERGY CORP  2020                 19.0             19.88%\n",
      "\n",
      " High AI / Low Return (AI Hypers): Found 4 firms.\n",
      "Ticker                       Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   BKH              BLACK HILLS CORP /SD/  2022            19.000000            -23.95%\n",
      "   AWK American Water Works Company, Inc.  2022            21.666667            -21.83%\n",
      "   POR  PORTLAND GENERAL ELECTRIC CO /OR/  2022            24.666667            -20.16%\n",
      "   NJR          NEW JERSEY RESOURCES CORP  2022            19.000000            -10.41%\n",
      "\n",
      " Low AI / High Return (Quiet Performers): Found 5 firms.\n",
      "Ticker                   Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "  CWCO    Consolidated Water Co. Ltd.  2022            16.666667             84.32%\n",
      "  CWCO    Consolidated Water Co. Ltd.  2021            16.666667             41.02%\n",
      "  EVRG                   Evergy, Inc.  2023            17.333333             34.02%\n",
      "   AEP AMERICAN ELECTRIC POWER CO INC  2023            16.666667             27.23%\n",
      "   AEP AMERICAN ELECTRIC POWER CO INC  2020            16.666667             17.25%\n",
      "\n",
      " Low AI / Low Return (Laggards): Found 8 firms.\n",
      "Ticker                   Company Name  Year  AI_Cumulative_Score excess_return_12mo\n",
      "   HTO                    H2O AMERICA  2022            16.666667            -31.69%\n",
      "   CWT CALIFORNIA WATER SERVICE GROUP  2022            16.666667            -23.68%\n",
      "  WTRG      Essential Utilities, Inc.  2022            16.666667            -23.25%\n",
      "  CWCO    Consolidated Water Co. Ltd.  2020            16.666667            -17.77%\n",
      "   AEP AMERICAN ELECTRIC POWER CO INC  2022            16.666667            -14.21%\n",
      "... and 3 more.\n",
      "\n",
      "\n",
      "\n",
      " Complete list of all outliers from all sectors exported to: sector_quadrant_analysis_all_outliers.csv \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def run_sector_quadrant_analysis(reg_data, return_col='excess_return_12mo', quantile_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Performs a comprehensive quadrant analysis for every sector to find outliers based on\n",
    "    AI scores and subsequent returns.\n",
    "\n",
    "    Args:\n",
    "        reg_data (pd.DataFrame): The final regression dataframe.\n",
    "        return_col (str): The return column to use for the analysis.\n",
    "        quantile_threshold (float): The percentage to use for defining high/low quantiles (e.g., 0.25 for quartiles).\n",
    "    \"\"\"\n",
    "    print(\" SECTOR OUTLIER & QUADRANT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" Objective: For each sector, find firms in all four quadrants of AI Score vs. Return.\")\n",
    "    print(f\"   (Using top/bottom {int(quantile_threshold*100)}% for high/low thresholds)\")\n",
    "\n",
    "    all_sectors = sorted([s for s in reg_data['Sector'].unique() if pd.notna(s)])\n",
    "    all_outliers_list = []\n",
    "\n",
    "    for sector in all_sectors:\n",
    "        print(f\"\\n\\nANALYZING SECTOR: {sector.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        sector_df = reg_data[reg_data['Sector'] == sector].copy()\n",
    "        if len(sector_df) < 20:\n",
    "            print(\"Skipping due to insufficient observations.\")\n",
    "            continue\n",
    "\n",
    "        # Define high/low thresholds for AI Score and Returns for the current sector\n",
    "        high_ai_thresh = sector_df['AI_Cumulative_Score'].quantile(1 - quantile_threshold)\n",
    "        low_ai_thresh = sector_df['AI_Cumulative_Score'].quantile(quantile_threshold)\n",
    "        high_ret_thresh = sector_df[return_col].quantile(1 - quantile_threshold)\n",
    "        low_ret_thresh = sector_df[return_col].quantile(quantile_threshold)\n",
    "\n",
    "        # Identify firms in each of the four quadrants\n",
    "        quadrants = {\n",
    "            \"High AI / High Return (Champions)\": sector_df[(sector_df['AI_Cumulative_Score'] >= high_ai_thresh) & (sector_df[return_col] >= high_ret_thresh)],\n",
    "            \"High AI / Low Return (AI Hypers)\": sector_df[(sector_df['AI_Cumulative_Score'] >= high_ai_thresh) & (sector_df[return_col] <= low_ret_thresh)],\n",
    "            \"Low AI / High Return (Quiet Performers)\": sector_df[(sector_df['AI_Cumulative_Score'] <= low_ai_thresh) & (sector_df[return_col] >= high_ret_thresh)],\n",
    "            \"Low AI / Low Return (Laggards)\": sector_df[(sector_df['AI_Cumulative_Score'] <= low_ai_thresh) & (sector_df[return_col] <= low_ret_thresh)],\n",
    "        }\n",
    "        \n",
    "        # Display results and collect data for export\n",
    "        for quad_name, quad_df in quadrants.items():\n",
    "            print(f\"\\n {quad_name}: Found {len(quad_df)} firms.\")\n",
    "            if not quad_df.empty:\n",
    "                # Prepare table for display\n",
    "                display_cols = ['Ticker', 'Company Name', 'Year', 'AI_Cumulative_Score', return_col]\n",
    "                report_table = quad_df[display_cols].copy()\n",
    "                # Sort by return to see the most extreme examples\n",
    "                sort_asc = True if 'Low Return' in quad_name else False\n",
    "                report_table = report_table.sort_values(by=return_col, ascending=sort_asc)\n",
    "                report_table[return_col] = report_table[return_col].apply(lambda x: f\"{x:.2%}\")\n",
    "                \n",
    "                print(report_table.head().to_string(index=False))\n",
    "                if len(report_table) > 5:\n",
    "                    print(f\"... and {len(report_table)-5} more.\")\n",
    "\n",
    "                # Add data to our master list for final export\n",
    "                for _, row in quad_df.iterrows():\n",
    "                    all_outliers_list.append({\n",
    "                        'Sector': sector,\n",
    "                        'Quadrant': quad_name,\n",
    "                        'Ticker': row['Ticker'],\n",
    "                        'Company Name': row['Company Name'],\n",
    "                        'Year': row['Year'],\n",
    "                        'AI_Cumulative_Score': row['AI_Cumulative_Score'],\n",
    "                        'Return': row[return_col]\n",
    "                    })\n",
    "    \n",
    "    # Export the complete list of outliers to a single CSV file\n",
    "    if all_outliers_list:\n",
    "        outliers_df = pd.DataFrame(all_outliers_list)\n",
    "        filename = \"sector_quadrant_analysis_all_outliers.csv\"\n",
    "        outliers_df.to_csv(filename, index=False)\n",
    "        print(f\"\\n\\n\\n Complete list of all outliers from all sectors exported to: {filename}\")\n",
    "\n",
    "#================================================================================\n",
    "#                                 RUN THE ANALYSIS\n",
    "#================================================================================\n",
    "# This calls the explorer function using your 'final_reg_data' dataframe.\n",
    "\n",
    "if 'final_reg_data' in locals():\n",
    "    # You can change the quantile_threshold to find more or fewer outliers (e.g., 0.2 for top/bottom 20%)\n",
    "    run_sector_quadrant_analysis(final_reg_data, quantile_threshold=0.25)\n",
    "else:\n",
    "    print(\"\\n\\n ERROR: The 'final_reg_data' dataframe was not found.\")\n",
    "    print(\"Please ensure you have run the preceding cells in your notebook to create it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584612a5-6278-4eaf-8fb3-23bba54839fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9c6d5-4c77-40ad-8eda-51974e918227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e447b-da5b-4973-9201-02f247687d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e389e-629f-471c-ba14-1560996904b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41baf48e-c346-4e1f-ae6d-ee4061526d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6269d0-be1b-4c45-8548-dc00fc86635d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321b35a-d5aa-47ba-b87b-a194e04b42a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99629326-0275-4651-84b2-65d3c779a9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d8ded-e5b0-4db2-a8b0-52f2f5bdb0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe983f-74ee-4fff-a495-a88e6cb897ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680dbd6c-9c01-49fa-bcff-fb19855f17ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f43acd-c47f-47d2-862c-9abcc12214f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f0741-186b-4f5c-9500-9ffccb5975f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240765f1-573c-4634-81a9-6ded0529cf13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72081154-aee1-4a25-bfdf-08fba52382fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b20be-df19-4d2f-84a6-b6816d879202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENDOGENEITY TESTING MODULE FOR AI FACTOR THESIS\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.enum.table import WD_TABLE_ALIGNMENT\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EndogeneityTester:\n",
    "    \"\"\"\n",
    "    Comprehensive endogeneity testing for AI factor thesis.\n",
    "    Tests for:\n",
    "    1. Omitted Variable Bias (management quality proxies)\n",
    "    2. Reverse Causality (lagged performance  current AI metrics)\n",
    "    3. Robustness checks with additional controls\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, output_path):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.df = None\n",
    "        self.results = {}\n",
    "        self.diagnostics = {}\n",
    "        \n",
    "        # Core variables from main analysis\n",
    "        self.y_var = 'excess_return_12mo_annualized'\n",
    "        \n",
    "        # Standard controls (same as main analysis)\n",
    "        self.standard_controls_config = {\n",
    "            'Log Market Cap': 'calc_log_market_cap',\n",
    "            'Price-to-Book': 'calc_price_to_book',\n",
    "            'ROA': 'calc_roa',\n",
    "            'Asset Growth': 'calc_asset_growth',\n",
    "            'Momentum (12-1m)': 'calc_mom_12_1'\n",
    "        }\n",
    "        \n",
    "        # Management quality proxies for omitted variable bias testing\n",
    "        self.management_proxies_config = {\n",
    "            'ROA Volatility': 'calc_roa_volatility',\n",
    "            'Return Volatility': 'calc_return_volatility',\n",
    "            'Earnings Quality': 'calc_earnings_quality',\n",
    "            'Past Performance (t-1)': 'excess_return_12mo_annualized_lag1',\n",
    "            'Past Performance (t-2)': 'excess_return_12mo_annualized_lag2',\n",
    "            'Past ROA (t-1)': 'calc_roa_lag1',\n",
    "            'Revenue Growth Volatility': 'calc_revenue_growth_volatility'\n",
    "        }\n",
    "        \n",
    "        # Lagged variables for reverse causality testing\n",
    "        self.lagged_performance_vars = {\n",
    "            'Lagged Return (t-1)': 'excess_return_12mo_annualized_lag1',\n",
    "            'Lagged Return (t-2)': 'excess_return_12mo_annualized_lag2',\n",
    "            'Lagged ROA (t-1)': 'calc_roa_lag1',\n",
    "            'Lagged ROA (t-2)': 'calc_roa_lag2',\n",
    "            'Lagged Market Cap (t-1)': 'calc_log_market_cap_lag1',\n",
    "            'Lagged Momentum (t-1)': 'calc_mom_12_1_lag1'\n",
    "        }\n",
    "        \n",
    "        self.fe_config = {'year': True, 'sector': True}  # Year + Sector FE\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"Load data and create management quality proxies\"\"\"\n",
    "        print(\" Loading data for endogeneity testing...\")\n",
    "        self.df = pd.read_csv(self.data_path, dtype={'gvkey': str, 'CIK': str})\n",
    "        \n",
    "        # Create AI factor averages (same as main analysis)\n",
    "        self.ai_factors_config = {\n",
    "            'Strategic Depth': ['Strategic Depth_gpt4o_Numeric', 'Strategic Depth_flash1_5_Numeric', 'Strategic Depth_flash2_5_Numeric'],\n",
    "            'AI Sentiment': ['Disclosure Sentiment_gpt4o_Numeric', 'Disclosure Sentiment_flash1_5_Numeric', 'Disclosure Sentiment_flash2_5_Numeric'],\n",
    "            'Risk Own Adoption': ['Risk - Own Adoption_gpt4o_Numeric', 'Risk - Own Adoption_flash1_5_Numeric', 'Risk - Own Adoption_flash2_5_Numeric'],\n",
    "            'Risk External Threats': ['Risk - External Threats_gpt4o_Numeric', 'Risk - External Threats_flash1_5_Numeric', 'Risk - External Threats_flash2_5_Numeric'],\n",
    "            'Risk Non-Adoption': ['Risk - Non-Adoption_gpt4o_Numeric', 'Risk - Non-Adoption_flash1_5_Numeric', 'Risk - Non-Adoption_flash2_5_Numeric'],\n",
    "            'Forward Looking': ['Forward-Looking_gpt4o_Numeric', 'Forward-Looking_flash1_5_Numeric', 'Forward-Looking_flash2_5_Numeric'],\n",
    "            'AI Washing': ['AI Washing Index_gpt4o_Numeric', 'AI Washing Index_flash1_5_Numeric', 'AI Washing Index_flash2_5_Numeric'],\n",
    "            'Talent Investment': ['Talent & Investment_gpt4o_Numeric', 'Talent & Investment_flash1_5_Numeric', 'Talent & Investment_flash2_5_Numeric'],\n",
    "        }\n",
    "        \n",
    "        self.average_ai_factor_cols_map = {}\n",
    "        for factor_name, cols in self.ai_factors_config.items():\n",
    "            available_cols = [col for col in cols if col in self.df.columns]\n",
    "            if available_cols:\n",
    "                avg_col_name = f\"{factor_name.replace(' ', '_')}_Average\"\n",
    "                self.df[avg_col_name] = self.df[available_cols].mean(axis=1, skipna=True)\n",
    "                self.average_ai_factor_cols_map[factor_name] = avg_col_name\n",
    "                print(f\" Created {avg_col_name}: {self.df[avg_col_name].notna().sum():,} obs\")\n",
    "        \n",
    "        # Create management quality proxies\n",
    "        self._create_management_proxies()\n",
    "        \n",
    "        # Create lagged variables\n",
    "        self._create_lagged_variables()\n",
    "        \n",
    "        # Create fixed effects dummies\n",
    "        self._prepare_fixed_effects_dummies()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _create_management_proxies(self):\n",
    "        \"\"\"Create management quality proxy variables\"\"\"\n",
    "        print(\" Creating management quality proxies...\")\n",
    "        \n",
    "        # Sort by firm and year for lagging/rolling calculations\n",
    "        self.df = self.df.sort_values(['gvkey', 'Year'])\n",
    "        \n",
    "        # ROA Volatility (3-year rolling standard deviation)\n",
    "        if 'calc_roa' in self.df.columns:\n",
    "            self.df['calc_roa_volatility'] = self.df.groupby('gvkey')['calc_roa'].rolling(\n",
    "                window=3, min_periods=2\n",
    "            ).std().reset_index(0, drop=True)\n",
    "        \n",
    "        # Return Volatility (past 12 months if daily returns available, or 3-year annual)\n",
    "        if self.y_var in self.df.columns:\n",
    "            self.df['calc_return_volatility'] = self.df.groupby('gvkey')[self.y_var].rolling(\n",
    "                window=3, min_periods=2\n",
    "            ).std().reset_index(0, drop=True)\n",
    "        \n",
    "        # Earnings Quality (persistence of ROA - correlation with lagged ROA)\n",
    "        if 'calc_roa' in self.df.columns:\n",
    "            self.df['calc_roa_lag1'] = self.df.groupby('gvkey')['calc_roa'].shift(1)\n",
    "            # For earnings quality, we'll use the absolute difference from trend\n",
    "            self.df['calc_earnings_quality'] = self.df.groupby('gvkey')['calc_roa'].rolling(\n",
    "                window=3, min_periods=2\n",
    "            ).apply(lambda x: -np.std(x) if len(x) > 1 else np.nan).reset_index(0, drop=True)\n",
    "        \n",
    "        # Revenue Growth Volatility\n",
    "        if 'calc_asset_growth' in self.df.columns:  # Using asset growth as proxy if revenue growth not available\n",
    "            self.df['calc_revenue_growth_volatility'] = self.df.groupby('gvkey')['calc_asset_growth'].rolling(\n",
    "                window=3, min_periods=2\n",
    "            ).std().reset_index(0, drop=True)\n",
    "        \n",
    "        print(\" Management quality proxies created\")\n",
    "\n",
    "    def _create_lagged_variables(self):\n",
    "        \"\"\"Create lagged performance variables\"\"\"\n",
    "        print(\" Creating lagged variables for reverse causality testing...\")\n",
    "        \n",
    "        # Create lagged performance variables\n",
    "        perf_vars_to_lag = [\n",
    "            self.y_var, 'calc_roa', 'calc_log_market_cap', 'calc_mom_12_1'\n",
    "        ]\n",
    "        \n",
    "        for var in perf_vars_to_lag:\n",
    "            if var in self.df.columns:\n",
    "                self.df[f'{var}_lag1'] = self.df.groupby('gvkey')[var].shift(1)\n",
    "                self.df[f'{var}_lag2'] = self.df.groupby('gvkey')[var].shift(2)\n",
    "        \n",
    "        print(\" Lagged variables created\")\n",
    "\n",
    "    def _prepare_fixed_effects_dummies(self):\n",
    "        \"\"\"Create fixed effects dummy variables\"\"\"\n",
    "        # Year fixed effects\n",
    "        self.master_year_fe_cols = []\n",
    "        if 'Year' in self.df.columns:\n",
    "            valid_years = sorted(self.df['Year'].dropna().unique())\n",
    "            if len(valid_years) > 1:\n",
    "                for year_val in valid_years[1:]:  # Drop first year as base\n",
    "                    fe_col_name = f'year_{int(year_val)}'\n",
    "                    self.df[fe_col_name] = (self.df['Year'] == year_val).astype(int)\n",
    "                    self.master_year_fe_cols.append(fe_col_name)\n",
    "        \n",
    "        # Sector fixed effects\n",
    "        self.master_sector_fe_cols = []\n",
    "        if 'Sector' in self.df.columns:\n",
    "            valid_sectors = self.df['Sector'].dropna().unique()\n",
    "            if len(valid_sectors) > 1:\n",
    "                for i, sector_val in enumerate(valid_sectors[1:]):  # Drop first sector as base\n",
    "                    clean_sector_val = str(sector_val).replace(' ', '_').replace('&', 'and').replace('/', '_').replace('-', '_')\n",
    "                    fe_col_name = f'sector_{clean_sector_val}_{i}'\n",
    "                    self.df[fe_col_name] = (self.df['Sector'] == sector_val).astype(int)\n",
    "                    self.master_sector_fe_cols.append(fe_col_name)\n",
    "\n",
    "    def _run_regression_with_controls(self, y_var, x_vars, control_vars=None, include_fe=True):\n",
    "        \"\"\"Helper function to run regression with specified controls\"\"\"\n",
    "        if control_vars is None:\n",
    "            control_vars = []\n",
    "        \n",
    "        # Combine all variables\n",
    "        all_x_vars = x_vars.copy()\n",
    "        all_x_vars.extend(control_vars)\n",
    "        \n",
    "        if include_fe:\n",
    "            all_x_vars.extend(self.master_year_fe_cols)\n",
    "            all_x_vars.extend(self.master_sector_fe_cols)\n",
    "        \n",
    "        # Filter to available columns\n",
    "        all_x_vars = [var for var in all_x_vars if var in self.df.columns]\n",
    "        all_x_vars = list(set(all_x_vars))  # Remove duplicates\n",
    "        \n",
    "        # Prepare regression data\n",
    "        required_cols = [y_var] + all_x_vars + ['gvkey']\n",
    "        reg_data = self.df[required_cols].dropna()\n",
    "        \n",
    "        if len(reg_data) < len(all_x_vars) + 20:  # Need sufficient observations\n",
    "            return None\n",
    "        \n",
    "        # Winsorize variables\n",
    "        vars_to_winsorize = [y_var] + all_x_vars\n",
    "        for var in vars_to_winsorize:\n",
    "            if pd.api.types.is_numeric_dtype(reg_data[var]):\n",
    "                if reg_data[var].notna().sum() > 0 and reg_data[var].nunique() > 1:\n",
    "                    p1, p99 = reg_data[var].quantile([0.01, 0.99])\n",
    "                    if pd.notna(p1) and pd.notna(p99) and p1 != p99:\n",
    "                        reg_data[var] = reg_data[var].clip(lower=p1, upper=p99)\n",
    "        \n",
    "        # Run regression\n",
    "        try:\n",
    "            X = sm.add_constant(reg_data[all_x_vars])\n",
    "            y = reg_data[y_var]\n",
    "            model = OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': reg_data['gvkey']})\n",
    "            return model\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def test_omitted_variable_bias(self):\n",
    "        \"\"\"Test for omitted variable bias using management quality proxies\"\"\"\n",
    "        print(\"\\n Testing for Omitted Variable Bias...\")\n",
    "        \n",
    "        self.results['omitted_variable_bias'] = {}\n",
    "        \n",
    "        # Get available standard controls\n",
    "        std_controls = [col for col in self.standard_controls_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        \n",
    "        # Get available management proxies\n",
    "        mgmt_proxies = [col for col in self.management_proxies_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        \n",
    "        for ai_factor_name, ai_col in self.average_ai_factor_cols_map.items():\n",
    "            if ai_col not in self.df.columns:\n",
    "                continue\n",
    "                \n",
    "            factor_results = {}\n",
    "            \n",
    "            # Model 1: Baseline (AI factor + standard controls + FE)\n",
    "            model_baseline = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_baseline:\n",
    "                factor_results['Baseline'] = model_baseline\n",
    "            \n",
    "            # Model 2: Add management quality proxies\n",
    "            model_with_mgmt = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls + mgmt_proxies,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_with_mgmt:\n",
    "                factor_results['With Management Proxies'] = model_with_mgmt\n",
    "            \n",
    "            # Model 3: Only management proxies (to test their significance)\n",
    "            model_mgmt_only = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=mgmt_proxies,\n",
    "                control_vars=std_controls,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_mgmt_only:\n",
    "                factor_results['Management Proxies Only'] = model_mgmt_only\n",
    "            \n",
    "            if factor_results:\n",
    "                self.results['omitted_variable_bias'][ai_factor_name] = factor_results\n",
    "        \n",
    "        print(\" Omitted Variable Bias testing complete\")\n",
    "\n",
    "    def test_reverse_causality(self):\n",
    "        \"\"\"Test for reverse causality by regressing AI metrics on lagged performance\"\"\"\n",
    "        print(\"\\n Testing for Reverse Causality...\")\n",
    "        \n",
    "        self.results['reverse_causality'] = {}\n",
    "        \n",
    "        # Get available lagged performance variables\n",
    "        lagged_perf_vars = [col for col in self.lagged_performance_vars.values() \n",
    "                           if col and col in self.df.columns]\n",
    "        \n",
    "        # Get available standard controls (for control variables in reverse regression)\n",
    "        std_controls = [col for col in self.standard_controls_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        \n",
    "        for ai_factor_name, ai_col in self.average_ai_factor_cols_map.items():\n",
    "            if ai_col not in self.df.columns:\n",
    "                continue\n",
    "                \n",
    "            factor_results = {}\n",
    "            \n",
    "            # Test 1: AI factor regressed on lagged returns (main reverse causality test)\n",
    "            lagged_return_vars = [col for col in lagged_perf_vars \n",
    "                                 if 'excess_return' in col or 'calc_roa' in col]\n",
    "            \n",
    "            if lagged_return_vars:\n",
    "                model_reverse_main = self._run_regression_with_controls(\n",
    "                    y_var=ai_col,\n",
    "                    x_vars=lagged_return_vars,\n",
    "                    control_vars=[],  # No controls for pure reverse causality test\n",
    "                    include_fe=True\n",
    "                )\n",
    "                if model_reverse_main:\n",
    "                    factor_results['AI ~ Lagged Performance'] = model_reverse_main\n",
    "            \n",
    "            # Test 2: AI factor regressed on lagged returns + controls\n",
    "            if lagged_return_vars:\n",
    "                model_reverse_controls = self._run_regression_with_controls(\n",
    "                    y_var=ai_col,\n",
    "                    x_vars=lagged_return_vars,\n",
    "                    control_vars=std_controls,\n",
    "                    include_fe=True\n",
    "                )\n",
    "                if model_reverse_controls:\n",
    "                    factor_results['AI ~ Lagged Performance + Controls'] = model_reverse_controls\n",
    "            \n",
    "            # Test 3: Forward-looking test - current performance on lagged AI\n",
    "            # (Create lagged AI variables for this test)\n",
    "            ai_col_lag1 = f'{ai_col}_lag1'\n",
    "            if ai_col_lag1 not in self.df.columns:\n",
    "                self.df[ai_col_lag1] = self.df.groupby('gvkey')[ai_col].shift(1)\n",
    "            \n",
    "            if ai_col_lag1 in self.df.columns:\n",
    "                model_forward = self._run_regression_with_controls(\n",
    "                    y_var=self.y_var,\n",
    "                    x_vars=[ai_col_lag1],\n",
    "                    control_vars=std_controls,\n",
    "                    include_fe=True\n",
    "                )\n",
    "                if model_forward:\n",
    "                    factor_results['Current Return ~ Lagged AI'] = model_forward\n",
    "            \n",
    "            if factor_results:\n",
    "                self.results['reverse_causality'][ai_factor_name] = factor_results\n",
    "        \n",
    "        print(\" Reverse Causality testing complete\")\n",
    "\n",
    "    def test_coefficient_stability(self):\n",
    "        \"\"\"Test coefficient stability across different specifications\"\"\"\n",
    "        print(\"\\n Testing Coefficient Stability...\")\n",
    "        \n",
    "        self.results['coefficient_stability'] = {}\n",
    "        \n",
    "        std_controls = [col for col in self.standard_controls_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        mgmt_proxies = [col for col in self.management_proxies_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        \n",
    "        for ai_factor_name, ai_col in self.average_ai_factor_cols_map.items():\n",
    "            if ai_col not in self.df.columns:\n",
    "                continue\n",
    "                \n",
    "            factor_results = {}\n",
    "            \n",
    "            # Specification 1: AI factor only + FE\n",
    "            model_fe_only = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=[],\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_fe_only:\n",
    "                factor_results['FE Only'] = model_fe_only\n",
    "            \n",
    "            # Specification 2: + Standard controls\n",
    "            model_std = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_std:\n",
    "                factor_results['+ Standard Controls'] = model_std\n",
    "            \n",
    "            # Specification 3: + Management proxies\n",
    "            model_mgmt = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls + mgmt_proxies,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_mgmt:\n",
    "                factor_results['+ Management Proxies'] = model_mgmt\n",
    "            \n",
    "            # Specification 4: No fixed effects (to test FE impact)\n",
    "            model_no_fe = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls,\n",
    "                include_fe=False\n",
    "            )\n",
    "            if model_no_fe:\n",
    "                factor_results['No Fixed Effects'] = model_no_fe\n",
    "            \n",
    "            if factor_results:\n",
    "                self.results['coefficient_stability'][ai_factor_name] = factor_results\n",
    "        \n",
    "        print(\" Coefficient Stability testing complete\")\n",
    "\n",
    "    def run_all_endogeneity_tests(self):\n",
    "        \"\"\"Run all endogeneity tests\"\"\"\n",
    "        print(\"\\n Running comprehensive endogeneity testing...\")\n",
    "        \n",
    "        self.test_omitted_variable_bias()\n",
    "        self.test_reverse_causality()\n",
    "        self.test_coefficient_stability()\n",
    "        \n",
    "        print(\" All endogeneity tests complete\")\n",
    "        return self\n",
    "\n",
    "    def _get_significance_stars(self, pvalue):\n",
    "        \"\"\"Get significance stars for p-values\"\"\"\n",
    "        if pd.isna(pvalue):\n",
    "            return \"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"***\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"**\"\n",
    "        elif pvalue < 0.10:\n",
    "            return \"*\"\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _create_endogeneity_table(self, doc, title, results_dict, table_num):\n",
    "        \"\"\"Create a table for endogeneity test results\"\"\"\n",
    "        doc.add_heading(f\"Table {table_num}: {title}\", level=2)\n",
    "        \n",
    "        if not results_dict:\n",
    "            doc.add_paragraph(\"No results available for this test.\")\n",
    "            return\n",
    "        \n",
    "        # Get all AI factors and model specifications\n",
    "        ai_factors = list(results_dict.keys())\n",
    "        if not ai_factors:\n",
    "            doc.add_paragraph(\"No AI factors available for this test.\")\n",
    "            return\n",
    "        \n",
    "        # Get model specifications from first AI factor\n",
    "        model_specs = list(results_dict[ai_factors[0]].keys())\n",
    "        \n",
    "        # Create table\n",
    "        table = doc.add_table(rows=1, cols=1 + len(model_specs))\n",
    "        table.style = 'TableGrid'\n",
    "        table.alignment = WD_TABLE_ALIGNMENT.CENTER\n",
    "        \n",
    "        # Header row\n",
    "        hdr_cells = table.rows[0].cells\n",
    "        hdr_cells[0].text = \"AI Factor\"\n",
    "        hdr_cells[0].paragraphs[0].runs[0].font.bold = True\n",
    "        hdr_cells[0].paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        for i, spec in enumerate(model_specs):\n",
    "            hdr_cells[i+1].text = f\"({i+1})\\n{spec}\"\n",
    "            hdr_cells[i+1].paragraphs[0].runs[0].font.bold = True\n",
    "            hdr_cells[i+1].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "            hdr_cells[i+1].paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        # Add rows for each AI factor\n",
    "        for ai_factor in ai_factors:\n",
    "            row = table.add_row().cells\n",
    "            row[0].text = ai_factor\n",
    "            row[0].paragraphs[0].runs[0].font.bold = True\n",
    "            row[0].paragraphs[0].runs[0].font.size = Pt(9)\n",
    "            \n",
    "            factor_results = results_dict[ai_factor]\n",
    "            for i, spec in enumerate(model_specs):\n",
    "                cell = row[i+1]\n",
    "                \n",
    "                if spec in factor_results and factor_results[spec] is not None:\n",
    "                    model = factor_results[spec]\n",
    "                    \n",
    "                    # For reverse causality tests, look for AI factor coefficient\n",
    "                    # For other tests, look for the main variable of interest\n",
    "                    if 'reverse_causality' in title.lower():\n",
    "                        # In reverse causality, we're looking at lagged performance coefficients\n",
    "                        # Get the first significant lagged variable\n",
    "                        significant_coefs = []\n",
    "                        for param_name in model.params.index:\n",
    "                            if 'lag' in param_name.lower() and param_name != 'const':\n",
    "                                coef = model.params[param_name]\n",
    "                                pval = model.pvalues[param_name]\n",
    "                                se = model.bse[param_name]\n",
    "                                stars = self._get_significance_stars(pval)\n",
    "                                significant_coefs.append(f\"{coef:.3f}{stars}\")\n",
    "                        \n",
    "                        if significant_coefs:\n",
    "                            cell.text = significant_coefs[0]  # Show first significant coefficient\n",
    "                        else:\n",
    "                            cell.text = \"---\"\n",
    "                    else:\n",
    "                        # For other tests, look for the AI factor coefficient\n",
    "                        ai_col = self.average_ai_factor_cols_map.get(ai_factor)\n",
    "                        if ai_col and ai_col in model.params:\n",
    "                            coef = model.params[ai_col]\n",
    "                            pval = model.pvalues[ai_col]\n",
    "                            se = model.bse[ai_col]\n",
    "                            stars = self._get_significance_stars(pval)\n",
    "                            \n",
    "                            # Main coefficient\n",
    "                            p_coef = cell.paragraphs[0]\n",
    "                            p_coef.clear()\n",
    "                            run_coef = p_coef.add_run(f\"{coef:.3f}{stars}\")\n",
    "                            run_coef.font.size = Pt(9)\n",
    "                            p_coef.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "                            \n",
    "                            # Standard error\n",
    "                            p_se = cell.add_paragraph()\n",
    "                            run_se = p_se.add_run(f\"({se:.3f})\")\n",
    "                            run_se.font.italic = True\n",
    "                            run_se.font.size = Pt(8)\n",
    "                            p_se.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "                        else:\n",
    "                            cell.text = \"---\"\n",
    "                else:\n",
    "                    cell.text = \"---\"\n",
    "                \n",
    "                cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        \n",
    "        # Add model statistics\n",
    "        stats_to_show = [\n",
    "            (\"Observations\", \"nobs\"),\n",
    "            (\"R-squared\", \"rsquared\"),\n",
    "            (\"F-statistic\", \"fvalue\")\n",
    "        ]\n",
    "        \n",
    "        for stat_name, stat_attr in stats_to_show:\n",
    "            row = table.add_row().cells\n",
    "            row[0].text = stat_name\n",
    "            row[0].paragraphs[0].runs[0].font.bold = True\n",
    "            row[0].paragraphs[0].runs[0].font.size = Pt(9)\n",
    "            \n",
    "            for i, spec in enumerate(model_specs):\n",
    "                cell = row[i+1]\n",
    "                \n",
    "                # Get first AI factor's results for this spec to show statistics\n",
    "                first_ai_factor = ai_factors[0]\n",
    "                if (spec in results_dict[first_ai_factor] and \n",
    "                    results_dict[first_ai_factor][spec] is not None):\n",
    "                    \n",
    "                    model = results_dict[first_ai_factor][spec]\n",
    "                    \n",
    "                    if hasattr(model, stat_attr):\n",
    "                        stat_val = getattr(model, stat_attr)\n",
    "                        if stat_attr == \"nobs\":\n",
    "                            cell.text = f\"{int(stat_val):,}\"\n",
    "                        else:\n",
    "                            cell.text = f\"{stat_val:.3f}\"\n",
    "                    else:\n",
    "                        cell.text = \"N/A\"\n",
    "                else:\n",
    "                    cell.text = \"---\"\n",
    "                \n",
    "                cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "                cell.paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        doc.add_paragraph()\n",
    "\n",
    "    def create_endogeneity_document(self):\n",
    "        \"\"\"Create the endogeneity testing document\"\"\"\n",
    "        print(\"\\n Creating endogeneity testing document...\")\n",
    "        \n",
    "        doc = Document()\n",
    "        \n",
    "        # Set style\n",
    "        style = doc.styles['Normal']\n",
    "        style.font.name = 'Times New Roman'\n",
    "        style.font.size = Pt(10)\n",
    "        \n",
    "        # Title\n",
    "        title = doc.add_heading('Endogeneity Testing for AI Factor Analysis', level=0)\n",
    "        title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        \n",
    "        # Subtitle\n",
    "        subtitle = doc.add_paragraph()\n",
    "        subtitle_run = subtitle.add_run('Testing for Omitted Variable Bias and Reverse Causality')\n",
    "        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        subtitle_run.font.size = Pt(12)\n",
    "        subtitle_run.italic = True\n",
    "        \n",
    "        # Executive Summary\n",
    "        doc.add_heading('Executive Summary', level=1)\n",
    "        summary_text = \"\"\"\n",
    "        This document presents comprehensive endogeneity testing for the AI factor analysis. We test for two main sources of endogeneity:\n",
    "        \n",
    "        1. Omitted Variable Bias: We test whether the inclusion of management quality proxies (ROA volatility, past performance, earnings quality) significantly changes our AI factor coefficients.\n",
    "        \n",
    "        2. Reverse Causality: We test whether past performance predicts current AI disclosure patterns, which would suggest that our results are driven by firms' past success rather than genuine AI adoption effects.\n",
    "        \n",
    "        3. Coefficient Stability: We examine how sensitive our main results are to different model specifications and control variable sets.\n",
    "        \"\"\"\n",
    "        doc.add_paragraph(summary_text)\n",
    "        \n",
    "        # Test 1: Omitted Variable Bias\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Test 1: Omitted Variable Bias', level=1)\n",
    "        \n",
    "        doc.add_paragraph(\"\"\"\n",
    "        We test for omitted variable bias by including management quality proxies in our regressions. \n",
    "        If management quality is correlated with both AI disclosure patterns and future returns, \n",
    "        our baseline results may be biased. We include the following management quality proxies:\n",
    "        \n",
    "         ROA Volatility (3-year rolling standard deviation)\n",
    "         Return Volatility (3-year rolling standard deviation)  \n",
    "         Earnings Quality (negative of ROA volatility)\n",
    "         Past Performance (t-1 and t-2 lagged returns)\n",
    "         Revenue Growth Volatility\n",
    "        \n",
    "        If coefficients remain stable after including these proxies, it suggests our results are robust to this form of omitted variable bias.\n",
    "        \"\"\")\n",
    "        \n",
    "        if 'omitted_variable_bias' in self.results:\n",
    "            self._create_endogeneity_table(\n",
    "                doc, \n",
    "                \"Omitted Variable Bias Test\", \n",
    "                self.results['omitted_variable_bias'], \n",
    "                \"1\"\n",
    "            )\n",
    "        \n",
    "        # Test 2: Reverse Causality\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Test 2: Reverse Causality', level=1)\n",
    "        \n",
    "        doc.add_paragraph(\"\"\"\n",
    "        We test for reverse causality by regressing current AI metrics on lagged performance measures. \n",
    "        If past performance strongly predicts current AI disclosure patterns, it suggests that our \n",
    "        results may be driven by successful firms being more likely to discuss AI, rather than \n",
    "        AI disclosure predicting future success.\n",
    "        \n",
    "        We run the following tests:\n",
    "        \n",
    "         AI Factor ~ Lagged Returns (t-1, t-2) + Fixed Effects\n",
    "         AI Factor ~ Lagged Returns + Controls + Fixed Effects  \n",
    "         Current Returns ~ Lagged AI Factor + Controls (forward-looking test)\n",
    "        \n",
    "        Strong significance in the first two tests would indicate reverse causality concerns.\n",
    "        \"\"\")\n",
    "        \n",
    "        if 'reverse_causality' in self.results:\n",
    "            self._create_endogeneity_table(\n",
    "                doc, \n",
    "                \"Reverse Causality Test\", \n",
    "                self.results['reverse_causality'], \n",
    "                \"2\"\n",
    "            )\n",
    "        \n",
    "        # Test 3: Coefficient Stability\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Test 3: Coefficient Stability', level=1)\n",
    "        \n",
    "        doc.add_paragraph(\"\"\"\n",
    "        We examine the stability of our AI factor coefficients across different model specifications.\n",
    "        Large changes in coefficients when controls are added may indicate endogeneity concerns.\n",
    "        \n",
    "        Model specifications:\n",
    "        \n",
    "         AI Factor + Fixed Effects Only\n",
    "         AI Factor + Standard Controls + Fixed Effects\n",
    "         AI Factor + Standard Controls + Management Proxies + Fixed Effects\n",
    "         AI Factor + Standard Controls (No Fixed Effects)\n",
    "        \n",
    "        Stable coefficients across specifications suggest robustness to endogeneity.\n",
    "        \"\"\")\n",
    "        \n",
    "        if 'coefficient_stability' in self.results:\n",
    "            self._create_endogeneity_table(\n",
    "                doc, \n",
    "                \"Coefficient Stability Test\", \n",
    "                self.results['coefficient_stability'], \n",
    "                \"3\"\n",
    "            )\n",
    "        \n",
    "        # Interpretation Guidelines\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Interpretation Guidelines', level=1)\n",
    "        \n",
    "        doc.add_paragraph(\"\"\"\n",
    "        Interpreting Endogeneity Test Results:\n",
    "        \n",
    "        1. Omitted Variable Bias:\n",
    "            Small changes in AI factor coefficients when management proxies are added suggest robustness\n",
    "            Large changes (>25% coefficient change) may indicate omitted variable bias\n",
    "            Insignificant management proxies suggest they don't confound our results\n",
    "        \n",
    "        2. Reverse Causality:\n",
    "            Strong significance of lagged performance predicting current AI metrics indicates concern\n",
    "            Weak or insignificant relationships suggest limited reverse causality\n",
    "            Forward-looking tests (current returns ~ lagged AI) should show significance if AI truly predicts performance\n",
    "        \n",
    "        3. Coefficient Stability:\n",
    "            Stable coefficients across specifications indicate robustness\n",
    "            Large changes when controls are added suggest potential endogeneity\n",
    "            Comparison with/without fixed effects shows importance of unobserved heterogeneity control\n",
    "        \n",
    "        Overall Assessment:\n",
    "        If tests show: (1) stable coefficients when management proxies added, (2) weak reverse causality, \n",
    "        and (3) stable coefficients across specifications, then endogeneity concerns are likely limited.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Detailed Results Tables in Appendix\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Appendix: Detailed Regression Results', level=1)\n",
    "        \n",
    "        self._create_detailed_appendix_tables(doc)\n",
    "        \n",
    "        # General Notes\n",
    "        doc.add_paragraph()\n",
    "        notes_para = doc.add_paragraph()\n",
    "        notes_run = notes_para.add_run(\n",
    "            \"Notes: All regressions use firm-clustered standard errors. Variables are winsorized at 1%/99% levels. \"\n",
    "            \"Fixed effects include year and sector dummies. Management quality proxies include ROA volatility, \"\n",
    "            \"return volatility, earnings quality, and lagged performance measures. \"\n",
    "            \"*, **, *** indicate significance at 10%, 5%, and 1% levels respectively.\"\n",
    "        )\n",
    "        notes_run.font.size = Pt(8)\n",
    "        notes_run.italic = True\n",
    "        \n",
    "        try:\n",
    "            doc.save(self.output_path)\n",
    "            print(f\" Endogeneity testing document saved to: {self.output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error saving document: {e}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _create_detailed_appendix_tables(self, doc):\n",
    "        \"\"\"Create detailed regression tables for appendix\"\"\"\n",
    "        \n",
    "        # Management Quality Variables Summary\n",
    "        doc.add_heading('A. Management Quality Proxy Variables', level=2)\n",
    "        \n",
    "        mgmt_vars_desc = {\n",
    "            'ROA Volatility': 'Three-year rolling standard deviation of Return on Assets',\n",
    "            'Return Volatility': 'Three-year rolling standard deviation of annual returns', \n",
    "            'Earnings Quality': 'Negative of ROA volatility (higher = more stable earnings)',\n",
    "            'Past Performance (t-1)': 'One-year lagged excess returns',\n",
    "            'Past Performance (t-2)': 'Two-year lagged excess returns',\n",
    "            'Revenue Growth Volatility': 'Three-year rolling standard deviation of asset growth'\n",
    "        }\n",
    "        \n",
    "        # Create description table\n",
    "        table = doc.add_table(rows=1, cols=2)\n",
    "        table.style = 'TableGrid'\n",
    "        \n",
    "        hdr_cells = table.rows[0].cells\n",
    "        hdr_cells[0].text = \"Variable\"\n",
    "        hdr_cells[1].text = \"Description\"\n",
    "        for cell in hdr_cells:\n",
    "            cell.paragraphs[0].runs[0].font.bold = True\n",
    "            cell.paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        for var_name, description in mgmt_vars_desc.items():\n",
    "            row = table.add_row().cells\n",
    "            row[0].text = var_name\n",
    "            row[1].text = description\n",
    "            for cell in row:\n",
    "                cell.paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        # Summary Statistics for Management Proxies\n",
    "        doc.add_paragraph()\n",
    "        doc.add_heading('B. Summary Statistics for Management Quality Proxies', level=2)\n",
    "        \n",
    "        mgmt_proxy_cols = [col for col in self.management_proxies_config.values() \n",
    "                          if col and col in self.df.columns]\n",
    "        \n",
    "        if mgmt_proxy_cols:\n",
    "            self._create_summary_stats_table(doc, mgmt_proxy_cols, \"Management Quality Proxies\")\n",
    "        \n",
    "        # Correlation Matrix\n",
    "        doc.add_paragraph()\n",
    "        doc.add_heading('C. Correlation Matrix: AI Factors vs Management Proxies', level=2)\n",
    "        \n",
    "        ai_cols = list(self.average_ai_factor_cols_map.values())\n",
    "        all_vars_for_corr = ai_cols + mgmt_proxy_cols\n",
    "        all_vars_available = [col for col in all_vars_for_corr if col in self.df.columns]\n",
    "        \n",
    "        if len(all_vars_available) > 1:\n",
    "            self._create_correlation_table(doc, all_vars_available, \"AI Factors and Management Proxies\")\n",
    "\n",
    "    def _create_summary_stats_table(self, doc, variables, title):\n",
    "        \"\"\"Create summary statistics table for specified variables\"\"\"\n",
    "        \n",
    "        summary_data = []\n",
    "        for var in variables:\n",
    "            if var in self.df.columns:\n",
    "                data = self.df[var].dropna()\n",
    "                if not data.empty:\n",
    "                    var_display_name = None\n",
    "                    # Find display name from config\n",
    "                    for display_name, col_name in self.management_proxies_config.items():\n",
    "                        if col_name == var:\n",
    "                            var_display_name = display_name\n",
    "                            break\n",
    "                    \n",
    "                    if var_display_name is None:\n",
    "                        var_display_name = var\n",
    "                    \n",
    "                    summary_data.append({\n",
    "                        'Variable': var_display_name,\n",
    "                        'N': len(data),\n",
    "                        'Mean': data.mean(),\n",
    "                        'Std Dev': data.std(),\n",
    "                        'Min': data.min(),\n",
    "                        'Median': data.median(),\n",
    "                        'Max': data.max()\n",
    "                    })\n",
    "        \n",
    "        if not summary_data:\n",
    "            doc.add_paragraph(f\"No data available for {title} summary statistics.\")\n",
    "            return\n",
    "        \n",
    "        table = doc.add_table(rows=1, cols=7)\n",
    "        table.style = 'TableGrid'\n",
    "        table.alignment = WD_TABLE_ALIGNMENT.CENTER\n",
    "        \n",
    "        headers = ['Variable', 'N', 'Mean', 'Std Dev', 'Min', 'Median', 'Max']\n",
    "        for i, header in enumerate(headers):\n",
    "            table.cell(0, i).text = header\n",
    "            table.cell(0, i).paragraphs[0].runs[0].font.bold = True\n",
    "            table.cell(0, i).paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        for item in summary_data:\n",
    "            row = table.add_row().cells\n",
    "            row[0].text = item['Variable']\n",
    "            row[1].text = f\"{item['N']:,}\"\n",
    "            row[2].text = f\"{item['Mean']:.3f}\"\n",
    "            row[3].text = f\"{item['Std Dev']:.3f}\"\n",
    "            row[4].text = f\"{item['Min']:.3f}\"\n",
    "            row[5].text = f\"{item['Median']:.3f}\"\n",
    "            row[6].text = f\"{item['Max']:.3f}\"\n",
    "            \n",
    "            for cell_idx in range(len(row)):\n",
    "                if cell_idx > 0:\n",
    "                    row[cell_idx].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "                for run in row[cell_idx].paragraphs[0].runs:\n",
    "                    run.font.size = Pt(9)\n",
    "\n",
    "    def _create_correlation_table(self, doc, variables, title):\n",
    "        \"\"\"Create correlation table for specified variables\"\"\"\n",
    "        \n",
    "        if len(variables) < 2:\n",
    "            doc.add_paragraph(f\"Not enough variables for {title} correlation matrix.\")\n",
    "            return\n",
    "        \n",
    "        # Get variable display names\n",
    "        var_display_names = []\n",
    "        for var in variables:\n",
    "            display_name = var\n",
    "            # Check AI factors\n",
    "            for ai_name, ai_col in self.average_ai_factor_cols_map.items():\n",
    "                if ai_col == var:\n",
    "                    display_name = ai_name\n",
    "                    break\n",
    "            # Check management proxies\n",
    "            for mgmt_name, mgmt_col in self.management_proxies_config.items():\n",
    "                if mgmt_col == var:\n",
    "                    display_name = mgmt_name\n",
    "                    break\n",
    "            var_display_names.append(display_name)\n",
    "        \n",
    "        # Calculate correlations\n",
    "        vars_in_df = [var for var in variables if var in self.df.columns]\n",
    "        \n",
    "        if len(vars_in_df) < 2:\n",
    "            doc.add_paragraph(f\"Not enough variables available in data for {title} correlation matrix.\")\n",
    "            return\n",
    "        \n",
    "        corr_df = self.df[vars_in_df].corr()\n",
    "        \n",
    "        # Create table\n",
    "        n_vars = len(vars_in_df)\n",
    "        table = doc.add_table(rows=n_vars + 1, cols=n_vars + 1)\n",
    "        table.style = 'TableGrid'\n",
    "        \n",
    "        # Header row and column\n",
    "        table.cell(0, 0).text = \"Variable\"\n",
    "        table.cell(0, 0).paragraphs[0].runs[0].font.bold = True\n",
    "        table.cell(0, 0).paragraphs[0].runs[0].font.size = Pt(8)\n",
    "        \n",
    "        for i, var_name in enumerate(var_display_names):\n",
    "            # Column headers\n",
    "            table.cell(0, i + 1).text = f\"({i+1})\"\n",
    "            table.cell(0, i + 1).paragraphs[0].runs[0].font.bold = True\n",
    "            table.cell(0, i + 1).paragraphs[0].runs[0].font.size = Pt(8)\n",
    "            table.cell(0, i + 1).paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "            \n",
    "            # Row headers\n",
    "            table.cell(i + 1, 0).text = f\"({i+1}) {var_name}\"\n",
    "            table.cell(i + 1, 0).paragraphs[0].runs[0].font.bold = True\n",
    "            table.cell(i + 1, 0).paragraphs[0].runs[0].font.size = Pt(8)\n",
    "        \n",
    "        # Fill correlation values\n",
    "        for i in range(n_vars):\n",
    "            for j in range(n_vars):\n",
    "                corr_val = corr_df.iloc[i, j]\n",
    "                table.cell(i + 1, j + 1).text = f\"{corr_val:.2f}\"\n",
    "                table.cell(i + 1, j + 1).paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "                table.cell(i + 1, j + 1).paragraphs[0].runs[0].font.size = Pt(8)\n",
    "\n",
    "    def run_complete_endogeneity_analysis(self):\n",
    "        \"\"\"Run complete endogeneity analysis and generate document\"\"\"\n",
    "        print(\" RUNNING COMPREHENSIVE ENDOGENEITY TESTING\")\n",
    "        \n",
    "        try:\n",
    "            self.load_and_prepare_data()\n",
    "            self.run_all_endogeneity_tests()\n",
    "            self.create_endogeneity_document()\n",
    "            \n",
    "            # Print summary results\n",
    "            self._print_endogeneity_summary()\n",
    "            \n",
    "            return self.results\n",
    "        except Exception as e:\n",
    "            print(f\" Critical error in endogeneity testing: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def _print_endogeneity_summary(self):\n",
    "        \"\"\"Print summary of endogeneity test results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" ENDOGENEITY TESTING SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Omitted Variable Bias Summary\n",
    "        if 'omitted_variable_bias' in self.results:\n",
    "            print(\"\\n1. OMITTED VARIABLE BIAS TEST:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for ai_factor, results in self.results['omitted_variable_bias'].items():\n",
    "                if 'Baseline' in results and 'With Management Proxies' in results:\n",
    "                    baseline_model = results['Baseline']\n",
    "                    mgmt_model = results['With Management Proxies']\n",
    "                    \n",
    "                    ai_col = self.average_ai_factor_cols_map.get(ai_factor)\n",
    "                    if ai_col and ai_col in baseline_model.params and ai_col in mgmt_model.params:\n",
    "                        baseline_coef = baseline_model.params[ai_col]\n",
    "                        mgmt_coef = mgmt_model.params[ai_col]\n",
    "                        \n",
    "                        if baseline_coef != 0:\n",
    "                            pct_change = ((mgmt_coef - baseline_coef) / abs(baseline_coef)) * 100\n",
    "                            print(f\"{ai_factor}:\")\n",
    "                            print(f\"  Baseline coef: {baseline_coef:.3f}\")\n",
    "                            print(f\"  With mgmt proxies: {mgmt_coef:.3f}\")\n",
    "                            print(f\"  Change: {pct_change:.1f}%\")\n",
    "                            \n",
    "                            if abs(pct_change) < 25:\n",
    "                                print(f\"  Assessment:  ROBUST (small change)\")\n",
    "                            else:\n",
    "                                print(f\"  Assessment:  POTENTIAL BIAS (large change)\")\n",
    "        \n",
    "        # Reverse Causality Summary\n",
    "        if 'reverse_causality' in self.results:\n",
    "            print(\"\\n2. REVERSE CAUSALITY TEST:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for ai_factor, results in self.results['reverse_causality'].items():\n",
    "                print(f\"{ai_factor}:\")\n",
    "                \n",
    "                if 'AI ~ Lagged Performance' in results:\n",
    "                    model = results['AI ~ Lagged Performance']\n",
    "                    significant_lags = []\n",
    "                    \n",
    "                    for param_name in model.params.index:\n",
    "                        if 'lag' in param_name.lower() and param_name != 'const':\n",
    "                            pval = model.pvalues[param_name]\n",
    "                            if pval < 0.05:\n",
    "                                significant_lags.append(param_name)\n",
    "                    \n",
    "                    if significant_lags:\n",
    "                        print(f\"   REVERSE CAUSALITY CONCERN: {len(significant_lags)} significant lagged vars\")\n",
    "                    else:\n",
    "                        print(f\"   LIMITED REVERSE CAUSALITY: No significant lagged performance\")\n",
    "                \n",
    "                if 'Current Return ~ Lagged AI' in results:\n",
    "                    model = results['Current Return ~ Lagged AI']\n",
    "                    ai_col_lag = f\"{self.average_ai_factor_cols_map.get(ai_factor)}_lag1\"\n",
    "                    \n",
    "                    if ai_col_lag in model.params:\n",
    "                        pval = model.pvalues[ai_col_lag]\n",
    "                        coef = model.params[ai_col_lag]\n",
    "                        \n",
    "                        if pval < 0.05:\n",
    "                            print(f\"   FORWARD PREDICTIVE POWER: Lagged AI predicts returns (coef: {coef:.3f})\")\n",
    "                        else:\n",
    "                            print(f\"   WEAK FORWARD PREDICTION: Lagged AI doesn't predict returns\")\n",
    "        \n",
    "        # Coefficient Stability Summary\n",
    "        if 'coefficient_stability' in self.results:\n",
    "            print(\"\\n3. COEFFICIENT STABILITY TEST:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for ai_factor, results in self.results['coefficient_stability'].items():\n",
    "                if 'FE Only' in results and '+ Management Proxies' in results:\n",
    "                    fe_model = results['FE Only']\n",
    "                    full_model = results['+ Management Proxies']\n",
    "                    \n",
    "                    ai_col = self.average_ai_factor_cols_map.get(ai_factor)\n",
    "                    if ai_col and ai_col in fe_model.params and ai_col in full_model.params:\n",
    "                        fe_coef = fe_model.params[ai_col]\n",
    "                        full_coef = full_model.params[ai_col]\n",
    "                        \n",
    "                        if fe_coef != 0:\n",
    "                            pct_change = ((full_coef - fe_coef) / abs(fe_coef)) * 100\n",
    "                            print(f\"{ai_factor}:\")\n",
    "                            print(f\"  FE only: {fe_coef:.3f}\")\n",
    "                            print(f\"  Full controls: {full_coef:.3f}\")\n",
    "                            print(f\"  Stability: {100 - abs(pct_change):.1f}%\")\n",
    "                            \n",
    "                            if abs(pct_change) < 25:\n",
    "                                print(f\"  Assessment:  STABLE\")\n",
    "                            else:\n",
    "                                print(f\"  Assessment:  UNSTABLE\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" OVERALL ENDOGENEITY ASSESSMENT:\")\n",
    "        print(\"    = Low endogeneity concern\")\n",
    "        print(\"    = Potential endogeneity concern\") \n",
    "        print(\"   Results should be interpreted with appropriate caution\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Usage example and main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your file paths\n",
    "    data_path = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/final_clean_dataset_filtered_with_corrected_factor_loadings.csv\"\n",
    "    output_dir = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/EndogeneityTesting/\"\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\" ERROR: Data file not found at {data_path}\")\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file_path = os.path.join(output_dir, \"AI_Factor_Endogeneity_Tests.docx\")\n",
    "        \n",
    "        # Run endogeneity testing\n",
    "        tester = EndogeneityTester(data_path=data_path, output_path=output_file_path)\n",
    "        results = tester.run_complete_endogeneity_analysis()\n",
    "        \n",
    "        if results:\n",
    "            print(f\"\\n Endogeneity testing completed successfully!\")\n",
    "            print(f\" Document saved to: {output_file_path}\")\n",
    "        else:\n",
    "            print(f\"\\n Endogeneity testing failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75e154-8fff-4e80-bf61-bb71a5106511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBUST MOMENTUM RETURNS CALCULATOR - CUMULATIVE APPROACH\n",
    "# =========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RobustMomentumCalculator:\n",
    "    \"\"\"\n",
    "    Calculate momentum returns using cumulative daily returns approach\n",
    "    This avoids price discontinuity issues from exact date matching\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, daily_prices_path, final_dataset_path):\n",
    "        \"\"\"\n",
    "        Initialize with file paths\n",
    "        \"\"\"\n",
    "        self.daily_prices_path = daily_prices_path\n",
    "        self.final_dataset_path = final_dataset_path\n",
    "        \n",
    "        print(\" ROBUST MOMENTUM CALCULATOR INITIALIZED\")\n",
    "        print(f\"    Daily prices: {daily_prices_path}\")\n",
    "        print(f\"    Final dataset: {final_dataset_path}\")\n",
    "        print(\"    Uses cumulative daily returns to avoid price discontinuities\")\n",
    "    \n",
    "    def load_and_examine_data(self):\n",
    "        \"\"\"\n",
    "        Load and examine data with focus on quality\n",
    "        \"\"\"\n",
    "        print(\"\\n LOADING AND EXAMINING DATA\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        # Load daily prices\n",
    "        print(\"Loading daily prices...\")\n",
    "        daily_cols_needed = ['gvkey', 'datadate', 'prccd', 'ajexdi', 'cshoc']\n",
    "        daily_sample = pd.read_csv(self.daily_prices_path, nrows=5)\n",
    "        available_cols = [col for col in daily_cols_needed if col in daily_sample.columns]\n",
    "        print(f\"Available columns: {available_cols}\")\n",
    "        \n",
    "        self.daily_prices = pd.read_csv(self.daily_prices_path, usecols=available_cols, dtype={'gvkey': str})\n",
    "        \n",
    "        # Standardize GVKEY\n",
    "        self.daily_prices['gvkey'] = self.daily_prices['gvkey'].str.strip().str.zfill(6)\n",
    "        self.daily_prices['datadate'] = pd.to_datetime(self.daily_prices['datadate'])\n",
    "        \n",
    "        # Use adjusted prices if available, otherwise raw prices\n",
    "        if 'ajexdi' in self.daily_prices.columns:\n",
    "            self.daily_prices['adj_price'] = self.daily_prices['prccd'] / self.daily_prices['ajexdi']\n",
    "            print(\"    Using split/dividend adjusted prices\")\n",
    "        else:\n",
    "            self.daily_prices['adj_price'] = self.daily_prices['prccd']\n",
    "            print(\"    No adjustment factors - using raw prices\")\n",
    "        \n",
    "        # Clean data\n",
    "        original_count = len(self.daily_prices)\n",
    "        self.daily_prices = self.daily_prices[\n",
    "            (self.daily_prices['adj_price'].notna()) & \n",
    "            (self.daily_prices['adj_price'] > 0)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"    Cleaned: {original_count:,}  {len(self.daily_prices):,} ({len(self.daily_prices)/original_count:.1%})\")\n",
    "        \n",
    "        self.daily_prices = self.daily_prices.sort_values(['gvkey', 'datadate']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\" Daily Prices: {self.daily_prices.shape}\")\n",
    "        print(f\"    Date range: {self.daily_prices['datadate'].min()} to {self.daily_prices['datadate'].max()}\")\n",
    "        print(f\"    Unique GVKEYs: {self.daily_prices['gvkey'].nunique()}\")\n",
    "        \n",
    "        # Load final dataset\n",
    "        print(\"\\nLoading final dataset...\")\n",
    "        self.final_dataset = pd.read_csv(self.final_dataset_path, dtype={'gvkey': str})\n",
    "        self.final_dataset['gvkey'] = self.final_dataset['gvkey'].str.strip().str.zfill(6)\n",
    "        self.final_dataset['filingDate'] = pd.to_datetime(self.final_dataset['filingDate'])\n",
    "        \n",
    "        print(f\" Final Dataset: {self.final_dataset.shape}\")\n",
    "        print(f\"    Unique GVKEYs: {self.final_dataset['gvkey'].nunique()}\")\n",
    "        \n",
    "        # Check overlap\n",
    "        daily_gvkeys = set(self.daily_prices['gvkey'].unique())\n",
    "        final_gvkeys = set(self.final_dataset['gvkey'].unique())\n",
    "        overlap = daily_gvkeys.intersection(final_gvkeys)\n",
    "        \n",
    "        print(f\"\\n GVKEY OVERLAP: {len(overlap)}/{len(final_gvkeys)} ({len(overlap)/len(final_gvkeys):.1%})\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def calculate_clean_daily_returns(self):\n",
    "        \"\"\"\n",
    "        Calculate clean daily returns for all firms\n",
    "        \"\"\"\n",
    "        print(\"\\n CALCULATING CLEAN DAILY RETURNS\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        # Calculate daily returns with robust cleaning\n",
    "        self.daily_prices['price_lag'] = self.daily_prices.groupby('gvkey')['adj_price'].shift(1)\n",
    "        self.daily_prices['daily_return'] = (self.daily_prices['adj_price'] / self.daily_prices['price_lag']) - 1\n",
    "        \n",
    "        # Remove extreme daily returns and missing values  \n",
    "        original_count = len(self.daily_prices)\n",
    "        self.daily_prices = self.daily_prices[\n",
    "            (self.daily_prices['daily_return'].between(-0.50, 0.50)) &  # 50% max daily (realistic for legitimate moves)\n",
    "            (self.daily_prices['daily_return'].notna())\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"    After return filtering: {original_count:,}  {len(self.daily_prices):,}\")\n",
    "        print(f\"    Daily return stats:\")\n",
    "        returns = self.daily_prices['daily_return']\n",
    "        print(f\"      Mean: {returns.mean():.6f}\")\n",
    "        print(f\"      Std:  {returns.std():.6f}\")\n",
    "        print(f\"      Min:  {returns.min():.6f}\")\n",
    "        print(f\"      Max:  {returns.max():.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def calculate_momentum_via_cumulative_returns(self, months_back=[1, 2, 3]):\n",
    "        \"\"\"\n",
    "        Calculate momentum using cumulative daily returns approach\n",
    "        \"\"\"\n",
    "        print(f\"\\n CALCULATING MOMENTUM VIA CUMULATIVE RETURNS\")\n",
    "        print(\"=\" * 55)\n",
    "        print(f\"    Months back: {months_back}\")\n",
    "        print(f\"    Method: Cumulative daily returns over ~21 trading days with winsorization\")\n",
    "        \n",
    "        momentum_results = []\n",
    "        \n",
    "        # Get overlapping firms\n",
    "        daily_gvkeys = set(self.daily_prices['gvkey'].unique())\n",
    "        final_gvkeys = set(self.final_dataset['gvkey'].unique())\n",
    "        processable_gvkeys = daily_gvkeys.intersection(final_gvkeys)\n",
    "        \n",
    "        print(f\"    Processing {len(processable_gvkeys)} firms\")\n",
    "        \n",
    "        # Statistics tracking\n",
    "        stats = {month: {'attempts': 0, 'successful': 0, 'avg_days': 0} for month in months_back}\n",
    "        \n",
    "        for gvkey in tqdm(processable_gvkeys, desc=\"Calculating momentum\"):\n",
    "            # Get firm data\n",
    "            firm_data = self.daily_prices[self.daily_prices['gvkey'] == gvkey].copy()\n",
    "            firm_filings = self.final_dataset[self.final_dataset['gvkey'] == gvkey].copy()\n",
    "            \n",
    "            if len(firm_data) < 100:  # Need sufficient history\n",
    "                continue\n",
    "            \n",
    "            # Calculate log returns for better aggregation properties\n",
    "            firm_data['log_return'] = np.log(1 + firm_data['daily_return'])\n",
    "            firm_data = firm_data.sort_values('datadate')\n",
    "            \n",
    "            # Process each filing\n",
    "            for _, filing_row in firm_filings.iterrows():\n",
    "                filing_date = filing_row['filingDate']\n",
    "                year = filing_row.get('Year', filing_date.year)\n",
    "                \n",
    "                result = {\n",
    "                    'gvkey': gvkey,\n",
    "                    'Year': year,\n",
    "                    'filing_date': filing_date\n",
    "                }\n",
    "                \n",
    "                # Calculate returns for each lookback period\n",
    "                for months in months_back:\n",
    "                    stats[months]['attempts'] += 1\n",
    "                    \n",
    "                    # Define lookback period (approximately months * 21 trading days)\n",
    "                    target_days = months * 21\n",
    "                    lookback_start = filing_date - pd.DateOffset(days=int(months * 35))  # Buffer for weekends\n",
    "                    lookback_end = filing_date - pd.DateOffset(days=5)  # Small buffer before filing\n",
    "                    \n",
    "                    # Get data in lookback window\n",
    "                    window_data = firm_data[\n",
    "                        (firm_data['datadate'] >= lookback_start) &\n",
    "                        (firm_data['datadate'] <= lookback_end)\n",
    "                    ].copy()\n",
    "                    \n",
    "                    if len(window_data) < 10:  # Need minimum observations\n",
    "                        result[f'return_t_minus_{months}m'] = np.nan\n",
    "                        result[f'n_days_t_minus_{months}m'] = 0\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Take the most recent 'target_days' observations\n",
    "                        recent_data = window_data.tail(min(target_days, len(window_data)))\n",
    "                        \n",
    "                        # Calculate cumulative return using simple compounding\n",
    "                        daily_returns = recent_data['daily_return'].values\n",
    "                        cumulative_return = np.prod(1 + daily_returns) - 1\n",
    "                        \n",
    "                        # Store results (no hard caps - will winsorize later)\n",
    "                        result[f'return_t_minus_{months}m'] = cumulative_return\n",
    "                        result[f'n_days_t_minus_{months}m'] = len(recent_data)\n",
    "                        result[f'start_date_t_minus_{months}m'] = recent_data['datadate'].iloc[0]\n",
    "                        result[f'end_date_t_minus_{months}m'] = recent_data['datadate'].iloc[-1]\n",
    "                        \n",
    "                        # Update statistics\n",
    "                        stats[months]['successful'] += 1\n",
    "                        stats[months]['avg_days'] += len(recent_data)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        result[f'return_t_minus_{months}m'] = np.nan\n",
    "                        result[f'n_days_t_minus_{months}m'] = 0\n",
    "                        continue\n",
    "                \n",
    "                momentum_results.append(result)\n",
    "        \n",
    "        # Calculate final statistics\n",
    "        for month in stats:\n",
    "            if stats[month]['successful'] > 0:\n",
    "                stats[month]['avg_days'] /= stats[month]['successful']\n",
    "        \n",
    "        if momentum_results:\n",
    "            self.momentum_data = pd.DataFrame(momentum_results)\n",
    "            \n",
    "            # Apply winsorization to handle remaining extreme values\n",
    "            print(f\"\\n APPLYING WINSORIZATION TO MOMENTUM RETURNS:\")\n",
    "            for month in months_back:\n",
    "                return_col = f'return_t_minus_{month}m'\n",
    "                if return_col in self.momentum_data.columns:\n",
    "                    original_returns = self.momentum_data[return_col].dropna()\n",
    "                    if len(original_returns) > 0:\n",
    "                        # Winsorize at 1st and 99th percentiles\n",
    "                        p01 = original_returns.quantile(0.01)\n",
    "                        p99 = original_returns.quantile(0.99)\n",
    "                        \n",
    "                        winsorized_count = ((original_returns < p01) | (original_returns > p99)).sum()\n",
    "                        \n",
    "                        # Apply winsorization\n",
    "                        self.momentum_data[return_col] = self.momentum_data[return_col].clip(lower=p01, upper=p99)\n",
    "                        \n",
    "                        print(f\"   t-{month}m: winsorized {winsorized_count} values at [{p01:.3f}, {p99:.3f}]\")\n",
    "            \n",
    "            print(f\"\\n Momentum calculations completed:\")\n",
    "            print(f\"    Total observations: {len(self.momentum_data)}\")\n",
    "            \n",
    "            # Success rates\n",
    "            print(f\"\\n Success Rate by Lookback Period:\")\n",
    "            for month in months_back:\n",
    "                total = stats[month]['attempts']\n",
    "                success = stats[month]['successful']\n",
    "                avg_days = stats[month]['avg_days']\n",
    "                rate = success/total*100 if total > 0 else 0\n",
    "                print(f\"   t-{month}m: {success:4d}/{total:4d} ({rate:5.1f}%) | Avg days: {avg_days:5.1f}\")\n",
    "            \n",
    "            # Return statistics AFTER winsorization\n",
    "            print(f\"\\n Momentum Return Statistics (After Winsorization):\")\n",
    "            for month in months_back:\n",
    "                return_col = f'return_t_minus_{month}m'\n",
    "                if return_col in self.momentum_data.columns:\n",
    "                    returns = self.momentum_data[return_col].dropna()\n",
    "                    if len(returns) > 0:\n",
    "                        print(f\"   t-{month}m: mean={returns.mean():7.4f}, std={returns.std():6.4f}, \"\n",
    "                              f\"min={returns.min():7.4f}, max={returns.max():7.4f}\")\n",
    "                        \n",
    "                        # Check for remaining extremes\n",
    "                        extreme_count = ((returns < -0.5) | (returns > 0.5)).sum()\n",
    "                        if extreme_count > 0:\n",
    "                            print(f\"            {extreme_count} returns still >50% (after winsorization)\")\n",
    "                        else:\n",
    "                            print(f\"            All returns within reasonable bounds\")\n",
    "        else:\n",
    "            raise ValueError(\"No momentum returns calculated!\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def merge_with_final_dataset(self):\n",
    "        \"\"\"\n",
    "        Merge momentum data with final dataset\n",
    "        \"\"\"\n",
    "        print(\"\\n MERGING WITH FINAL DATASET\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Merge on gvkey and Year\n",
    "        momentum_cols = [col for col in self.momentum_data.columns \n",
    "                        if col not in ['gvkey', 'Year', 'filing_date']]\n",
    "        merge_cols = ['gvkey', 'Year'] + momentum_cols\n",
    "        \n",
    "        self.final_dataset_with_momentum = pd.merge(\n",
    "            self.final_dataset,\n",
    "            self.momentum_data[merge_cols],\n",
    "            on=['gvkey', 'Year'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        total_obs = len(self.final_dataset)\n",
    "        \n",
    "        print(f\" Merge completed:\")\n",
    "        print(f\"    Total observations: {total_obs}\")\n",
    "        print(f\"    New momentum columns: {len(momentum_cols)}\")\n",
    "        \n",
    "        # Availability check\n",
    "        print(f\"\\n Final momentum availability:\")\n",
    "        for month in [1, 2, 3]:\n",
    "            return_col = f'return_t_minus_{month}m'\n",
    "            if return_col in self.final_dataset_with_momentum.columns:\n",
    "                available = self.final_dataset_with_momentum[return_col].notna().sum()\n",
    "                print(f\"   t-{month}m: {available:4d}/{total_obs:4d} ({available/total_obs:.1%})\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def validate_momentum_quality(self):\n",
    "        \"\"\"\n",
    "        Perform quality checks on momentum returns\n",
    "        \"\"\"\n",
    "        print(f\"\\n MOMENTUM QUALITY VALIDATION\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for month in [1, 2, 3]:\n",
    "            return_col = f'return_t_minus_{month}m'\n",
    "            if return_col in self.final_dataset_with_momentum.columns:\n",
    "                returns = self.final_dataset_with_momentum[return_col].dropna()\n",
    "                \n",
    "                if len(returns) > 0:\n",
    "                    print(f\"\\n t-{month}m Quality Check:\")\n",
    "                    print(f\"   Valid observations: {len(returns)}\")\n",
    "                    print(f\"   Mean: {returns.mean():8.4f}\")\n",
    "                    print(f\"   Std:  {returns.std():8.4f}\")\n",
    "                    print(f\"   Skew: {returns.skew():8.4f}\")\n",
    "                    print(f\"   Kurt: {returns.kurtosis():8.4f}\")\n",
    "                    \n",
    "                    # Percentile analysis\n",
    "                    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "                    pct_values = returns.quantile([p/100 for p in percentiles])\n",
    "                    print(f\"   Percentiles: \" + \" | \".join([f\"p{p}={pct_values[p/100]:.3f}\" for p in [1, 5, 95, 99]]))\n",
    "                    \n",
    "                    # Flag suspicious patterns\n",
    "                    if abs(returns.mean()) > 0.05:\n",
    "                        print(f\"    High average return ({returns.mean():.3f}) - check for data issues\")\n",
    "                    if returns.std() > 0.3:\n",
    "                        print(f\"    High volatility ({returns.std():.3f}) - possible remaining outliers\")\n",
    "                    if abs(returns.skew()) > 3:\n",
    "                        print(f\"    High skewness ({returns.skew():.2f}) - asymmetric distribution\")\n",
    "                    \n",
    "                    # Check extreme values\n",
    "                    extreme_positive = (returns > 0.5).sum()\n",
    "                    extreme_negative = (returns < -0.5).sum()\n",
    "                    if extreme_positive + extreme_negative > 0:\n",
    "                        print(f\"    Extreme values: {extreme_positive} >50%, {extreme_negative} <-50%\")\n",
    "                    else:\n",
    "                        print(f\"    No extreme values detected\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def save_enhanced_dataset(self, output_path=None):\n",
    "        \"\"\"\n",
    "        Save enhanced dataset\n",
    "        \"\"\"\n",
    "        print(\"\\n SAVING ENHANCED DATASET\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        if output_path is None:\n",
    "            output_path = self.final_dataset_path.replace('.csv', '_with_robust_momentum.csv')\n",
    "        \n",
    "        self.final_dataset_with_momentum.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\" Dataset saved: {output_path}\")\n",
    "        print(f\"    Shape: {self.final_dataset_with_momentum.shape}\")\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"\n",
    "        Run complete robust momentum analysis\n",
    "        \"\"\"\n",
    "        print(\" RUNNING ROBUST MOMENTUM ANALYSIS\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        try:\n",
    "            self.load_and_examine_data()\n",
    "            self.calculate_clean_daily_returns()\n",
    "            self.calculate_momentum_via_cumulative_returns()\n",
    "            self.merge_with_final_dataset()\n",
    "            self.validate_momentum_quality()\n",
    "            output_path = self.save_enhanced_dataset()\n",
    "            \n",
    "            print(f\"\\n ROBUST MOMENTUM ANALYSIS COMPLETE!\")\n",
    "            print(f\"    Used cumulative daily returns approach\")\n",
    "            print(f\"    Applied strict outlier filtering\")\n",
    "            print(f\"    Quality validated\")\n",
    "            print(f\"    Ready for regression analysis!\")\n",
    "            \n",
    "            return self.final_dataset_with_momentum, output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n ERROR: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" ROBUST MOMENTUM CALCULATOR\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    calculator = RobustMomentumCalculator(\n",
    "        daily_prices_path=\"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Controls/daily.csv\",\n",
    "        final_dataset_path=\"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/final_clean_dataset_filtered_with_corrected_factor_loadings.csv\"\n",
    "    )\n",
    "    \n",
    "    enhanced_dataset, output_path = calculator.run_complete_analysis()\n",
    "    \n",
    "    if enhanced_dataset is not None:\n",
    "        print(f\"\\n SUCCESS! Robust momentum returns calculated!\")\n",
    "        print(f\"Output: {output_path}\")\n",
    "    else:\n",
    "        print(f\"\\n Analysis failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce276e95-7a2b-43cc-ae11-cbe6a626e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED ENDOGENEITY TESTING MODULE FOR AI FACTOR THESIS\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.enum.table import WD_TABLE_ALIGNMENT\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class UpdatedEndogeneityTester:\n",
    "    \"\"\"\n",
    "    Comprehensive endogeneity testing for AI factor thesis.\n",
    "    Updated to work with current dataframe structure including:\n",
    "    - Momentum returns (return_t_minus_1m, return_t_minus_2m, return_t_minus_3m)\n",
    "    - Factor loadings (beta_mktrf_t3, beta_mktrf_t6, etc.)\n",
    "    - AI factors from multiple LLMs\n",
    "    - Calculated financial metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, output_path):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.df = None\n",
    "        self.results = {}\n",
    "        self.diagnostics = {}\n",
    "        \n",
    "        # Primary dependent variable\n",
    "        self.y_var = 'excess_return_12mo_annualized'\n",
    "        \n",
    "        # Standard controls (updated for current dataframe)\n",
    "        self.standard_controls_config = {\n",
    "            'Log Market Cap': 'calc_log_market_cap',\n",
    "            'Price-to-Book': 'calc_price_to_book', \n",
    "            'ROA': 'calc_roa',\n",
    "            'Market Beta': 'beta_mktrf_t6',  # Use 6-month beta\n",
    "            'Momentum (t-1m)': 'return_t_minus_1m'  # Use momentum returns we just calculated\n",
    "        }\n",
    "        \n",
    "        # Management quality proxies (updated)\n",
    "        self.management_proxies_config = {\n",
    "            'ROA': 'calc_roa',\n",
    "            'ROE': 'calc_roe', \n",
    "            'Operating Margin': 'calc_operating_margin',\n",
    "            'Profit Margin': 'calc_profit_margin',\n",
    "            'Debt-to-Assets': 'calc_debt_to_assets',\n",
    "            'Asset Turnover': 'calc_asset_turnover',\n",
    "            'Price-to-Earnings': 'calc_price_to_earnings'\n",
    "        }\n",
    "        \n",
    "        # Fixed effects configuration\n",
    "        self.fe_config = {'year': True, 'sector': True}\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"Load data and create necessary variables\"\"\"\n",
    "        print(\" Loading data for endogeneity testing...\")\n",
    "        self.df = pd.read_csv(self.data_path, dtype={'gvkey': str, 'CIK': str})\n",
    "        \n",
    "        print(f\" Loaded dataset: {self.df.shape}\")\n",
    "        print(f\"   Available columns: {len(self.df.columns)}\")\n",
    "        print(f\"   Sample period: {self.df['Year'].min()}-{self.df['Year'].max()}\")\n",
    "        \n",
    "        # Create AI factor averages (same methodology as main analysis)\n",
    "        self.ai_factors_config = {\n",
    "            'Strategic Depth': ['Strategic Depth_gpt4o_Numeric', 'Strategic Depth_flash1_5_Numeric', 'Strategic Depth_flash2_5_Numeric'],\n",
    "            'AI Sentiment': ['Disclosure Sentiment_gpt4o_Numeric', 'Disclosure Sentiment_flash1_5_Numeric', 'Disclosure Sentiment_flash2_5_Numeric'],\n",
    "            'Risk Own Adoption': ['Risk - Own Adoption_gpt4o_Numeric', 'Risk - Own Adoption_flash1_5_Numeric', 'Risk - Own Adoption_flash2_5_Numeric'],\n",
    "            'Risk External Threats': ['Risk - External Threats_gpt4o_Numeric', 'Risk - External Threats_flash1_5_Numeric', 'Risk - External Threats_flash2_5_Numeric'],\n",
    "            'Risk Non-Adoption': ['Risk - Non-Adoption_gpt4o_Numeric', 'Risk - Non-Adoption_flash1_5_Numeric', 'Risk - Non-Adoption_flash2_5_Numeric'],\n",
    "            'Forward Looking': ['Forward-Looking_gpt4o_Numeric', 'Forward-Looking_flash1_5_Numeric', 'Forward-Looking_flash2_5_Numeric'],\n",
    "            'AI Washing': ['AI Washing Index_gpt4o_Numeric', 'AI Washing Index_flash1_5_Numeric', 'AI Washing Index_flash2_5_Numeric'],\n",
    "            'Talent Investment': ['Talent & Investment_gpt4o_Numeric', 'Talent & Investment_flash1_5_Numeric', 'Talent & Investment_flash2_5_Numeric']\n",
    "        }\n",
    "        \n",
    "        self.average_ai_factor_cols_map = {}\n",
    "        for factor_name, cols in self.ai_factors_config.items():\n",
    "            available_cols = [col for col in cols if col in self.df.columns]\n",
    "            if available_cols:\n",
    "                avg_col_name = f\"{factor_name.replace(' ', '_')}_Average\"\n",
    "                self.df[avg_col_name] = self.df[available_cols].mean(axis=1, skipna=True)\n",
    "                self.average_ai_factor_cols_map[factor_name] = avg_col_name\n",
    "                print(f\" Created {avg_col_name}: {self.df[avg_col_name].notna().sum():,} obs\")\n",
    "        \n",
    "        # Create additional management proxies and lagged variables\n",
    "        self._create_management_proxies()\n",
    "        self._create_lagged_variables()\n",
    "        self._prepare_fixed_effects_dummies()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _create_management_proxies(self):\n",
    "        \"\"\"Create additional management quality proxy variables\"\"\"\n",
    "        print(\" Creating management quality proxies...\")\n",
    "        \n",
    "        # Sort by firm and year for lagging/rolling calculations\n",
    "        self.df = self.df.sort_values(['gvkey', 'Year'])\n",
    "        \n",
    "        # Create volatility measures using rolling windows\n",
    "        rolling_vars = {\n",
    "            'calc_roa_volatility': 'calc_roa',\n",
    "            'calc_roe_volatility': 'calc_roe', \n",
    "            'calc_return_volatility': self.y_var,\n",
    "            'calc_profit_margin_volatility': 'calc_profit_margin'\n",
    "        }\n",
    "        \n",
    "        for vol_var, base_var in rolling_vars.items():\n",
    "            if base_var in self.df.columns:\n",
    "                self.df[vol_var] = self.df.groupby('gvkey')[base_var].rolling(\n",
    "                    window=3, min_periods=2\n",
    "                ).std().reset_index(0, drop=True)\n",
    "        \n",
    "        # Create earnings quality measure (negative of ROA volatility)\n",
    "        if 'calc_roa_volatility' in self.df.columns:\n",
    "            self.df['calc_earnings_quality'] = -self.df['calc_roa_volatility']\n",
    "        \n",
    "        # Create momentum measures from our new momentum returns\n",
    "        momentum_cols = ['return_t_minus_1m', 'return_t_minus_2m', 'return_t_minus_3m']\n",
    "        available_momentum = [col for col in momentum_cols if col in self.df.columns]\n",
    "        \n",
    "        if len(available_momentum) >= 2:\n",
    "            # Create 2-month momentum average\n",
    "            self.df['calc_mom_2m_avg'] = self.df[available_momentum[:2]].mean(axis=1, skipna=True)\n",
    "            \n",
    "        if len(available_momentum) >= 3:\n",
    "            # Create 3-month momentum average  \n",
    "            self.df['calc_mom_3m_avg'] = self.df[available_momentum].mean(axis=1, skipna=True)\n",
    "        \n",
    "        print(\" Management quality proxies created\")\n",
    "\n",
    "    def _create_lagged_variables(self):\n",
    "        \"\"\"Create lagged performance variables for reverse causality testing\"\"\"\n",
    "        print(\" Creating lagged variables for reverse causality testing...\")\n",
    "        \n",
    "        # Variables to lag\n",
    "        vars_to_lag = [\n",
    "            self.y_var, 'calc_roa', 'calc_roe', 'calc_log_market_cap',\n",
    "            'return_t_minus_1m', 'calc_profit_margin', 'calc_operating_margin'\n",
    "        ]\n",
    "        \n",
    "        # Add AI factors to lagging\n",
    "        ai_factor_cols = list(self.average_ai_factor_cols_map.values())\n",
    "        vars_to_lag.extend(ai_factor_cols)\n",
    "        \n",
    "        for var in vars_to_lag:\n",
    "            if var in self.df.columns:\n",
    "                self.df[f'{var}_lag1'] = self.df.groupby('gvkey')[var].shift(1)\n",
    "                self.df[f'{var}_lag2'] = self.df.groupby('gvkey')[var].shift(2)\n",
    "        \n",
    "        print(\" Lagged variables created\")\n",
    "\n",
    "    def _prepare_fixed_effects_dummies(self):\n",
    "        \"\"\"Create fixed effects dummy variables\"\"\"\n",
    "        print(\" Creating fixed effects dummies...\")\n",
    "        \n",
    "        # Year fixed effects\n",
    "        self.master_year_fe_cols = []\n",
    "        if 'Year' in self.df.columns:\n",
    "            valid_years = sorted(self.df['Year'].dropna().unique())\n",
    "            if len(valid_years) > 1:\n",
    "                for year_val in valid_years[1:]:  # Drop first year as base\n",
    "                    fe_col_name = f'year_{int(year_val)}'\n",
    "                    self.df[fe_col_name] = (self.df['Year'] == year_val).astype(int)\n",
    "                    self.master_year_fe_cols.append(fe_col_name)\n",
    "        \n",
    "        # Sector fixed effects (use sector_gsector if available)\n",
    "        self.master_sector_fe_cols = []\n",
    "        sector_cols = ['Sector', 'sector_gsector', 'sector_ggroup']\n",
    "        sector_col = None\n",
    "        \n",
    "        for col in sector_cols:\n",
    "            if col in self.df.columns and self.df[col].notna().sum() > 0:\n",
    "                sector_col = col\n",
    "                break\n",
    "        \n",
    "        if sector_col:\n",
    "            valid_sectors = self.df[sector_col].dropna().unique()\n",
    "            if len(valid_sectors) > 1:\n",
    "                for i, sector_val in enumerate(valid_sectors[1:]):  # Drop first as base\n",
    "                    fe_col_name = f'sector_{i+1}'\n",
    "                    self.df[fe_col_name] = (self.df[sector_col] == sector_val).astype(int)\n",
    "                    self.master_sector_fe_cols.append(fe_col_name)\n",
    "        \n",
    "        print(f\" Created {len(self.master_year_fe_cols)} year FE, {len(self.master_sector_fe_cols)} sector FE\")\n",
    "\n",
    "    def _run_regression_with_controls(self, y_var, x_vars, control_vars=None, include_fe=True):\n",
    "        \"\"\"Helper function to run regression with specified controls\"\"\"\n",
    "        if control_vars is None:\n",
    "            control_vars = []\n",
    "        \n",
    "        # Combine all variables\n",
    "        all_x_vars = x_vars.copy()\n",
    "        all_x_vars.extend(control_vars)\n",
    "        \n",
    "        if include_fe:\n",
    "            all_x_vars.extend(self.master_year_fe_cols)\n",
    "            all_x_vars.extend(self.master_sector_fe_cols)\n",
    "        \n",
    "        # Filter to available columns\n",
    "        all_x_vars = [var for var in all_x_vars if var in self.df.columns]\n",
    "        all_x_vars = list(set(all_x_vars))  # Remove duplicates\n",
    "        \n",
    "        # Prepare regression data\n",
    "        required_cols = [y_var] + all_x_vars + ['gvkey']\n",
    "        available_required_cols = [col for col in required_cols if col in self.df.columns]\n",
    "        \n",
    "        reg_data = self.df[available_required_cols].dropna()\n",
    "        \n",
    "        if len(reg_data) < len(all_x_vars) + 20:  # Need sufficient observations\n",
    "            print(f\" Insufficient data for regression: {len(reg_data)} obs, {len(all_x_vars)} vars\")\n",
    "            return None\n",
    "        \n",
    "        # Winsorize variables at 1%/99%\n",
    "        vars_to_winsorize = [col for col in [y_var] + all_x_vars if col in reg_data.columns]\n",
    "        for var in vars_to_winsorize:\n",
    "            if pd.api.types.is_numeric_dtype(reg_data[var]):\n",
    "                if reg_data[var].notna().sum() > 0 and reg_data[var].nunique() > 1:\n",
    "                    p1, p99 = reg_data[var].quantile([0.01, 0.99])\n",
    "                    if pd.notna(p1) and pd.notna(p99) and p1 != p99:\n",
    "                        reg_data[var] = reg_data[var].clip(lower=p1, upper=p99)\n",
    "        \n",
    "        # Run regression with firm-clustered standard errors\n",
    "        try:\n",
    "            available_x_vars = [var for var in all_x_vars if var in reg_data.columns]\n",
    "            if not available_x_vars:\n",
    "                return None\n",
    "                \n",
    "            X = sm.add_constant(reg_data[available_x_vars])\n",
    "            y = reg_data[y_var]\n",
    "            \n",
    "            # Use firm-clustered standard errors if gvkey available\n",
    "            if 'gvkey' in reg_data.columns:\n",
    "                model = OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': reg_data['gvkey']})\n",
    "            else:\n",
    "                model = OLS(y, X).fit(cov_type='HC1')  # Robust standard errors\n",
    "            \n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\" Regression failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def test_omitted_variable_bias(self):\n",
    "        \"\"\"Test for omitted variable bias using management quality proxies\"\"\"\n",
    "        print(\"\\n Testing for Omitted Variable Bias...\")\n",
    "        \n",
    "        self.results['omitted_variable_bias'] = {}\n",
    "        \n",
    "        # Get available controls\n",
    "        std_controls = [col for col in self.standard_controls_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        \n",
    "        mgmt_proxies = [col for col in self.management_proxies_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        \n",
    "        print(f\"   Standard controls available: {len(std_controls)}\")\n",
    "        print(f\"   Management proxies available: {len(mgmt_proxies)}\")\n",
    "        \n",
    "        for ai_factor_name, ai_col in self.average_ai_factor_cols_map.items():\n",
    "            if ai_col not in self.df.columns:\n",
    "                continue\n",
    "                \n",
    "            print(f\"   Testing {ai_factor_name}...\")\n",
    "            factor_results = {}\n",
    "            \n",
    "            # Model 1: Baseline (AI factor + standard controls + FE)\n",
    "            model_baseline = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_baseline:\n",
    "                factor_results['Baseline'] = model_baseline\n",
    "            \n",
    "            # Model 2: Add management quality proxies\n",
    "            model_with_mgmt = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls + mgmt_proxies,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_with_mgmt:\n",
    "                factor_results['With Management Proxies'] = model_with_mgmt\n",
    "            \n",
    "            # Model 3: Only management proxies (test their significance)\n",
    "            model_mgmt_only = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=mgmt_proxies,\n",
    "                control_vars=std_controls,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_mgmt_only:\n",
    "                factor_results['Management Proxies Only'] = model_mgmt_only\n",
    "            \n",
    "            if factor_results:\n",
    "                self.results['omitted_variable_bias'][ai_factor_name] = factor_results\n",
    "        \n",
    "        print(\" Omitted Variable Bias testing complete\")\n",
    "\n",
    "    def test_reverse_causality(self):\n",
    "        \"\"\"Test for reverse causality by regressing AI metrics on lagged performance\"\"\"\n",
    "        print(\"\\n Testing for Reverse Causality...\")\n",
    "        \n",
    "        self.results['reverse_causality'] = {}\n",
    "        \n",
    "        # Lagged performance variables\n",
    "        lagged_performance_vars = [\n",
    "            f'{self.y_var}_lag1', f'{self.y_var}_lag2',\n",
    "            'calc_roa_lag1', 'calc_roa_lag2',\n",
    "            'return_t_minus_1m_lag1', 'return_t_minus_2m_lag1'\n",
    "        ]\n",
    "        lagged_perf_available = [col for col in lagged_performance_vars \n",
    "                                if col and col in self.df.columns]\n",
    "        \n",
    "        std_controls = [col for col in self.standard_controls_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        \n",
    "        print(f\"   Lagged performance variables available: {len(lagged_perf_available)}\")\n",
    "        \n",
    "        for ai_factor_name, ai_col in self.average_ai_factor_cols_map.items():\n",
    "            if ai_col not in self.df.columns:\n",
    "                continue\n",
    "                \n",
    "            print(f\"   Testing reverse causality for {ai_factor_name}...\")\n",
    "            factor_results = {}\n",
    "            \n",
    "            # Test 1: AI factor regressed on lagged performance (main test)\n",
    "            if lagged_perf_available:\n",
    "                model_reverse_main = self._run_regression_with_controls(\n",
    "                    y_var=ai_col,\n",
    "                    x_vars=lagged_perf_available,\n",
    "                    control_vars=[],  # No controls for pure test\n",
    "                    include_fe=True\n",
    "                )\n",
    "                if model_reverse_main:\n",
    "                    factor_results['AI ~ Lagged Performance'] = model_reverse_main\n",
    "            \n",
    "            # Test 2: AI factor regressed on lagged performance + controls\n",
    "            if lagged_perf_available:\n",
    "                model_reverse_controls = self._run_regression_with_controls(\n",
    "                    y_var=ai_col,\n",
    "                    x_vars=lagged_perf_available,\n",
    "                    control_vars=std_controls,\n",
    "                    include_fe=True\n",
    "                )\n",
    "                if model_reverse_controls:\n",
    "                    factor_results['AI ~ Lagged Performance + Controls'] = model_reverse_controls\n",
    "            \n",
    "            # Test 3: Forward-looking test - current performance on lagged AI\n",
    "            ai_col_lag1 = f'{ai_col}_lag1'\n",
    "            if ai_col_lag1 in self.df.columns:\n",
    "                model_forward = self._run_regression_with_controls(\n",
    "                    y_var=self.y_var,\n",
    "                    x_vars=[ai_col_lag1],\n",
    "                    control_vars=std_controls,\n",
    "                    include_fe=True\n",
    "                )\n",
    "                if model_forward:\n",
    "                    factor_results['Current Return ~ Lagged AI'] = model_forward\n",
    "            \n",
    "            if factor_results:\n",
    "                self.results['reverse_causality'][ai_factor_name] = factor_results\n",
    "        \n",
    "        print(\" Reverse Causality testing complete\")\n",
    "\n",
    "    def test_coefficient_stability(self):\n",
    "        \"\"\"Test coefficient stability across different specifications\"\"\"\n",
    "        print(\"\\n Testing Coefficient Stability...\")\n",
    "        \n",
    "        self.results['coefficient_stability'] = {}\n",
    "        \n",
    "        std_controls = [col for col in self.standard_controls_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        mgmt_proxies = [col for col in self.management_proxies_config.values() \n",
    "                       if col and col in self.df.columns]\n",
    "        \n",
    "        for ai_factor_name, ai_col in self.average_ai_factor_cols_map.items():\n",
    "            if ai_col not in self.df.columns:\n",
    "                continue\n",
    "                \n",
    "            print(f\"   Testing stability for {ai_factor_name}...\")\n",
    "            factor_results = {}\n",
    "            \n",
    "            # Specification 1: AI factor only + FE\n",
    "            model_fe_only = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=[],\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_fe_only:\n",
    "                factor_results['FE Only'] = model_fe_only\n",
    "            \n",
    "            # Specification 2: + Standard controls\n",
    "            model_std = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_std:\n",
    "                factor_results['+ Standard Controls'] = model_std\n",
    "            \n",
    "            # Specification 3: + Management proxies\n",
    "            model_mgmt = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls + mgmt_proxies,\n",
    "                include_fe=True\n",
    "            )\n",
    "            if model_mgmt:\n",
    "                factor_results['+ Management Proxies'] = model_mgmt\n",
    "            \n",
    "            # Specification 4: No fixed effects (test FE impact)\n",
    "            model_no_fe = self._run_regression_with_controls(\n",
    "                y_var=self.y_var,\n",
    "                x_vars=[ai_col],\n",
    "                control_vars=std_controls,\n",
    "                include_fe=False\n",
    "            )\n",
    "            if model_no_fe:\n",
    "                factor_results['No Fixed Effects'] = model_no_fe\n",
    "            \n",
    "            if factor_results:\n",
    "                self.results['coefficient_stability'][ai_factor_name] = factor_results\n",
    "        \n",
    "        print(\" Coefficient Stability testing complete\")\n",
    "\n",
    "    def run_all_endogeneity_tests(self):\n",
    "        \"\"\"Run all endogeneity tests\"\"\"\n",
    "        print(\"\\n Running comprehensive endogeneity testing...\")\n",
    "        \n",
    "        self.test_omitted_variable_bias()\n",
    "        self.test_reverse_causality()\n",
    "        self.test_coefficient_stability()\n",
    "        \n",
    "        print(\" All endogeneity tests complete\")\n",
    "        return self\n",
    "\n",
    "    def _get_significance_stars(self, pvalue):\n",
    "        \"\"\"Get significance stars for p-values\"\"\"\n",
    "        if pd.isna(pvalue):\n",
    "            return \"\"\n",
    "        if pvalue < 0.01:\n",
    "            return \"***\"\n",
    "        elif pvalue < 0.05:\n",
    "            return \"**\"\n",
    "        elif pvalue < 0.10:\n",
    "            return \"*\"\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _create_endogeneity_table(self, doc, title, results_dict, table_num):\n",
    "        \"\"\"Create a table for endogeneity test results\"\"\"\n",
    "        doc.add_heading(f\"Table {table_num}: {title}\", level=2)\n",
    "        \n",
    "        if not results_dict:\n",
    "            doc.add_paragraph(\"No results available for this test.\")\n",
    "            return\n",
    "        \n",
    "        # Get all AI factors and model specifications\n",
    "        ai_factors = list(results_dict.keys())\n",
    "        if not ai_factors:\n",
    "            doc.add_paragraph(\"No AI factors available for this test.\")\n",
    "            return\n",
    "        \n",
    "        # Get model specifications from first AI factor\n",
    "        model_specs = list(results_dict[ai_factors[0]].keys())\n",
    "        \n",
    "        # Create table\n",
    "        table = doc.add_table(rows=1, cols=1 + len(model_specs))\n",
    "        table.style = 'TableGrid'\n",
    "        table.alignment = WD_TABLE_ALIGNMENT.CENTER\n",
    "        \n",
    "        # Header row\n",
    "        hdr_cells = table.rows[0].cells\n",
    "        hdr_cells[0].text = \"AI Factor\"\n",
    "        hdr_cells[0].paragraphs[0].runs[0].font.bold = True\n",
    "        hdr_cells[0].paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        for i, spec in enumerate(model_specs):\n",
    "            hdr_cells[i+1].text = f\"({i+1})\\n{spec}\"\n",
    "            hdr_cells[i+1].paragraphs[0].runs[0].font.bold = True\n",
    "            hdr_cells[i+1].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "            hdr_cells[i+1].paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        # Add rows for each AI factor\n",
    "        for ai_factor in ai_factors:\n",
    "            row = table.add_row().cells\n",
    "            row[0].text = ai_factor\n",
    "            row[0].paragraphs[0].runs[0].font.bold = True\n",
    "            row[0].paragraphs[0].runs[0].font.size = Pt(9)\n",
    "            \n",
    "            factor_results = results_dict[ai_factor]\n",
    "            for i, spec in enumerate(model_specs):\n",
    "                cell = row[i+1]\n",
    "                \n",
    "                if spec in factor_results and factor_results[spec] is not None:\n",
    "                    model = factor_results[spec]\n",
    "                    \n",
    "                    # For reverse causality tests, look for significant lagged variables\n",
    "                    if 'reverse_causality' in title.lower():\n",
    "                        significant_coefs = []\n",
    "                        for param_name in model.params.index:\n",
    "                            if 'lag' in param_name.lower() and param_name != 'const':\n",
    "                                coef = model.params[param_name]\n",
    "                                pval = model.pvalues[param_name]\n",
    "                                stars = self._get_significance_stars(pval)\n",
    "                                if pval < 0.10:  # Show significant coefficients\n",
    "                                    significant_coefs.append(f\"{coef:.3f}{stars}\")\n",
    "                        \n",
    "                        if significant_coefs:\n",
    "                            cell.text = significant_coefs[0]  # Show first significant\n",
    "                        else:\n",
    "                            cell.text = \"None Sig.\"\n",
    "                    else:\n",
    "                        # For other tests, look for the AI factor coefficient\n",
    "                        ai_col = self.average_ai_factor_cols_map.get(ai_factor)\n",
    "                        if ai_col and ai_col in model.params:\n",
    "                            coef = model.params[ai_col]\n",
    "                            pval = model.pvalues[ai_col]\n",
    "                            se = model.bse[ai_col]\n",
    "                            stars = self._get_significance_stars(pval)\n",
    "                            \n",
    "                            # Main coefficient\n",
    "                            p_coef = cell.paragraphs[0]\n",
    "                            p_coef.clear()\n",
    "                            run_coef = p_coef.add_run(f\"{coef:.3f}{stars}\")\n",
    "                            run_coef.font.size = Pt(9)\n",
    "                            p_coef.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "                            \n",
    "                            # Standard error\n",
    "                            p_se = cell.add_paragraph()\n",
    "                            run_se = p_se.add_run(f\"({se:.3f})\")\n",
    "                            run_se.font.italic = True\n",
    "                            run_se.font.size = Pt(8)\n",
    "                            p_se.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "                        else:\n",
    "                            cell.text = \"---\"\n",
    "                else:\n",
    "                    cell.text = \"---\"\n",
    "                \n",
    "                cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        \n",
    "        # Add model statistics\n",
    "        stats_to_show = [\n",
    "            (\"Observations\", \"nobs\"),\n",
    "            (\"R-squared\", \"rsquared\"),\n",
    "            (\"F-statistic\", \"fvalue\")\n",
    "        ]\n",
    "        \n",
    "        for stat_name, stat_attr in stats_to_show:\n",
    "            row = table.add_row().cells\n",
    "            row[0].text = stat_name\n",
    "            row[0].paragraphs[0].runs[0].font.bold = True\n",
    "            row[0].paragraphs[0].runs[0].font.size = Pt(9)\n",
    "            \n",
    "            for i, spec in enumerate(model_specs):\n",
    "                cell = row[i+1]\n",
    "                \n",
    "                # Get first AI factor's results for this spec to show statistics\n",
    "                first_ai_factor = ai_factors[0]\n",
    "                if (spec in results_dict[first_ai_factor] and \n",
    "                    results_dict[first_ai_factor][spec] is not None):\n",
    "                    \n",
    "                    model = results_dict[first_ai_factor][spec]\n",
    "                    \n",
    "                    if hasattr(model, stat_attr):\n",
    "                        stat_val = getattr(model, stat_attr)\n",
    "                        if stat_attr == \"nobs\":\n",
    "                            cell.text = f\"{int(stat_val):,}\"\n",
    "                        else:\n",
    "                            cell.text = f\"{stat_val:.3f}\"\n",
    "                    else:\n",
    "                        cell.text = \"N/A\"\n",
    "                else:\n",
    "                    cell.text = \"---\"\n",
    "                \n",
    "                cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "                cell.paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        doc.add_paragraph()\n",
    "\n",
    "    def create_endogeneity_document(self):\n",
    "        \"\"\"Create the comprehensive endogeneity testing document\"\"\"\n",
    "        print(\"\\n Creating endogeneity testing document...\")\n",
    "        \n",
    "        doc = Document()\n",
    "        \n",
    "        # Set style\n",
    "        style = doc.styles['Normal']\n",
    "        style.font.name = 'Times New Roman'\n",
    "        style.font.size = Pt(10)\n",
    "        \n",
    "        # Title\n",
    "        title = doc.add_heading('Endogeneity Testing for AI Factor Analysis', level=0)\n",
    "        title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        \n",
    "        # Subtitle\n",
    "        subtitle = doc.add_paragraph()\n",
    "        subtitle_run = subtitle.add_run('Testing for Omitted Variable Bias and Reverse Causality')\n",
    "        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        subtitle_run.font.size = Pt(12)\n",
    "        subtitle_run.italic = True\n",
    "        \n",
    "        # Executive Summary\n",
    "        doc.add_heading('Executive Summary', level=1)\n",
    "        summary_text = f\"\"\"\n",
    "        This document presents comprehensive endogeneity testing for the AI factor analysis using {len(self.df):,} firm-year observations \n",
    "        from {self.df['Year'].min()}-{self.df['Year'].max()}. We test for three main sources of endogeneity:\n",
    "        \n",
    "        1. Omitted Variable Bias: We test whether inclusion of management quality proxies (financial ratios, \n",
    "           operational metrics) significantly changes our AI factor coefficients.\n",
    "        \n",
    "        2. Reverse Causality: We test whether past performance predicts current AI disclosure patterns, which \n",
    "           would suggest results are driven by firms' past success rather than genuine AI adoption effects.\n",
    "        \n",
    "        3. Coefficient Stability: We examine sensitivity of main results to different model specifications \n",
    "           and control variable sets.\n",
    "        \n",
    "        Key findings are summarized in the tables below, with detailed interpretation guidelines provided.\n",
    "        \"\"\"\n",
    "        doc.add_paragraph(summary_text)\n",
    "        \n",
    "        # Test 1: Omitted Variable Bias\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Test 1: Omitted Variable Bias', level=1)\n",
    "        \n",
    "        doc.add_paragraph(\"\"\"\n",
    "        We test for omitted variable bias by including management quality proxies in our regressions. \n",
    "        If management quality is correlated with both AI disclosure patterns and future returns, \n",
    "        our baseline results may be biased. We include the following management quality proxies:\n",
    "        \n",
    "         Financial Performance: ROA, ROE, Operating Margin, Profit Margin\n",
    "         Financial Structure: Debt-to-Assets, Asset Turnover  \n",
    "         Market Valuation: Price-to-Earnings, Price-to-Book\n",
    "         Momentum Controls: Recent stock returns (t-1, t-2, t-3 months)\n",
    "        \n",
    "        If coefficients remain stable after including these proxies, it suggests our results are robust \n",
    "        to this form of omitted variable bias.\n",
    "        \"\"\")\n",
    "        \n",
    "        if 'omitted_variable_bias' in self.results:\n",
    "            self._create_endogeneity_table(\n",
    "                doc, \n",
    "                \"Omitted Variable Bias Test\", \n",
    "                self.results['omitted_variable_bias'], \n",
    "                \"1\"\n",
    "            )\n",
    "        \n",
    "        # Test 2: Reverse Causality\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Test 2: Reverse Causality', level=1)\n",
    "        \n",
    "        doc.add_paragraph(\"\"\"\n",
    "        We test for reverse causality by regressing current AI metrics on lagged performance measures. \n",
    "        If past performance strongly predicts current AI disclosure patterns, it suggests that our \n",
    "        results may be driven by successful firms being more likely to discuss AI, rather than \n",
    "        AI disclosure predicting future success.\n",
    "        \n",
    "        We run the following tests:\n",
    "        \n",
    "         AI Factor ~ Lagged Returns (t-1, t-2) + Fixed Effects\n",
    "         AI Factor ~ Lagged Returns + Controls + Fixed Effects  \n",
    "         Current Returns ~ Lagged AI Factor + Controls (forward-looking test)\n",
    "        \n",
    "        Strong significance in the first two tests would indicate reverse causality concerns.\n",
    "        Strong significance in the third test supports genuine predictive power.\n",
    "        \"\"\")\n",
    "        \n",
    "        if 'reverse_causality' in self.results:\n",
    "            self._create_endogeneity_table(\n",
    "                doc, \n",
    "                \"Reverse Causality Test\", \n",
    "                self.results['reverse_causality'], \n",
    "                \"2\"\n",
    "            )\n",
    "        \n",
    "        # Test 3: Coefficient Stability\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Test 3: Coefficient Stability', level=1)\n",
    "        \n",
    "        doc.add_paragraph(\"\"\"\n",
    "        We examine the stability of our AI factor coefficients across different model specifications.\n",
    "        Large changes in coefficients when controls are added may indicate endogeneity concerns.\n",
    "        \n",
    "        Model specifications:\n",
    "        \n",
    "         AI Factor + Fixed Effects Only\n",
    "         AI Factor + Standard Controls + Fixed Effects\n",
    "         AI Factor + Standard Controls + Management Proxies + Fixed Effects\n",
    "         AI Factor + Standard Controls (No Fixed Effects)\n",
    "        \n",
    "        Stable coefficients across specifications suggest robustness to endogeneity.\n",
    "        \"\"\")\n",
    "        \n",
    "        if 'coefficient_stability' in self.results:\n",
    "            self._create_endogeneity_table(\n",
    "                doc, \n",
    "                \"Coefficient Stability Test\", \n",
    "                self.results['coefficient_stability'], \n",
    "                \"3\"\n",
    "            )\n",
    "        \n",
    "        # Interpretation Guidelines\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Interpretation Guidelines', level=1)\n",
    "        \n",
    "        doc.add_paragraph(\"\"\"\n",
    "        Interpreting Endogeneity Test Results:\n",
    "        \n",
    "        1. Omitted Variable Bias:\n",
    "            Small changes in AI factor coefficients when management proxies are added suggest robustness\n",
    "            Large changes (>25% coefficient change) may indicate omitted variable bias\n",
    "            Insignificant management proxies suggest they don't confound our results\n",
    "        \n",
    "        2. Reverse Causality:\n",
    "            Strong significance of lagged performance predicting current AI metrics indicates concern\n",
    "            Weak or insignificant relationships suggest limited reverse causality\n",
    "            Forward-looking tests (current returns ~ lagged AI) should show significance if AI truly predicts performance\n",
    "        \n",
    "        3. Coefficient Stability:\n",
    "            Stable coefficients across specifications indicate robustness\n",
    "            Large changes when controls are added suggest potential endogeneity\n",
    "            Comparison with/without fixed effects shows importance of unobserved heterogeneity control\n",
    "        \n",
    "        Overall Assessment:\n",
    "        If tests show: (1) stable coefficients when management proxies added, (2) weak reverse causality, \n",
    "        and (3) stable coefficients across specifications, then endogeneity concerns are likely limited.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Data Summary\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Data Summary', level=1)\n",
    "        \n",
    "        # Create data summary table\n",
    "        self._create_data_summary_table(doc)\n",
    "        \n",
    "        # Variable Definitions\n",
    "        doc.add_heading('Variable Definitions', level=2)\n",
    "        self._create_variable_definitions_table(doc)\n",
    "        \n",
    "        # General Notes\n",
    "        doc.add_paragraph()\n",
    "        notes_para = doc.add_paragraph()\n",
    "        notes_run = notes_para.add_run(\n",
    "            \"Notes: All regressions use firm-clustered standard errors. Variables are winsorized at 1%/99% levels. \"\n",
    "            \"Fixed effects include year and sector dummies. Management quality proxies include financial performance \"\n",
    "            \"and operational efficiency measures. AI factors are averages across multiple LLM assessments (GPT-4o, \"\n",
    "            \"Gemini Flash 1.5, Gemini Flash 2.5). \"\n",
    "            \"*, **, *** indicate significance at 10%, 5%, and 1% levels respectively.\"\n",
    "        )\n",
    "        notes_run.font.size = Pt(8)\n",
    "        notes_run.italic = True\n",
    "        \n",
    "        try:\n",
    "            doc.save(self.output_path)\n",
    "            print(f\" Endogeneity testing document saved to: {self.output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error saving document: {e}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _create_data_summary_table(self, doc):\n",
    "        \"\"\"Create data summary table\"\"\"\n",
    "        doc.add_paragraph()\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        total_obs = len(self.df)\n",
    "        total_firms = self.df['gvkey'].nunique() if 'gvkey' in self.df.columns else 'N/A'\n",
    "        years = f\"{self.df['Year'].min()}-{self.df['Year'].max()}\" if 'Year' in self.df.columns else 'N/A'\n",
    "        \n",
    "        # Count available variables\n",
    "        ai_factors_count = len(self.average_ai_factor_cols_map)\n",
    "        std_controls_count = len([col for col in self.standard_controls_config.values() \n",
    "                                 if col and col in self.df.columns])\n",
    "        mgmt_proxies_count = len([col for col in self.management_proxies_config.values() \n",
    "                                 if col and col in self.df.columns])\n",
    "        \n",
    "        # Create summary table\n",
    "        table = doc.add_table(rows=1, cols=2)\n",
    "        table.style = 'TableGrid'\n",
    "        \n",
    "        hdr_cells = table.rows[0].cells\n",
    "        hdr_cells[0].text = \"Data Characteristic\"\n",
    "        hdr_cells[1].text = \"Value\"\n",
    "        for cell in hdr_cells:\n",
    "            cell.paragraphs[0].runs[0].font.bold = True\n",
    "            cell.paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        summary_data = [\n",
    "            (\"Total Observations\", f\"{total_obs:,}\"),\n",
    "            (\"Unique Firms\", f\"{total_firms:,}\" if isinstance(total_firms, int) else str(total_firms)),\n",
    "            (\"Time Period\", years),\n",
    "            (\"AI Factors\", str(ai_factors_count)),\n",
    "            (\"Standard Controls\", str(std_controls_count)), \n",
    "            (\"Management Proxies\", str(mgmt_proxies_count)),\n",
    "            (\"Dependent Variable\", self.y_var),\n",
    "            (\"Fixed Effects\", \"Year + Sector\")\n",
    "        ]\n",
    "        \n",
    "        for characteristic, value in summary_data:\n",
    "            row = table.add_row().cells\n",
    "            row[0].text = characteristic\n",
    "            row[1].text = value\n",
    "            for cell in row:\n",
    "                cell.paragraphs[0].runs[0].font.size = Pt(9)\n",
    "\n",
    "    def _create_variable_definitions_table(self, doc):\n",
    "        \"\"\"Create variable definitions table\"\"\"\n",
    "        doc.add_paragraph()\n",
    "        \n",
    "        # Variable definitions\n",
    "        variable_defs = {\n",
    "            **{display_name: f\"Average of {display_name} across GPT-4o, Gemini Flash 1.5, and Gemini Flash 2.5 assessments\" \n",
    "               for display_name in self.average_ai_factor_cols_map.keys()},\n",
    "            **{display_name: description for display_name, description in {\n",
    "                'Log Market Cap': 'Natural logarithm of market capitalization',\n",
    "                'Price-to-Book': 'Market value of equity divided by book value of equity',\n",
    "                'ROA': 'Return on Assets (Net Income / Total Assets)',\n",
    "                'Market Beta': 'Market beta from 6-month factor loading estimation',\n",
    "                'Momentum (t-1m)': 'Stock return over month ending 1 month before filing date',\n",
    "                'ROE': 'Return on Equity (Net Income / Shareholders Equity)',\n",
    "                'Operating Margin': 'Operating Income / Revenue',\n",
    "                'Profit Margin': 'Net Income / Revenue', \n",
    "                'Debt-to-Assets': 'Total Debt / Total Assets',\n",
    "                'Asset Turnover': 'Revenue / Total Assets',\n",
    "                'Price-to-Earnings': 'Market value per share / Earnings per share'\n",
    "            }.items()}\n",
    "        }\n",
    "        \n",
    "        table = doc.add_table(rows=1, cols=2)\n",
    "        table.style = 'TableGrid'\n",
    "        \n",
    "        hdr_cells = table.rows[0].cells\n",
    "        hdr_cells[0].text = \"Variable\"\n",
    "        hdr_cells[1].text = \"Definition\"\n",
    "        for cell in hdr_cells:\n",
    "            cell.paragraphs[0].runs[0].font.bold = True\n",
    "            cell.paragraphs[0].runs[0].font.size = Pt(9)\n",
    "        \n",
    "        for var_name, definition in variable_defs.items():\n",
    "            row = table.add_row().cells\n",
    "            row[0].text = var_name\n",
    "            row[1].text = definition\n",
    "            for cell in row:\n",
    "                cell.paragraphs[0].runs[0].font.size = Pt(9)\n",
    "\n",
    "    def run_complete_endogeneity_analysis(self):\n",
    "        \"\"\"Run complete endogeneity analysis and generate document\"\"\"\n",
    "        print(\" RUNNING COMPREHENSIVE ENDOGENEITY TESTING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            self.load_and_prepare_data()\n",
    "            self.run_all_endogeneity_tests()\n",
    "            self.create_endogeneity_document()\n",
    "            \n",
    "            # Print summary results\n",
    "            self._print_endogeneity_summary()\n",
    "            \n",
    "            return self.results\n",
    "        except Exception as e:\n",
    "            print(f\" Critical error in endogeneity testing: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def _print_endogeneity_summary(self):\n",
    "        \"\"\"Print summary of endogeneity test results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" ENDOGENEITY TESTING SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Omitted Variable Bias Summary\n",
    "        if 'omitted_variable_bias' in self.results:\n",
    "            print(\"\\n1. OMITTED VARIABLE BIAS TEST:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for ai_factor, results in self.results['omitted_variable_bias'].items():\n",
    "                if 'Baseline' in results and 'With Management Proxies' in results:\n",
    "                    baseline_model = results['Baseline']\n",
    "                    mgmt_model = results['With Management Proxies']\n",
    "                    \n",
    "                    ai_col = self.average_ai_factor_cols_map.get(ai_factor)\n",
    "                    if ai_col and ai_col in baseline_model.params and ai_col in mgmt_model.params:\n",
    "                        baseline_coef = baseline_model.params[ai_col]\n",
    "                        mgmt_coef = mgmt_model.params[ai_col]\n",
    "                        \n",
    "                        if baseline_coef != 0:\n",
    "                            pct_change = ((mgmt_coef - baseline_coef) / abs(baseline_coef)) * 100\n",
    "                            print(f\"{ai_factor}:\")\n",
    "                            print(f\"  Baseline coef: {baseline_coef:.3f}\")\n",
    "                            print(f\"  With mgmt proxies: {mgmt_coef:.3f}\")\n",
    "                            print(f\"  Change: {pct_change:.1f}%\")\n",
    "                            \n",
    "                            if abs(pct_change) < 25:\n",
    "                                print(f\"  Assessment:  ROBUST (small change)\")\n",
    "                            else:\n",
    "                                print(f\"  Assessment:  POTENTIAL BIAS (large change)\")\n",
    "        \n",
    "        # Reverse Causality Summary\n",
    "        if 'reverse_causality' in self.results:\n",
    "            print(\"\\n2. REVERSE CAUSALITY TEST:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for ai_factor, results in self.results['reverse_causality'].items():\n",
    "                print(f\"{ai_factor}:\")\n",
    "                \n",
    "                if 'AI ~ Lagged Performance' in results:\n",
    "                    model = results['AI ~ Lagged Performance']\n",
    "                    significant_lags = []\n",
    "                    \n",
    "                    for param_name in model.params.index:\n",
    "                        if 'lag' in param_name.lower() and param_name != 'const':\n",
    "                            pval = model.pvalues[param_name]\n",
    "                            if pval < 0.05:\n",
    "                                significant_lags.append(param_name)\n",
    "                    \n",
    "                    if significant_lags:\n",
    "                        print(f\"   REVERSE CAUSALITY CONCERN: {len(significant_lags)} significant lagged vars\")\n",
    "                    else:\n",
    "                        print(f\"   LIMITED REVERSE CAUSALITY: No significant lagged performance\")\n",
    "                \n",
    "                if 'Current Return ~ Lagged AI' in results:\n",
    "                    model = results['Current Return ~ Lagged AI']\n",
    "                    ai_col_lag = f\"{self.average_ai_factor_cols_map.get(ai_factor)}_lag1\"\n",
    "                    \n",
    "                    if ai_col_lag in model.params:\n",
    "                        pval = model.pvalues[ai_col_lag]\n",
    "                        coef = model.params[ai_col_lag]\n",
    "                        \n",
    "                        if pval < 0.05:\n",
    "                            print(f\"   FORWARD PREDICTIVE POWER: Lagged AI predicts returns (coef: {coef:.3f})\")\n",
    "                        else:\n",
    "                            print(f\"   WEAK FORWARD PREDICTION: Lagged AI doesn't predict returns\")\n",
    "        \n",
    "        # Coefficient Stability Summary\n",
    "        if 'coefficient_stability' in self.results:\n",
    "            print(\"\\n3. COEFFICIENT STABILITY TEST:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for ai_factor, results in self.results['coefficient_stability'].items():\n",
    "                if 'FE Only' in results and '+ Management Proxies' in results:\n",
    "                    fe_model = results['FE Only']\n",
    "                    full_model = results['+ Management Proxies']\n",
    "                    \n",
    "                    ai_col = self.average_ai_factor_cols_map.get(ai_factor)\n",
    "                    if ai_col and ai_col in fe_model.params and ai_col in full_model.params:\n",
    "                        fe_coef = fe_model.params[ai_col]\n",
    "                        full_coef = full_model.params[ai_col]\n",
    "                        \n",
    "                        if fe_coef != 0:\n",
    "                            pct_change = ((full_coef - fe_coef) / abs(fe_coef)) * 100\n",
    "                            print(f\"{ai_factor}:\")\n",
    "                            print(f\"  FE only: {fe_coef:.3f}\")\n",
    "                            print(f\"  Full controls: {full_coef:.3f}\")\n",
    "                            print(f\"  Stability: {100 - abs(pct_change):.1f}%\")\n",
    "                            \n",
    "                            if abs(pct_change) < 25:\n",
    "                                print(f\"  Assessment:  STABLE\")\n",
    "                            else:\n",
    "                                print(f\"  Assessment:  UNSTABLE\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" OVERALL ENDOGENEITY ASSESSMENT:\")\n",
    "        print(\"    = Low endogeneity concern\")\n",
    "        print(\"    = Potential endogeneity concern\") \n",
    "        print(\"   Results should be interpreted with appropriate caution\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Usage example and main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your file paths - UPDATE THESE TO YOUR ACTUAL PATHS\n",
    "    data_path = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/final_clean_dataset_filtered_with_corrected_factor_loadings_with_robust_momentum.csv\"\n",
    "    output_dir = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/EndogeneityTesting/\"\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\" ERROR: Data file not found at {data_path}\")\n",
    "        print(\"Please update the data_path to point to your momentum-enhanced dataset\")\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file_path = os.path.join(output_dir, \"AI_Factor_Endogeneity_Tests_Updated.docx\")\n",
    "        \n",
    "        print(f\" Data file: {data_path}\")\n",
    "        print(f\" Output: {output_file_path}\")\n",
    "        \n",
    "        # Run endogeneity testing\n",
    "        tester = UpdatedEndogeneityTester(data_path=data_path, output_path=output_file_path)\n",
    "        results = tester.run_complete_endogeneity_analysis()\n",
    "        \n",
    "        if results:\n",
    "            print(f\"\\n Endogeneity testing completed successfully!\")\n",
    "            print(f\" Document saved to: {output_file_path}\")\n",
    "            print(f\" Check the console output above for summary results\")\n",
    "            print(f\" Detailed tables and analysis in the Word document\")\n",
    "        else:\n",
    "            print(f\"\\n Endogeneity testing failed - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b76e46-b345-4f5c-bfa5-7f3739b52cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_path = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/final_clean_dataset_filtered_with_corrected_factor_loadings_with_robust_momentum.csv\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
