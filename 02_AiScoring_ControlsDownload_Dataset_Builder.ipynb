{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cee209-b55c-4551-9566-f40193260bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# Define the root output directory as specified in your notebook\n",
    "ROOT_OUT = pathlib.Path(\n",
    "    \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/\"\n",
    "    \"Documents/Master Finance/MasterThesis/ThesisData\"\n",
    ")\n",
    "\n",
    "# Specify the correct filename for the final dataset\n",
    "file_name = \"panel_data_charactermax200k.parquet\"\n",
    "\n",
    "# Construct the full path to the file\n",
    "file_path = ROOT_OUT / file_name\n",
    "\n",
    "# Load the parquet file into a pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(f\"Successfully loaded final dataset from: {file_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Unique CIKs: {df['cik'].nunique()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386db901-027c-4e11-96f1-bccc367406db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI, RateLimitError, APIError\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import instructor\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "from typing import List, Literal, Optional, Union\n",
    "\n",
    "# pydantic model definitions\n",
    "AIStrategicDepthBucket = Literal[\n",
    "    \"A (Core Strategic Enabler)\",\n",
    "    \"B (Significant Operational/Product Integration)\",\n",
    "    \"C (Emerging/Exploratory Mentions)\",\n",
    "    \"D (Superficial/Generic Mentions OR Risk of Non-Adoption Only)\",\n",
    "    \"E (No Mention of Own AI Strategy/Adoption)\"\n",
    "]\n",
    "\n",
    "AIDisclosureSentimentBucket = Literal[\n",
    "    \"A (Clearly Positive)\",\n",
    "    \"B (Mostly Positive/Balanced)\",\n",
    "    \"C (Neutral/Factual)\",\n",
    "    \"D (Cautious/Mixed)\",\n",
    "    \"E (Negative OR Not Applicable - No Own AI Discussion)\"\n",
    "]\n",
    "\n",
    "AIRiskOwnAdoptionBucket = Literal[\n",
    "    \"A (Detailed & Specific Discussion)\",\n",
    "    \"B (General Mention of Own AI Adoption Risk)\",\n",
    "    \"C (No Mention of Own AI Adoption Risk)\"\n",
    "]\n",
    "\n",
    "AIRiskExternalThreatsBucket = Literal[\n",
    "    \"A (Detailed & Specific Discussion)\",\n",
    "    \"B (General Mention of External AI Threat)\",\n",
    "    \"C (No Mention of External AI Threat)\"\n",
    "]\n",
    "\n",
    "AIRiskNonAdoptionBucket = Literal[\n",
    "    \"A (Explicitly Discussed)\",\n",
    "    \"B (Implicitly Suggested)\",\n",
    "    \"C (No Mention of Non-Adoption/Competitive AI Risk)\"\n",
    "]\n",
    "\n",
    "AIForwardLookingBucket = Literal[\n",
    "    \"A (Specific & Detailed Future Plans)\",\n",
    "    \"B (General Future Intent)\",\n",
    "    \"C (Implicit Future Focus Only)\",\n",
    "    \"D (No Mention of Future AI Plans)\"\n",
    "]\n",
    "\n",
    "AIWashingHypeBucket = Literal[\n",
    "    \"A (Substantive & Grounded)\",\n",
    "    \"B (Mostly Substantive)\",\n",
    "    \"C (Mixed - Some Substance, Some Hype)\",\n",
    "    \"D (Mostly Hype)\",\n",
    "    \"E (Pure Hype OR Not Applicable - No Positive AI Claims)\"\n",
    "]\n",
    "\n",
    "AITalentInvestmentBucket = Literal[\n",
    "    \"A (Explicit & Significant Focus)\",\n",
    "    \"B (General Mention of AI Talent/Investment)\",\n",
    "    \"C (Implicit Focus Only)\",\n",
    "    \"D (No Mention of AI Talent/Investment)\"\n",
    "]\n",
    "\n",
    "class BaseAssessment(BaseModel):\n",
    "    supporting_evidence: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Key verbatim quotes (or concise summaries) from the text that justify your assessment. Limit to 2-3 key pieces of evidence. If no direct evidence, provide an empty list [].\"\n",
    "    )\n",
    "    chain_of_thought_reasoning: str = Field(\n",
    "        description=\"A brief explanation (1-2 sentences) of your thought process for arriving at the bucket assessment, referencing the criteria and evidence.\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"supporting_evidence\", mode='before')\n",
    "    @classmethod\n",
    "    def normalize_supporting_evidence(cls, v):\n",
    "        if v is None: return []\n",
    "        if isinstance(v, str):\n",
    "            stripped_v = v.strip()\n",
    "            if stripped_v.lower() in [\"no direct evidence found.\", \"no evidence found.\"]:\n",
    "                return []\n",
    "            return [stripped_v] if stripped_v else []\n",
    "        if isinstance(v, list):\n",
    "            return [\n",
    "                str(item).strip() for item in v \n",
    "                if isinstance(item, str) and \n",
    "                   str(item).strip() and \n",
    "                   str(item).strip().lower() not in [\"no direct evidence found.\", \"no evidence found.\"]\n",
    "            ]\n",
    "        return []\n",
    "\n",
    "class AIStrategicDepthAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIStrategicDepthBucket\n",
    "\n",
    "class AIDisclosureSentimentAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIDisclosureSentimentBucket\n",
    "\n",
    "class AIRiskOwnAdoptionAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIRiskOwnAdoptionBucket\n",
    "\n",
    "class AIRiskExternalThreatsAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIRiskExternalThreatsBucket\n",
    "\n",
    "class AIRiskNonAdoptionAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIRiskNonAdoptionBucket\n",
    "\n",
    "class AIRiskDisclosure(BaseModel):\n",
    "    risk_from_own_ai_adoption: AIRiskOwnAdoptionAssessment\n",
    "    risk_from_external_ai_threats: AIRiskExternalThreatsAssessment\n",
    "    risk_of_not_adopting_ai_or_competitive_ai_threat: AIRiskNonAdoptionAssessment\n",
    "\n",
    "class AIForwardLookingAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIForwardLookingBucket\n",
    "\n",
    "class AIWashingHypeAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIWashingHypeBucket\n",
    "\n",
    "class AITalentInvestmentAssessment(BaseAssessment):\n",
    "    bucket_assessment: AITalentInvestmentBucket\n",
    "\n",
    "class CompanyAIAnalysis(BaseModel):\n",
    "    company_identifier: Optional[str] = Field(default=None, description=\"EXTRACT_COMPANY_NAME_IF_POSSIBLE_ELSE_NULL\")\n",
    "    overall_ai_preparedness_summary_cot: str = Field(description=\"LLM_GENERATED_1_2_SENTENCE_SUMMARY_OF_OVERALL_AI_POSTURE_BASED_ON_ALL_FACTORS\")\n",
    "    \n",
    "    ai_strategic_depth: AIStrategicDepthAssessment\n",
    "    ai_disclosure_sentiment: AIDisclosureSentimentAssessment\n",
    "    ai_risk_disclosure: AIRiskDisclosure\n",
    "    forward_looking_ai_statements: AIForwardLookingAssessment \n",
    "    ai_washing_hype_index: AIWashingHypeAssessment\n",
    "    ai_talent_and_investment_focus: AITalentInvestmentAssessment\n",
    "    \n",
    "    key_ai_related_terminology_found: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        description=\"List any explicit AI-related terms found (e.g., 'artificial intelligence', 'machine learning'). If none, provide an empty list [].\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"key_ai_related_terminology_found\", mode='before')\n",
    "    @classmethod\n",
    "    def normalize_key_terms(cls, v):\n",
    "        if v is None: return []\n",
    "        if isinstance(v, str):\n",
    "            return [term.strip() for term in v.split(',') if term.strip()]\n",
    "        if isinstance(v, list):\n",
    "            return [str(item).strip() for item in v if isinstance(item, str) and str(item).strip()]\n",
    "        return []\n",
    "\n",
    "# prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert financial analyst and AI strategist, specializing in evaluating corporate disclosures for insights into a company's engagement with Artificial Intelligence (AI). Your task is to meticulously analyze the provided text from a company's 10-K filing (specifically excerpts from \"Item 1. Risk Factors\" and \"Item 7. Management's Discussion and Analysis\") and provide a structured JSON output assessing the company's AI preparedness based on the defined factors and criteria.\n",
    "\n",
    "**DOCUMENT CONTEXT:**\n",
    "The provided text below, enclosed in triple backticks (```), contains excerpts from \"Item 1. Risk Factors\" and \"Item 7. Management's Discussion and Analysis (MD&A)\" of a single company's 10-K report.\n",
    "\n",
    "```\n",
    "{document_text}\n",
    "```\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Carefully read the entire provided text.\n",
    "2.  For each of the AI Assessment Factors listed below, evaluate the text based on the defined Buckets & Criteria.\n",
    "3.  Your entire response MUST be a single, valid JSON object. Do not include any explanatory text before or after the JSON object. The JSON object MUST conform to the Pydantic schema `CompanyAIAnalysis`.\n",
    "4.  For each factor, you MUST include in the JSON:\n",
    "    * `bucket_assessment`: The exact string for the category you've assigned (e.g., \"A (Core Strategic Enabler)\", \"E (No Mention of Own AI Strategy/Adoption)\"). It must match one of the predefined bucket descriptions for that factor.\n",
    "    * `chain_of_thought_reasoning`: A brief explanation (1-2 sentences) of your thought process for arriving at the bucket assessment, referencing the criteria and evidence.\n",
    "    * `supporting_evidence`: Key verbatim quotes (or concise summaries of multiple related statements) from the text that justify your assessment. Limit to 2-3 key pieces of evidence. If no direct evidence is found for a factor, you MUST provide an empty list `[]` for this field.\n",
    "5.  If information for a factor or sub-factor is not present in the text, use the appropriate \"No Mention\" bucket assessment for that factor. For `supporting_evidence` in such cases, provide an empty list `[]`.\n",
    "    *IMPORTANT*: Ensure ALL main assessment factor objects (ai_strategic_depth, ai_disclosure_sentiment, ai_risk_disclosure, forward_looking_ai_statements, ai_washing_hype_index, ai_talent_and_investment_focus) are ALWAYS present as keys in your JSON output. If a factor has no relevant information, its `bucket_assessment` should reflect \"No Mention\" (or an equivalent E/D grade for that factor's rubric), and `supporting_evidence` should be an empty list. Do not omit these top-level keys from the JSON.\n",
    "6.  For the `company_identifier` field, extract the company name from the \"Company Name (if known):\" line in the DOCUMENT CONTEXT. If it's \"Unknown Company\" or not clearly identifiable, set it to null.\n",
    "7.  For the `overall_ai_preparedness_summary_cot` field, you MUST provide a 1-2 sentence summary of the company's overall AI posture based on your analysis of all factors.\n",
    "8.  For the `key_ai_related_terminology_found` field, you MUST list any explicit AI-related terms found in the text (e.g., \"artificial intelligence\", \"machine learning\", \"NLP\", \"generative AI\"). If no such terms are found, provide an empty list `[]`.\n",
    "\n",
    "**AI ASSESSMENT FACTORS, BUCKETS & CRITERIA:**\n",
    "(Your detailed criteria here - ensure this is complete in your actual script)\n",
    "**1. AI Strategic Depth:** ...\n",
    "**2. AI Disclosure Sentiment:** ...\n",
    "**3. AI Risk Disclosure:** ...\n",
    "**4. Forward-Looking AI Statements:** ...\n",
    "**5. \"AI Washing\" Hype Index:** ...\n",
    "**6. AI Talent & Investment Focus:** ...\n",
    "\"\"\"\n",
    "\n",
    "# configuration & client initialization for openai\n",
    "print(\"--- starting openai configuration and client initialization ---\")\n",
    "CONFIGURED_OPENAI_API_KEY = None\n",
    "OPENAI_CLIENT_CONFIGURED_SUCCESSFULLY = False\n",
    "INSTRUCTOR_OPENAI_CLIENT = None\n",
    "\n",
    "OPENAI_KEY_FROM_ENV = os.getenv('OPENAI_KEY')\n",
    "if OPENAI_KEY_FROM_ENV:\n",
    "    print(\"found 'OPENAI_KEY' in environment variables.\")\n",
    "    CONFIGURED_OPENAI_API_KEY = OPENAI_KEY_FROM_ENV\n",
    "elif 'OPENAI_KEY' in globals():\n",
    "    print(\"found 'OPENAI_KEY' in global script variables.\")\n",
    "    CONFIGURED_OPENAI_API_KEY = globals()['OPENAI_KEY']\n",
    "else:\n",
    "    print(\"error: 'OPENAI_KEY' is not defined in environment or global script variables.\")\n",
    "\n",
    "if CONFIGURED_OPENAI_API_KEY:\n",
    "    if isinstance(CONFIGURED_OPENAI_API_KEY, str) and CONFIGURED_OPENAI_API_KEY.strip().startswith(\"sk-\"):\n",
    "        print(f\"success: openai api key loaded and appears valid.\")\n",
    "    else:\n",
    "        print(f\"error: openai api key is not a valid non-empty string or does not start with 'sk-'.\")\n",
    "        CONFIGURED_OPENAI_API_KEY = None\n",
    "\n",
    "if CONFIGURED_OPENAI_API_KEY:\n",
    "    print(\"attempting: to create instructor-patched openai client...\")\n",
    "    try:\n",
    "        base_openai_client = OpenAI(api_key=CONFIGURED_OPENAI_API_KEY)\n",
    "        INSTRUCTOR_OPENAI_CLIENT = instructor.from_openai(client=base_openai_client, mode=instructor.Mode.JSON) \n",
    "        OPENAI_CLIENT_CONFIGURED_SUCCESSFULLY = True\n",
    "        print(\"  success: instructor-patched openai client created using json mode.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  error: exception during openai client creation or patching: {e}\")\n",
    "else:\n",
    "    print(\"skipped: openai client creation because api key was not valid or not found.\")\n",
    "\n",
    "if not OPENAI_CLIENT_CONFIGURED_SUCCESSFULLY:\n",
    "    print(\"failure: instructor_openai_client could not be initialized. analysis will not work.\")\n",
    "print(\"--- openai configuration and client initialization finished ---\")\n",
    "\n",
    "\n",
    "# cost tracking configuration\n",
    "GPT4O_MINI_INPUT_COST_PER_MILLION_TOKENS = 0.15\n",
    "GPT4O_MINI_OUTPUT_COST_PER_MILLION_TOKENS = 0.60\n",
    "cumulative_prompt_tokens = 0\n",
    "cumulative_completion_tokens = 0\n",
    "cumulative_cost = 0.0\n",
    "\n",
    "def calculate_and_update_cost(prompt_tokens: int, completion_tokens: int, api_call_made_and_tokens_returned: bool):\n",
    "    global cumulative_prompt_tokens, cumulative_completion_tokens, cumulative_cost\n",
    "    call_cost = 0.0\n",
    "    if prompt_tokens > 0 or completion_tokens > 0:\n",
    "        input_cost = (prompt_tokens / 1_000_000) * GPT4O_MINI_INPUT_COST_PER_MILLION_TOKENS\n",
    "        output_cost = (completion_tokens / 1_000_000) * GPT4O_MINI_OUTPUT_COST_PER_MILLION_TOKENS\n",
    "        call_cost = input_cost + output_cost\n",
    "        tqdm.write(f\"api call tokens: prompt={prompt_tokens}, completion={completion_tokens}. cost for this call attempt: ${call_cost:.6f}.\")\n",
    "        if api_call_made_and_tokens_returned:\n",
    "            cumulative_prompt_tokens += prompt_tokens\n",
    "            cumulative_completion_tokens += completion_tokens\n",
    "            cumulative_cost += call_cost\n",
    "    tqdm.write(f\"cumulative: prompt tokens={cumulative_prompt_tokens}, completion tokens={cumulative_completion_tokens}, cost=${cumulative_cost:.6f}\")\n",
    "    return call_cost\n",
    "\n",
    "# analysis function using instructor with openai\n",
    "def analyze_text_with_openai_instructor(\n",
    "    risk_factors_text: str,\n",
    "    mda_text: str,\n",
    "    company_name: str = \"Unknown Company\",\n",
    "    temperature: float = 0.0\n",
    ") -> (Optional[CompanyAIAnalysis], int, int):\n",
    "    if not INSTRUCTOR_OPENAI_CLIENT:\n",
    "        raise Exception(\"critical: INSTRUCTOR_OPENAI_CLIENT is not available for analysis function.\")\n",
    "\n",
    "    document_text = f\"\"\"Company Name (if known): {company_name}\n",
    "Item 1. Risk Factors:\\n{risk_factors_text}\\n\\nItem 7. Management's Discussion and Analysis (MD&A):\\n{mda_text}\"\"\"\n",
    "    full_prompt = PROMPT_TEMPLATE.format(document_text=document_text)\n",
    "    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "    prompt_tokens_used, completion_tokens_used = 0, 0\n",
    "    analysis_pydantic_object = None\n",
    "    \n",
    "    try:\n",
    "        # when using mode.json, instructor passes response_format={\"type\": \"json_object\"}\n",
    "        parsed_model, completion_obj = INSTRUCTOR_OPENAI_CLIENT.chat.completions.create_with_completion(\n",
    "            model=\"gpt-4o-mini\", \n",
    "            messages=messages,\n",
    "            response_model=CompanyAIAnalysis, \n",
    "            temperature=temperature,\n",
    "        )\n",
    "        analysis_pydantic_object = parsed_model\n",
    "\n",
    "        if completion_obj and hasattr(completion_obj, 'usage') and completion_obj.usage:\n",
    "            prompt_tokens_used = completion_obj.usage.prompt_tokens if completion_obj.usage.prompt_tokens is not None else 0\n",
    "            completion_tokens_used = completion_obj.usage.completion_tokens if completion_obj.usage.completion_tokens is not None else 0\n",
    "        \n",
    "        return analysis_pydantic_object, prompt_tokens_used, completion_tokens_used\n",
    "\n",
    "    except ValidationError as ve:\n",
    "        tqdm.write(f\"pydantic validationerror for {company_name} (instructor/json mode): {str(ve)[:200]}\")\n",
    "        raise ve \n",
    "    except (RateLimitError, APIError, TypeError) as api_err:\n",
    "        raise api_err \n",
    "    except Exception as e_unexpected:\n",
    "        tqdm.write(f\"unexpected error in analyze_text_with_openai_instructor for {company_name}: {type(e_unexpected).__name__} - {str(e_unexpected)[:200]}\")\n",
    "        raise e_unexpected\n",
    "\n",
    "# helper and checkpoint functions\n",
    "def extract_grade_letter(bucket_assessment_string: str) -> str:\n",
    "    if bucket_assessment_string and isinstance(bucket_assessment_string, str):\n",
    "        grade_part = bucket_assessment_string.split(\" \")[0]\n",
    "        if len(grade_part) == 1 and grade_part.isalpha(): return grade_part.upper()\n",
    "    return \"N/A\"\n",
    "\n",
    "def find_latest_openai_checkpoint_info(checkpoint_dir: str, filename_prefix: str) -> (Optional[str], int):\n",
    "    latest_checkpoint_filename = None\n",
    "    max_original_df_rows_processed = 0 \n",
    "    checkpoint_pattern = re.compile(rf\"{filename_prefix}(?:ERROR_)?checkpoint_rows_upto_(\\d+)\\.csv\")\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        tqdm.write(f\"checkpoint directory not found: {checkpoint_dir}\")\n",
    "        return None, 0\n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        match = checkpoint_pattern.match(filename)\n",
    "        if match:\n",
    "            try:\n",
    "                rows_in_this_filename = int(match.group(1))\n",
    "                if rows_in_this_filename > max_original_df_rows_processed:\n",
    "                    max_original_df_rows_processed = rows_in_this_filename\n",
    "                    latest_checkpoint_filename = filename\n",
    "                elif rows_in_this_filename == max_original_df_rows_processed and latest_checkpoint_filename:\n",
    "                    if \"ERROR\" in latest_checkpoint_filename and \"ERROR\" not in filename:\n",
    "                        latest_checkpoint_filename = filename\n",
    "            except ValueError:\n",
    "                tqdm.write(f\"warning: could not parse row count from checkpoint filename: {filename}\")\n",
    "                continue\n",
    "    if latest_checkpoint_filename:\n",
    "        tqdm.write(f\"found latest checkpoint: {latest_checkpoint_filename} (represents processing up to original_df_index {max_original_df_rows_processed -1}).\")\n",
    "    else:\n",
    "        tqdm.write(f\"no valid '{filename_prefix}' checkpoint found in {checkpoint_dir}. starting fresh.\")\n",
    "    return latest_checkpoint_filename, max_original_df_rows_processed\n",
    "\n",
    "def save_openai_checkpoint(results_list_to_save: list, total_original_df_rows_covered: int, base_path: str, filename_prefix: str, is_error_save: bool = False):\n",
    "    if not results_list_to_save:\n",
    "        tqdm.write(\"no new results to save in this checkpoint interval (results_list_to_save is empty).\")\n",
    "        return\n",
    "    temp_df = pd.DataFrame(results_list_to_save)\n",
    "    temp_df['cumulative_prompt_tokens_at_save'] = cumulative_prompt_tokens\n",
    "    temp_df['cumulative_completion_tokens_at_save'] = cumulative_completion_tokens\n",
    "    temp_df['cumulative_cost_at_save'] = cumulative_cost\n",
    "    error_tag = \"ERROR_\" if is_error_save else \"\"\n",
    "    filename = os.path.join(base_path, f\"{filename_prefix}{error_tag}checkpoint_rows_upto_{total_original_df_rows_covered}.csv\")\n",
    "    try:\n",
    "        temp_df.to_csv(filename, index=False)\n",
    "        tqdm.write(f\"--- {'ERROR' if is_error_save else 'Regular'} checkpoint saved: {filename} (covers {total_original_df_rows_covered} original df rows) ---\")\n",
    "    except Exception as e_save:\n",
    "        tqdm.write(f\"error saving checkpoint {filename}: {e_save}\")\n",
    "\n",
    "# main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- main execution for openai detailed analysis (v9 - json mode, corrected columns) ---\")\n",
    "\n",
    "    OUTPUT_BASE_DIR = \"./OpenAI_Analysis_Results/\" \n",
    "    CHECKPOINT_FILENAME_PREFIX = \"openai_detailed_analysis_\"\n",
    "    FINAL_RESULTS_FILENAME = os.path.join(OUTPUT_BASE_DIR, f\"{CHECKPOINT_FILENAME_PREFIX}FINAL_ALL_ROWS.csv\")\n",
    "    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)\n",
    "    \n",
    "    CHECKPOINT_INTERVAL = 50 \n",
    "    API_REQUEST_DELAY_ON_SUCCESS = 0.1 \n",
    "    BACKOFF_DELAYS = [3, 10, 30, 60, 120] \n",
    "    MAX_RETRIES_PER_ROW = len(BACKOFF_DELAYS)\n",
    "\n",
    "    if not OPENAI_CLIENT_CONFIGURED_SUCCESSFULLY:\n",
    "        print(\"critical error: openai client not initialized. exiting.\")\n",
    "        exit()\n",
    "    if 'df' not in globals() or not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        print(\"critical error: input dataframe 'df' not found, not a pandas dataframe, or is empty. exiting.\")\n",
    "        exit()\n",
    "    print(f\"input dataframe 'df' loaded with {len(df)} rows.\")\n",
    "    \n",
    "    input_col_cik = \"cik\" \n",
    "    input_col_ticker = \"tickers_sec\" \n",
    "    input_col_company_name = \"companyName_sec\" \n",
    "    input_col_sector = \"sector_user\" \n",
    "    input_col_year = \"year\" \n",
    "    input_col_filing_date = \"filingDate\" \n",
    "    input_col_risk_factors = \"risk_factors_text\"\n",
    "    input_col_mda = \"item7_mda_text\"\n",
    "\n",
    "    if input_col_cik in df.columns:\n",
    "        df[input_col_cik] = df[input_col_cik].astype(str).str.zfill(10)\n",
    "        print(f\"standardized '{input_col_cik}' column in input dataframe 'df'.\")\n",
    "    else:\n",
    "        print(f\"critical error: input cik column '{input_col_cik}' not found in dataframe 'df'. exiting.\")\n",
    "        exit()\n",
    "\n",
    "    essential_text_cols = [input_col_risk_factors, input_col_mda]\n",
    "    missing_text_cols = [col for col in essential_text_cols if col not in df.columns]\n",
    "    if missing_text_cols:\n",
    "        print(f\"critical error: input dataframe 'df' is missing essential text columns: {missing_text_cols}. exiting.\")\n",
    "        exit()\n",
    "    \n",
    "    identifier_cols_to_check_in_input = [input_col_ticker, input_col_company_name, input_col_sector, input_col_year, input_col_filing_date]\n",
    "    for col in identifier_cols_to_check_in_input:\n",
    "        if col not in df.columns:\n",
    "            print(f\"warning: identifier column '{col}' not found in input dataframe 'df'. it will be 'n/a' or none in results.\")\n",
    "\n",
    "    all_processed_results = []\n",
    "    start_processing_from_original_df_index = 0\n",
    "    cumulative_prompt_tokens = 0\n",
    "    cumulative_completion_tokens = 0\n",
    "    cumulative_cost = 0.0\n",
    "\n",
    "    latest_checkpoint_file, max_original_rows_covered_by_checkpoint = find_latest_openai_checkpoint_info(OUTPUT_BASE_DIR, CHECKPOINT_FILENAME_PREFIX)\n",
    "    \n",
    "    if latest_checkpoint_file:\n",
    "        full_checkpoint_path = os.path.join(OUTPUT_BASE_DIR, latest_checkpoint_file)\n",
    "        try:\n",
    "            tqdm.write(f\"loading data from checkpoint: {full_checkpoint_path}\")\n",
    "            checkpoint_df = pd.read_csv(full_checkpoint_path, dtype={'CIK': str}) \n",
    "            if 'CIK' in checkpoint_df.columns:\n",
    "                checkpoint_df['CIK'] = checkpoint_df['CIK'].astype(str).str.zfill(10)\n",
    "\n",
    "            if not checkpoint_df.empty:\n",
    "                last_chkpt_row = checkpoint_df.iloc[-1]\n",
    "                cumulative_prompt_tokens = int(last_chkpt_row.get('cumulative_prompt_tokens_at_save', 0))\n",
    "                cumulative_completion_tokens = int(last_chkpt_row.get('cumulative_completion_tokens_at_save', 0))\n",
    "                cumulative_cost = float(last_chkpt_row.get('cumulative_cost_at_save', 0.0))\n",
    "                tqdm.write(f\"restored cumulative stats: cost=${cumulative_cost:.6f}, ptokens={cumulative_prompt_tokens}, ctokens={cumulative_completion_tokens}\")\n",
    "\n",
    "                last_row_error_info = last_chkpt_row.get('error')\n",
    "                is_last_row_an_error = pd.notna(last_row_error_info) and str(last_row_error_info).strip() != \"\"\n",
    "\n",
    "                if \"ERROR\" in latest_checkpoint_file and is_last_row_an_error:\n",
    "                    all_processed_results = checkpoint_df.iloc[:-1].to_dict('records')\n",
    "                    start_processing_from_original_df_index = max_original_rows_covered_by_checkpoint - 1 \n",
    "                    tqdm.write(f\"loaded {len(all_processed_results)} successful results. will re-attempt original df index {start_processing_from_original_df_index}.\")\n",
    "                else:\n",
    "                    all_processed_results = checkpoint_df.to_dict('records')\n",
    "                    start_processing_from_original_df_index = max_original_rows_covered_by_checkpoint\n",
    "                    tqdm.write(f\"loaded {len(all_processed_results)} results. resuming from original df index {start_processing_from_original_df_index}.\")\n",
    "            else: tqdm.write(\"checkpoint was empty.\")\n",
    "        except Exception as e_load_chkpt:\n",
    "            tqdm.write(f\"error loading checkpoint {full_checkpoint_path}: {e_load_chkpt}. starting fresh.\")\n",
    "            all_processed_results = []; start_processing_from_original_df_index = 0\n",
    "            cumulative_cost = 0.0; cumulative_prompt_tokens = 0; cumulative_completion_tokens = 0\n",
    "    else: tqdm.write(\"no checkpoint found. starting fresh.\")\n",
    "    \n",
    "    df_to_process_slice = df.iloc[start_processing_from_original_df_index:]\n",
    "\n",
    "    if df_to_process_slice.empty and start_processing_from_original_df_index >= len(df):\n",
    "        tqdm.write(f\"all {len(df)} rows processed based on checkpoints.\")\n",
    "    elif len(df) == 0:\n",
    "        tqdm.write(\"input dataframe 'df' is empty. exiting.\"); exit()\n",
    "    \n",
    "    newly_processed_results_this_session = []\n",
    "    stop_all_processing_flag = False\n",
    "\n",
    "    for original_df_index, row_data in tqdm(df_to_process_slice.iterrows(), \n",
    "                                             initial=0, \n",
    "                                             total=len(df_to_process_slice), \n",
    "                                             desc=f\"Processing Rows (Original df index {start_processing_from_original_df_index} to {len(df)-1})\"):\n",
    "        \n",
    "        current_row_result_dict = {\n",
    "            'CIK': str(row_data.get(input_col_cik, \"N/A_CIK\")).zfill(10),\n",
    "            'Ticker': str(row_data.get(input_col_ticker, \"N/A_Ticker\")),\n",
    "            'Company Name': str(row_data.get(input_col_company_name, \"N/A_CompName\")),\n",
    "            'Sector': str(row_data.get(input_col_sector, \"N/A_Sector\")),\n",
    "            'Year': row_data.get(input_col_year, None), \n",
    "            'filingDate': row_data.get(input_col_filing_date, None),\n",
    "            'Overall Summary': \"N/A\", 'Strategic Depth': \"N/A\", 'Disclosure Sentiment': \"N/A\",\n",
    "            'Risk - Own Adoption': \"N/A\", 'Risk - External Threats': \"N/A\", 'Risk - Non-Adoption': \"N/A\",\n",
    "            'Forward-Looking': \"N/A\", 'AI Washing Index': \"N/A\", 'Talent & Investment': \"N/A\",\n",
    "            'Key AI Terms': \"\", 'error': None,\n",
    "            'api_call_cost_for_row': 0.0, \n",
    "            'prompt_tokens_for_row': 0,\n",
    "            'completion_tokens_for_row': 0\n",
    "        }\n",
    "        cik_val_for_log = current_row_result_dict['CIK']\n",
    "        company_name_for_llm = current_row_result_dict['Company Name'] if current_row_result_dict['Company Name'] not in [\"N/A_CompName\", \"nan\", \"None\", \"\"] else current_row_result_dict['Ticker']\n",
    "        if not company_name_for_llm or company_name_for_llm.lower() == \"n/a_ticker\" or company_name_for_llm.lower() == \"nan\":\n",
    "            company_name_for_llm = f\"CIK_{cik_val_for_log}\"\n",
    "\n",
    "        risk_text_val = str(row_data.get(input_col_risk_factors, \"\"))\n",
    "        mda_text_val = str(row_data.get(input_col_mda, \"\"))\n",
    "\n",
    "        risk_text_is_missing = pd.isna(row_data.get(input_col_risk_factors)) or not risk_text_val.strip()\n",
    "        mda_text_is_missing = pd.isna(row_data.get(input_col_mda)) or not mda_text_val.strip()\n",
    "\n",
    "        if risk_text_is_missing or mda_text_is_missing:\n",
    "            error_message_parts = []\n",
    "            if risk_text_is_missing: error_message_parts.append(f\"missing/empty {input_col_risk_factors}\")\n",
    "            if mda_text_is_missing: error_message_parts.append(f\"missing/empty {input_col_mda}\")\n",
    "            current_row_result_dict['error'] = \" and \".join(error_message_parts)\n",
    "            tqdm.write(f\"skipping cik {cik_val_for_log} (original index: {original_df_index}): {current_row_result_dict['error']}.\")\n",
    "            newly_processed_results_this_session.append(current_row_result_dict)\n",
    "            current_total_original_df_rows_covered = start_processing_from_original_df_index + len(newly_processed_results_this_session)\n",
    "            if len(newly_processed_results_this_session) > 0 and \\\n",
    "               (current_total_original_df_rows_covered % CHECKPOINT_INTERVAL == 0 or \\\n",
    "                original_df_index == df.index[-1]):\n",
    "                save_openai_checkpoint(all_processed_results + newly_processed_results_this_session, \n",
    "                                      current_total_original_df_rows_covered, \n",
    "                                      OUTPUT_BASE_DIR, CHECKPOINT_FILENAME_PREFIX,\n",
    "                                      is_error_save=True)\n",
    "            continue\n",
    "\n",
    "        consecutive_api_failures_for_row = 0\n",
    "        api_call_successful_and_parsed = False\n",
    "        \n",
    "        while consecutive_api_failures_for_row < MAX_RETRIES_PER_ROW:\n",
    "            analysis_object = None\n",
    "            attempt_p_tokens, attempt_c_tokens = 0, 0\n",
    "            \n",
    "            try:\n",
    "                tqdm.write(f\"attempt {consecutive_api_failures_for_row + 1}/{MAX_RETRIES_PER_ROW} for cik {cik_val_for_log} (original index: {original_df_index})\")\n",
    "                analysis_object, attempt_p_tokens, attempt_c_tokens = analyze_text_with_openai_instructor(\n",
    "                    risk_text_val, mda_text_val, company_name_for_llm, temperature=0.0\n",
    "                )\n",
    "                \n",
    "                current_row_result_dict['prompt_tokens_for_row'] += attempt_p_tokens\n",
    "                current_row_result_dict['completion_tokens_for_row'] += attempt_c_tokens\n",
    "                current_call_cost = calculate_and_update_cost(attempt_p_tokens, attempt_c_tokens, True)\n",
    "                current_row_result_dict['api_call_cost_for_row'] += current_call_cost\n",
    "\n",
    "                if analysis_object:\n",
    "                    tqdm.write(f\"success: cik {cik_val_for_log} (original index: {original_df_index}) parsed.\")\n",
    "                    current_row_result_dict.update({\n",
    "                        'Overall Summary': analysis_object.overall_ai_preparedness_summary_cot,\n",
    "                        'Strategic Depth': extract_grade_letter(analysis_object.ai_strategic_depth.bucket_assessment),\n",
    "                        'Disclosure Sentiment': extract_grade_letter(analysis_object.ai_disclosure_sentiment.bucket_assessment),\n",
    "                        'Risk - Own Adoption': extract_grade_letter(analysis_object.ai_risk_disclosure.risk_from_own_ai_adoption.bucket_assessment),\n",
    "                        'Risk - External Threats': extract_grade_letter(analysis_object.ai_risk_disclosure.risk_from_external_ai_threats.bucket_assessment),\n",
    "                        'Risk - Non-Adoption': extract_grade_letter(analysis_object.ai_risk_disclosure.risk_of_not_adopting_ai_or_competitive_ai_threat.bucket_assessment),\n",
    "                        'Forward-Looking': extract_grade_letter(analysis_object.forward_looking_ai_statements.bucket_assessment),\n",
    "                        'AI Washing Index': extract_grade_letter(analysis_object.ai_washing_hype_index.bucket_assessment),\n",
    "                        'Talent & Investment': extract_grade_letter(analysis_object.ai_talent_and_investment_focus.bucket_assessment),\n",
    "                        'Key AI Terms': ', '.join(analysis_object.key_ai_related_terminology_found or []),\n",
    "                        'error': None \n",
    "                    })\n",
    "                    api_call_successful_and_parsed = True\n",
    "                    break \n",
    "                else: \n",
    "                    current_row_result_dict['error'] = 'analysis returned no object without exception.'\n",
    "                    break \n",
    "\n",
    "            except (RateLimitError, APIError) as api_err:\n",
    "                tqdm.write(f\"api error (attempt {consecutive_api_failures_for_row + 1}) for cik {cik_val_for_log}: {type(api_err).__name__} - {api_err}\")\n",
    "                current_row_result_dict['error'] = f'api error attempt {consecutive_api_failures_for_row + 1}: {type(api_err).__name__} - {str(api_err)[:100]}'\n",
    "                consecutive_api_failures_for_row += 1\n",
    "                if consecutive_api_failures_for_row >= MAX_RETRIES_PER_ROW:\n",
    "                    tqdm.write(f\"max retries for api errors on cik {cik_val_for_log}. recording error.\")\n",
    "                    temp_error_save_list = newly_processed_results_this_session + [current_row_result_dict] \n",
    "                    save_openai_checkpoint(all_processed_results + temp_error_save_list,\n",
    "                                          start_processing_from_original_df_index + len(newly_processed_results_this_session) + 1,\n",
    "                                          OUTPUT_BASE_DIR, CHECKPOINT_FILENAME_PREFIX, is_error_save=True)\n",
    "                    break \n",
    "                else:\n",
    "                    delay = BACKOFF_DELAYS[consecutive_api_failures_for_row -1]\n",
    "                    tqdm.write(f\"waiting {delay}s before next retry for cik {cik_val_for_log}.\")\n",
    "                    time.sleep(delay)\n",
    "            \n",
    "            except ValidationError as ve:\n",
    "                tqdm.write(f\"validation error (attempt {consecutive_api_failures_for_row + 1}) for cik {cik_val_for_log}: {str(ve)[:200]}\")\n",
    "                current_row_result_dict['error'] = f'pydantic validationerror: {str(ve)[:150]}'\n",
    "                tqdm.write(f\"terminal error for cik {cik_val_for_log} due to unparsable llm response.\")\n",
    "                break \n",
    "\n",
    "            except Exception as e_unexpected:\n",
    "                tqdm.write(f\"unexpected error (attempt {consecutive_api_failures_for_row + 1}) for cik {cik_val_for_log}: {type(e_unexpected).__name__} - {str(e_unexpected)[:200]}\")\n",
    "                current_row_result_dict['error'] = f'unexpected attempt {consecutive_api_failures_for_row + 1}: {str(e_unexpected)[:100]}'\n",
    "                consecutive_api_failures_for_row += 1\n",
    "                if consecutive_api_failures_for_row >= MAX_RETRIES_PER_ROW:\n",
    "                    tqdm.write(f\"max retries for unexpected errors on cik {cik_val_for_log}. recording error.\")\n",
    "                    break\n",
    "                else:\n",
    "                    delay = BACKOFF_DELAYS[consecutive_api_failures_for_row -1]\n",
    "                    tqdm.write(f\"waiting {delay}s before next retry for cik {cik_val_for_log}.\")\n",
    "                    time.sleep(delay)\n",
    "\n",
    "        newly_processed_results_this_session.append(current_row_result_dict)\n",
    "\n",
    "        if stop_all_processing_flag:\n",
    "            tqdm.write(\"stop flag activated. ending processing loop.\")\n",
    "            break \n",
    "        \n",
    "        current_total_original_df_rows_covered = start_processing_from_original_df_index + len(newly_processed_results_this_session)\n",
    "        if len(newly_processed_results_this_session) > 0 and \\\n",
    "           (current_total_original_df_rows_covered % CHECKPOINT_INTERVAL == 0 or \\\n",
    "            original_df_index == df.index[-1]): \n",
    "            \n",
    "            combined_results_for_checkpoint = all_processed_results + newly_processed_results_this_session\n",
    "            save_openai_checkpoint(combined_results_for_checkpoint, \n",
    "                                  current_total_original_df_rows_covered, \n",
    "                                  OUTPUT_BASE_DIR, \n",
    "                                  CHECKPOINT_FILENAME_PREFIX, \n",
    "                                  is_error_save=bool(current_row_result_dict.get('error')))\n",
    "        \n",
    "        if api_call_successful_and_parsed and API_REQUEST_DELAY_ON_SUCCESS > 0:\n",
    "            time.sleep(API_REQUEST_DELAY_ON_SUCCESS)\n",
    "\n",
    "    final_complete_list_of_results = all_processed_results + newly_processed_results_this_session\n",
    "\n",
    "    if final_complete_list_of_results:\n",
    "        final_save_is_error_state = stop_all_processing_flag and bool(final_complete_list_of_results[-1].get('error'))\n",
    "        save_openai_checkpoint(final_complete_list_of_results, \n",
    "                              len(final_complete_list_of_results), \n",
    "                              OUTPUT_BASE_DIR, \n",
    "                              CHECKPOINT_FILENAME_PREFIX, \n",
    "                              is_error_save=final_save_is_error_state) \n",
    "        \n",
    "        df_results_final_output = pd.DataFrame(final_complete_list_of_results)\n",
    "        try:\n",
    "            if 'CIK' in df_results_final_output.columns:\n",
    "                df_results_final_output['CIK'] = df_results_final_output['CIK'].astype(str).str.zfill(10)\n",
    "\n",
    "            output_columns_ordered = [\n",
    "                'CIK', 'Ticker', 'Company Name', 'Sector', 'Year', 'filingDate',\n",
    "                'Overall Summary', 'Strategic Depth', 'Disclosure Sentiment',\n",
    "                'Risk - Own Adoption', 'Risk - External Threats', 'Risk - Non-Adoption',\n",
    "                'Forward-Looking', 'AI Washing Index', 'Talent & Investment', 'Key AI Terms',\n",
    "                'error', 'api_call_cost_for_row', 'prompt_tokens_for_row', 'completion_tokens_for_row',\n",
    "            ]\n",
    "            final_output_cols_present = [col for col in output_columns_ordered if col in df_results_final_output.columns]\n",
    "            for stat_col in ['cumulative_prompt_tokens_at_save', 'cumulative_completion_tokens_at_save', 'cumulative_cost_at_save']:\n",
    "                if stat_col in df_results_final_output.columns and stat_col not in final_output_cols_present:\n",
    "                    final_output_cols_present.append(stat_col)\n",
    "\n",
    "            df_results_final_output[final_output_cols_present].to_csv(FINAL_RESULTS_FILENAME, index=False)\n",
    "            print(f\"\\n--- processing finished. final results saved to: {FINAL_RESULTS_FILENAME} ({len(df_results_final_output)} rows) ---\")\n",
    "            if not df_results_final_output.empty:\n",
    "                print(df_results_final_output[final_output_cols_present].head().to_string())\n",
    "        except Exception as e_final_save:\n",
    "            print(f\"error saving final results to {FINAL_RESULTS_FILENAME}: {e_final_save}\")\n",
    "    else:\n",
    "        print(\"\\nno results were processed or loaded from checkpoints to save in the final file.\")\n",
    "    \n",
    "    print(f\"\\n--- final cost summary ---\\ntotal prompt tokens: {cumulative_prompt_tokens}\\ntotal completion tokens: {cumulative_completion_tokens}\\ntotal estimated cost: ${cumulative_cost:.6f}\")\n",
    "    print(\"\\n--- main execution finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1302e9f-fffc-4e2d-a576-8081387b5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import instructor\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Literal, Optional, Union\n",
    "\n",
    "# pydantic model definitions\n",
    "# these are the same as the openai version, as the desired output structure is identical.\n",
    "AIStrategicDepthBucket = Literal[\n",
    "    \"A (Core Strategic Enabler)\",\n",
    "    \"B (Significant Operational/Product Integration)\",\n",
    "    \"C (Emerging/Exploratory Mentions)\",\n",
    "    \"D (Superficial/Generic Mentions OR Risk of Non-Adoption Only)\",\n",
    "    \"E (No Mention of Own AI Strategy/Adoption)\"\n",
    "]\n",
    "\n",
    "AIDisclosureSentimentBucket = Literal[\n",
    "    \"A (Clearly Positive)\",\n",
    "    \"B (Mostly Positive/Balanced)\",\n",
    "    \"C (Neutral/Factual)\",\n",
    "    \"D (Cautious/Mixed)\",\n",
    "    \"E (Negative OR Not Applicable - No Own AI Discussion)\"\n",
    "]\n",
    "\n",
    "AIRiskOwnAdoptionBucket = Literal[\n",
    "    \"A (Detailed & Specific Discussion)\",\n",
    "    \"B (General Mention of Own AI Adoption Risk)\",\n",
    "    \"C (No Mention of Own AI Adoption Risk)\"\n",
    "]\n",
    "\n",
    "AIRiskExternalThreatsBucket = Literal[\n",
    "    \"A (Detailed & Specific Discussion)\",\n",
    "    \"B (General Mention of External AI Threat)\",\n",
    "    \"C (No Mention of External AI Threat)\"\n",
    "]\n",
    "\n",
    "AIRiskNonAdoptionBucket = Literal[\n",
    "    \"A (Explicitly Discussed)\",\n",
    "    \"B (Implicitly Suggested)\",\n",
    "    \"C (No Mention of Non-Adoption/Competitive AI Risk)\"\n",
    "]\n",
    "\n",
    "AIForwardLookingBucket = Literal[\n",
    "    \"A (Specific & Detailed Future Plans)\",\n",
    "    \"B (General Future Intent)\",\n",
    "    \"C (Implicit Future Focus Only)\",\n",
    "    \"D (No Mention of Future AI Plans)\"\n",
    "]\n",
    "\n",
    "AIWashingHypeBucket = Literal[\n",
    "    \"A (Substantive & Grounded)\",\n",
    "    \"B (Mostly Substantive)\",\n",
    "    \"C (Mixed - Some Substance, Some Hype)\",\n",
    "    \"D (Mostly Hype)\",\n",
    "    \"E (Pure Hype OR Not Applicable - No Positive AI Claims)\"\n",
    "]\n",
    "\n",
    "AITalentInvestmentBucket = Literal[\n",
    "    \"A (Explicit & Significant Focus)\",\n",
    "    \"B (General Mention of AI Talent/Investment)\",\n",
    "    \"C (Implicit Focus Only)\",\n",
    "    \"D (No Mention of AI Talent/Investment)\"\n",
    "]\n",
    "\n",
    "class BaseAssessment(BaseModel):\n",
    "    supporting_evidence: List[str] = Field(default_factory=list)\n",
    "    chain_of_thought_reasoning: str\n",
    "\n",
    "    @field_validator(\"supporting_evidence\", mode='before')\n",
    "    @classmethod\n",
    "    def normalize_supporting_evidence(cls, v):\n",
    "        if v is None: return []\n",
    "        if isinstance(v, str): return [v] if v.strip() else []\n",
    "        if isinstance(v, list): return [str(item).strip() for item in v if isinstance(item, str) and str(item).strip()]\n",
    "        return []\n",
    "\n",
    "class AIStrategicDepthAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIStrategicDepthBucket\n",
    "\n",
    "class AIDisclosureSentimentAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIDisclosureSentimentBucket\n",
    "\n",
    "class AIRiskOwnAdoptionAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIRiskOwnAdoptionBucket\n",
    "\n",
    "class AIRiskExternalThreatsAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIRiskExternalThreatsBucket\n",
    "\n",
    "class AIRiskNonAdoptionAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIRiskNonAdoptionBucket\n",
    "\n",
    "class AIRiskDisclosure(BaseModel):\n",
    "    risk_from_own_ai_adoption: AIRiskOwnAdoptionAssessment\n",
    "    risk_from_external_ai_threats: AIRiskExternalThreatsAssessment\n",
    "    risk_of_not_adopting_ai_or_competitive_ai_threat: AIRiskNonAdoptionAssessment\n",
    "\n",
    "class AIForwardLookingAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIForwardLookingBucket\n",
    "\n",
    "class AIWashingHypeAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIWashingHypeBucket\n",
    "\n",
    "class AITalentInvestmentAssessment(BaseAssessment):\n",
    "    bucket_assessment: AITalentInvestmentBucket\n",
    "\n",
    "class CompanyAIAnalysis(BaseModel):\n",
    "    company_identifier: Optional[str] = Field(default=None, description=\"EXTRACT_COMPANY_NAME_IF_POSSIBLE_ELSE_NULL\")\n",
    "    overall_ai_preparedness_summary_cot: str = Field(description=\"LLM_GENERATED_1_2_SENTENCE_SUMMARY_OF_OVERALL_AI_POSTURE_BASED_ON_ALL_FACTORS\")\n",
    "    ai_strategic_depth: AIStrategicDepthAssessment\n",
    "    ai_disclosure_sentiment: AIDisclosureSentimentAssessment\n",
    "    ai_risk_disclosure: AIRiskDisclosure\n",
    "    forward_looking_ai_statements: AIForwardLookingAssessment\n",
    "    ai_washing_hype_index: AIWashingHypeAssessment\n",
    "    ai_talent_and_investment_focus: AITalentInvestmentAssessment\n",
    "    key_ai_related_terminology_found: List[str] = Field(default_factory=list, description=\"if_any_explicit_ai_terms_like_machine_learning_nlp_etc\")\n",
    "\n",
    "    @field_validator(\"key_ai_related_terminology_found\", mode='before')\n",
    "    @classmethod\n",
    "    def normalize_key_terms(cls, v):\n",
    "        if v is None: return []\n",
    "        if isinstance(v, str): return [term.strip() for term in v.split(',') if term.strip()]\n",
    "        if isinstance(v, list): return [str(item).strip() for item in v if isinstance(item, str) and str(item).strip()]\n",
    "        return []\n",
    "\n",
    "# prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert financial analyst and AI strategist, specializing in evaluating corporate disclosures for insights into a company's engagement with Artificial Intelligence (AI). Your task is to meticulously analyze the provided text from a company's 10-K filing (specifically excerpts from \"Item 1. Risk Factors\" and \"Item 7. Management's Discussion and Analysis\") and provide a structured JSON output assessing the company's AI preparedness based on the defined factors and criteria.\n",
    "\n",
    "**DOCUMENT CONTEXT:**\n",
    "The provided text below, enclosed in triple backticks (```), contains excerpts from \"Item 1. Risk Factors\" and \"Item 7. Management's Discussion and Analysis (MD&A)\" of a single company's 10-K report.\n",
    "\n",
    "```\n",
    "{document_text}\n",
    "```\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Carefully read the entire provided text.\n",
    "2.  For each of the AI Assessment Factors listed below, evaluate the text based on the defined Buckets & Criteria.\n",
    "3.  Provide your entire response as a single, valid JSON object. Do not include any explanatory text before or after the JSON object. The JSON object should conform to the Pydantic schema provided implicitly by the system.\n",
    "4.  For each factor, include:\n",
    "    * `bucket_assessment`: The exact string for the category you've assigned (e.g., \"A (Core Strategic Enabler)\", \"E (No Mention of Own AI Strategy/Adoption)\"). It must match one of the predefined bucket descriptions for that factor.\n",
    "    * `supporting_evidence`: Key verbatim quotes (or concise summaries of multiple related statements) from the text that justify your assessment. Limit to 2-3 key pieces of evidence. If no direct evidence, provide an empty list or a list containing \"No direct evidence found.\".\n",
    "    * `chain_of_thought_reasoning`: A brief explanation (1-2 sentences) of your thought process for arriving at the bucket assessment, referencing the criteria and evidence.\n",
    "5.  If information for a factor or sub-factor is not present in the text, use the appropriate \"No Mention\" bucket assessment for that factor and provide an empty list or [\"No direct evidence found.\"] for `supporting_evidence`.\n",
    "\n",
    "**AI ASSESSMENT FACTORS, BUCKETS & CRITERIA:**\n",
    "\n",
    "**1. AI Strategic Depth:**\n",
    "    * *Concept:* How deeply and explicitly AI is integrated into the company's core business strategy and future plans, as evidenced by concrete examples, initiatives, or stated intentions.\n",
    "    * *Buckets & Criteria:*\n",
    "        * `A (Core Strategic Enabler)`: AI explicitly central to core strategy, key driver of competitive advantage/future growth, multiple concrete examples linked to strategic objectives.\n",
    "        * `B (Significant Operational/Product Integration)`: AI explicitly enhances key operations/products with specific examples, strategic for those areas.\n",
    "        * `C (Emerging/Exploratory Mentions)`: AI mentioned for isolated projects, pilot programs, or limited exploration; strategic link less developed.\n",
    "        * `D (Superficial/Generic Mentions OR Risk of Non-Adoption Only)`: Generic AI statements without concrete examples OR AI *only* mentioned as a risk of non-adoption by the company.\n",
    "        * `E (No Mention of Own AI Strategy/Adoption)`: No discussion of the company's own AI strategy, adoption, or specific AI initiatives.\n",
    "\n",
    "**2. AI Disclosure Sentiment:**\n",
    "    * *Concept:* The overall tone of the company's *own statements* regarding its AI initiatives, capabilities, and future AI plans.\n",
    "    * *Buckets & Criteria:*\n",
    "        * `A (Clearly Positive)`: Optimistic language on AI benefits/achievements/opportunities, backed by specifics.\n",
    "        * `B (Mostly Positive/Balanced)`: Generally positive, perhaps tempered with neutral statements or acknowledged challenges.\n",
    "        * `C (Neutral/Factual)`: AI discussed objectively, descriptively, without strong positive/negative connotations.\n",
    "        * `D (Cautious/Mixed)`: AI discussions highlight significant uncertainties/challenges/risks of *own* AI efforts, or tone is wary.\n",
    "        * `E (Negative OR Not Applicable - No Own AI Discussion)`: Predominantly negative language regarding own AI experiences OR no discussion of own AI initiatives.\n",
    "\n",
    "**3. AI Risk Disclosure:**\n",
    "    * *Concept:* How thoroughly and specifically the company acknowledges and discusses potential risks associated with AI. Assessed across three sub-factors:\n",
    "    * **3a. Risk from Company's Own AI Adoption:** (e.g., AI system failure, data bias, ethical concerns, implementation challenges).\n",
    "        * *Buckets & Criteria for 3a:*\n",
    "            * `A (Detailed & Specific Discussion)`: Specific risks of *own AI adoption* identified and discussed in detail, perhaps with impacts/mitigation.\n",
    "            * `B (General Mention of Own AI Adoption Risk)`: Risks of *own AI adoption* mentioned generally, less detail.\n",
    "            * `C (No Mention of Own AI Adoption Risk)`: This specific type of risk is not mentioned.\n",
    "    * **3b. Risk from External AI (Threats):** (e.g., AI used in cyberattacks by others, AI-driven misinformation).\n",
    "        * *Buckets & Criteria for 3b:*\n",
    "            * `A (Detailed & Specific Discussion)`: Specific *external AI threats* identified and discussed in detail.\n",
    "            * `B (General Mention of External AI Threat)`: External AI threats mentioned generally.\n",
    "            * `C (No Mention of External AI Threat)`: This specific type of risk is not mentioned.\n",
    "    * **3c. Risk of Not Adopting AI / Competitive AI Threat:** (e.g., falling behind AI-leveraging competitors, failure to innovate using AI).\n",
    "        * *Buckets & Criteria for 3c:*\n",
    "            * `A (Explicitly Discussed)`: Risk of not adopting AI or being outcompeted by AI-using rivals is explicitly stated and perhaps detailed.\n",
    "            * `B (Implicitly Suggested)`: Competitive pressures from technologically advanced rivals are mentioned, technology adoption is stressed, but AI not explicitly named as the specific risk of non-adoption.\n",
    "            * `C (No Mention of Non-Adoption/Competitive AI Risk)`: This specific type of risk is not mentioned.\n",
    "\n",
    "**4. Forward-Looking AI Statements:**\n",
    "    * *Concept:* The extent and specificity of discussion about future plans, investments, or expected impact related to the company's own AI initiatives.\n",
    "    * *Buckets & Criteria:*\n",
    "        * `A (Specific & Detailed Future Plans)`: Concrete future AI initiatives, R&D, investments, or launches described with some detail.\n",
    "        * `B (General Future Intent)`: General intention to explore/invest in/utilize AI, without specific details.\n",
    "        * `C (Implicit Future Focus Only)`: General future-oriented tech/digital strategies mentioned that *might* include AI, but AI not explicitly named in forward-looking plans.\n",
    "        * `D (No Mention of Future AI Plans)`: No forward-looking statements regarding own AI initiatives.\n",
    "\n",
    "**5. \"AI Washing\" Hype Index:** (Factor is only applicable if positive claims about own AI are made)\n",
    "    * *Concept:* Degree to which a company's positive AI claims appear substantive vs. exaggerated or vague.\n",
    "    * *Buckets & Criteria:*\n",
    "        * `A (Substantive & Grounded)`: Positive AI claims consistently backed by specific examples, clear use cases, tangible outcomes, or detailed projects/investments. Risk acknowledgement of own AI can signal substance.\n",
    "        * `B (Mostly Substantive)`: Most positive AI claims supported by specifics, some mild aspirational language.\n",
    "        * `C (Mixed - Some Substance, Some Hype)`: Some specific AI applications mentioned, but interspersed with buzzwords or generic/aspirational statements.\n",
    "        * `D (Mostly Hype)`: Predominantly vague claims, AI buzzwords, general AI potential without concrete company-specific examples.\n",
    "        * `E (Pure Hype OR Not Applicable - No Positive AI Claims)`: AI mentioned in purely aspirational/marketing sense with no support OR no positive claims about own AI are made.\n",
    "\n",
    "**6. AI Talent & Investment Focus:**\n",
    "    * *Concept:* Explicit discussion of focus on acquiring/developing AI talent, or specific investments in AI R&D, technology, partnerships, or infrastructure.\n",
    "    * *Buckets & Criteria:*\n",
    "        * `A (Explicit & Significant Focus)`: Specific initiatives for AI talent (hiring, training, dedicated teams), AI R&D spending, or substantial investments/partnerships in AI detailed.\n",
    "        * `B (General Mention of AI Talent/Investment)`: Mentions tech talent implicitly including AI, or general statements about investing in tech/R&D where AI is likely but not singled out.\n",
    "        * `C (Implicit Focus Only)`: Significant R&D in tech-heavy areas or major \"business technology transformation\" mentioned, but AI not explicitly linked to talent/investment items.\n",
    "        * `D (No Mention of AI Talent/Investment)`: No explicit or strong implicit discussion of AI-specific talent or investments.\n",
    "\n",
    "**EXPECTED JSON OUTPUT STRUCTURE (Example for LLM guidance, actual structure enforced by Pydantic):**\n",
    "```json\n",
    "{{\n",
    "  \"company_identifier\": \"EXTRACT_COMPANY_NAME_IF_POSSIBLE_ELSE_NULL\",\n",
    "  \"overall_ai_preparedness_summary_cot\": \"LLM_GENERATED_1_2_SENTENCE_SUMMARY_OF_OVERALL_AI_POSTURE_BASED_ON_ALL_FACTORS\",\n",
    "  \"ai_strategic_depth\": {{\n",
    "    \"bucket_assessment\": \"A (Core Strategic Enabler)\",\n",
    "    \"supporting_evidence\": [\"quote1\", \"quote2\"],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"ai_disclosure_sentiment\": {{\n",
    "    \"bucket_assessment\": \"B (Mostly Positive/Balanced)\",\n",
    "    \"supporting_evidence\": [\"quote1\"],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"ai_risk_disclosure\": {{\n",
    "    \"risk_from_own_ai_adoption\": {{\n",
    "      \"bucket_assessment\": \"A (Detailed & Specific Discussion)\",\n",
    "      \"supporting_evidence\": [\"quote1\"],\n",
    "      \"chain_of_thought_reasoning\": \"Your reasoning for this sub-factor.\"\n",
    "    }},\n",
    "    \"risk_from_external_ai_threats\": {{\n",
    "      \"bucket_assessment\": \"B (General Mention of External AI Threat)\",\n",
    "      \"supporting_evidence\": [\"quote1\"],\n",
    "      \"chain_of_thought_reasoning\": \"Your reasoning for this sub-factor.\"\n",
    "    }},\n",
    "    \"risk_of_not_adopting_ai_or_competitive_ai_threat\": {{\n",
    "      \"bucket_assessment\": \"C (No Mention of Non-Adoption/Competitive AI Risk)\",\n",
    "      \"supporting_evidence\": [\"No direct evidence found.\"],\n",
    "      \"chain_of_thought_reasoning\": \"Your reasoning for this sub-factor.\"\n",
    "    }}\n",
    "  }},\n",
    "  \"forward_looking_ai_statements\": {{\n",
    "    \"bucket_assessment\": \"D (No Mention of Future AI Plans)\",\n",
    "    \"supporting_evidence\": [],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"ai_washing_hype_index\": {{\n",
    "    \"bucket_assessment\": \"E (Pure Hype OR Not Applicable - No Positive AI Claims)\",\n",
    "    \"supporting_evidence\": [\"No positive AI claims made.\"],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"ai_talent_and_investment_focus\": {{\n",
    "    \"bucket_assessment\": \"A (Explicit & Significant Focus)\",\n",
    "    \"supporting_evidence\": [\"quote relating to talent or investment\"],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"key_ai_related_terminology_found\": [\"artificial intelligence\", \"machine learning\"]\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# configuration and client initialization for gemini\n",
    "print(\"--- starting gemini configuration and client initialization ---\")\n",
    "CONFIGURED_GEMINI_API_KEY = None\n",
    "GENAI_CONFIGURED_SUCCESSFULLY = False\n",
    "INSTRUCTOR_GEMINI_CLIENT = None\n",
    "\n",
    "if 'gemini_api_key' in globals():\n",
    "    print(\"using pre-defined global 'gemini_api_key'.\")\n",
    "else:\n",
    "    print(\"info: 'gemini_api_key' is not defined globally. ensure it is set if required.\")\n",
    "    gemini_api_key = None\n",
    "\n",
    "if gemini_api_key is None:\n",
    "    print(\"error: global variable 'gemini_api_key' is not defined or not found via other methods.\")\n",
    "else:\n",
    "    CONFIGURED_GEMINI_API_KEY = gemini_api_key\n",
    "    if isinstance(CONFIGURED_GEMINI_API_KEY, str) and CONFIGURED_GEMINI_API_KEY.strip():\n",
    "        print(f\"success: 'gemini_api_key' variable found and is a non-empty string.\")\n",
    "    else:\n",
    "        print(f\"error: 'gemini_api_key' variable is not a valid non-empty string.\")\n",
    "        CONFIGURED_GEMINI_API_KEY = None\n",
    "\n",
    "if CONFIGURED_GEMINI_API_KEY:\n",
    "    try:\n",
    "        genai.configure(api_key=CONFIGURED_GEMINI_API_KEY)\n",
    "        print(\"success: genai.configure(api_key=...) called successfully.\")\n",
    "        GENAI_CONFIGURED_SUCCESSFULLY = True\n",
    "    except Exception as e:\n",
    "        print(f\"error: exception during genai.configure(api_key=...): {e}\")\n",
    "else:\n",
    "    print(\"skipped: genai.configure() because api key was not valid or not found.\")\n",
    "\n",
    "if GENAI_CONFIGURED_SUCCESSFULLY:\n",
    "    print(\"attempting: to create instructor-patched gemini client...\")\n",
    "    try:\n",
    "        print(\"  step 1: creating base genai.generativemodel('models/gemini-1.5-flash-latest')...\")\n",
    "        base_gemini_model_instance = genai.GenerativeModel(model_name='models/gemini-1.5-flash-latest')\n",
    "        print(\"  success: base genai.generativemodel created.\")\n",
    "        print(\"  step 2: creating client with instructor.from_gemini using instructor.mode.gemini_json...\")\n",
    "        INSTRUCTOR_GEMINI_CLIENT = instructor.from_gemini(\n",
    "            client=base_gemini_model_instance,\n",
    "            mode=instructor.Mode.GEMINI_JSON\n",
    "        )\n",
    "        print(\"  success: client created with instructor.from_gemini using gemini_json mode.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  error: exception during instructor.from_gemini client creation: {e}\")\n",
    "        INSTRUCTOR_GEMINI_CLIENT = None\n",
    "else:\n",
    "    print(\"skipped: instructor client creation for gemini because genai library was not configured successfully.\")\n",
    "\n",
    "if INSTRUCTOR_GEMINI_CLIENT:\n",
    "    print(\"success: instructor_gemini_client appears to be initialized.\")\n",
    "else:\n",
    "    print(\"failure: instructor_gemini_client could not be initialized. analysis will not work.\")\n",
    "\n",
    "print(\"--- gemini configuration and client initialization finished ---\")\n",
    "\n",
    "# analysis function using instructor with gemini\n",
    "def analyze_text_with_gemini_instructor(\n",
    "    risk_factors_text: str,\n",
    "    mda_text: str,\n",
    "    company_name: str = \"Unknown Company\",\n",
    "    max_retries: int = 1, # instructor's internal retries, set to 1 as we handle retries externally\n",
    "    temperature: float = 0.0\n",
    ") -> Optional[CompanyAIAnalysis]:\n",
    "    if not INSTRUCTOR_GEMINI_CLIENT:\n",
    "        tqdm.write(f\"error in analyze_text_with_gemini_instructor for {company_name}: instructor_gemini_client is not available.\")\n",
    "        return None\n",
    "\n",
    "    document_text = f\"\"\"Company Name (if known): {company_name}\n",
    "\n",
    "Item 1. Risk Factors:\n",
    "{risk_factors_text}\n",
    "\n",
    "Item 7. Management's Discussion and Analysis (MD&A):\n",
    "{mda_text}\n",
    "    \"\"\"\n",
    "    full_prompt = PROMPT_TEMPLATE.format(document_text=document_text)\n",
    "\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "        # note: max_retries for instructor.from_gemini is for the specific call attempt.\n",
    "        # our script handles higher-level retries for the row.\n",
    "        if hasattr(INSTRUCTOR_GEMINI_CLIENT, 'messages') and hasattr(INSTRUCTOR_GEMINI_CLIENT.messages, 'create'):\n",
    "            analysis_response = INSTRUCTOR_GEMINI_CLIENT.messages.create(\n",
    "                messages=messages,\n",
    "                response_model=CompanyAIAnalysis,\n",
    "                max_retries=max_retries, \n",
    "                generation_config={\"temperature\": temperature}\n",
    "            )\n",
    "        elif hasattr(INSTRUCTOR_GEMINI_CLIENT, 'generate_content'):\n",
    "            analysis_response = INSTRUCTOR_GEMINI_CLIENT.generate_content(\n",
    "                contents=[full_prompt],\n",
    "                response_model=CompanyAIAnalysis,\n",
    "                max_retries=max_retries,\n",
    "                generation_config=genai.types.GenerationConfig(temperature=temperature)\n",
    "            )\n",
    "        else:\n",
    "            tqdm.write(f\"error for {company_name}: patched instructor_gemini_client does not have a recognized method.\")\n",
    "            return None\n",
    "        return analysis_response\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"error in analyze_text_with_gemini_instructor: exception during api call for {company_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# helper function to extract letter grade\n",
    "def extract_grade_letter(bucket_assessment_string: str) -> str:\n",
    "    if bucket_assessment_string and isinstance(bucket_assessment_string, str):\n",
    "        grade_part = bucket_assessment_string.split(\" \")[0]\n",
    "        if len(grade_part) == 1 and grade_part.isalpha():\n",
    "            return grade_part.upper()\n",
    "    return \"N/A\"\n",
    "\n",
    "# function to find the latest checkpoint\n",
    "def find_latest_checkpoint_info(checkpoint_dir: str) -> (Optional[str], int):\n",
    "    latest_checkpoint_filename = None\n",
    "    max_rows_processed = 0 # this will be the number in the filename, e.g., from \"upto_n\"\n",
    "    checkpoint_pattern = re.compile(r\"gemini_analysis_(?:ERROR_)?checkpoint_rows_upto_(\\d+)\\.csv\")\n",
    "\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        tqdm.write(f\"checkpoint directory {checkpoint_dir} does not exist. starting fresh.\")\n",
    "        return None, 0\n",
    "\n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        match = checkpoint_pattern.match(filename)\n",
    "        if match:\n",
    "            try:\n",
    "                rows = int(match.group(1))\n",
    "                if rows > max_rows_processed:\n",
    "                    max_rows_processed = rows\n",
    "                    latest_checkpoint_filename = filename\n",
    "            except ValueError:\n",
    "                tqdm.write(f\"warning: could not parse row count from checkpoint filename: {filename}\")\n",
    "                continue\n",
    "\n",
    "    if latest_checkpoint_filename:\n",
    "        # max_rows_processed is the n from \"upto_n\". this n represents the count of rows in that file.\n",
    "        tqdm.write(f\"found latest checkpoint: {latest_checkpoint_filename} (contains {max_rows_processed} rows).\")\n",
    "    else:\n",
    "        tqdm.write(\"no valid checkpoint files found. starting fresh.\")\n",
    "    return latest_checkpoint_filename, max_rows_processed\n",
    "\n",
    "# function to save checkpoint\n",
    "def save_checkpoint(list_to_save: list, num_rows_in_list: int, base_path: str, is_error_save: bool = False):\n",
    "    if not list_to_save: # should not happen if called correctly, but good check\n",
    "        tqdm.write(\"no results to save in checkpoint (list_to_save is empty).\")\n",
    "        return\n",
    "\n",
    "    temp_df = pd.DataFrame(list_to_save)\n",
    "    prefix = \"gemini_analysis_ERROR_checkpoint_rows_upto_\" if is_error_save else \"gemini_analysis_checkpoint_rows_upto_\"\n",
    "    # num_rows_in_list is the actual count of items being saved\n",
    "    filename = os.path.join(base_path, f\"{prefix}{num_rows_in_list}.csv\") \n",
    "    \n",
    "    try:\n",
    "        temp_df.to_csv(filename, index=False)\n",
    "        status_message = \"ERROR\" if is_error_save else \"Regular\"\n",
    "        tqdm.write(f\"--- {status_message} checkpoint saved: {filename} ({num_rows_in_list} total rows in this file) ---\")\n",
    "    except Exception as e_save:\n",
    "        tqdm.write(f\"error saving {status_message.lower()} checkpoint {filename}: {e_save}\")\n",
    "\n",
    "# main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- main execution for gemini analysis ---\")\n",
    "\n",
    "    checkpoint_base_path = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/GeminiResults/\"\n",
    "    os.makedirs(checkpoint_base_path, exist_ok=True)\n",
    "\n",
    "    checkpoint_interval = 50 \n",
    "    api_request_delay_on_success = 1.0 \n",
    "\n",
    "    backoff_delays = [30, 60, 300]  # delays in seconds: 30s, 1min, 5min\n",
    "    max_retries_per_row = len(backoff_delays) # max attempts for a single row after initial try\n",
    "\n",
    "    if not CONFIGURED_GEMINI_API_KEY or not GENAI_CONFIGURED_SUCCESSFULLY or not INSTRUCTOR_GEMINI_CLIENT:\n",
    "        print(\"critical error: gemini api key not configured or instructor client not initialized. cannot proceed.\")\n",
    "    elif 'df' not in globals() or not isinstance(df, pd.DataFrame):\n",
    "        print(\"critical error: dataframe 'df' not found or is not a pandas dataframe. please load your dataframe first.\")\n",
    "    elif df.empty:\n",
    "        print(\"critical error: dataframe 'df' is empty. no data to analyze.\")\n",
    "    else:\n",
    "        print(f\"input dataframe 'df' has {len(df)} rows.\")\n",
    "\n",
    "        all_finalized_results_list = [] # stores rows that are successfully processed or skipped due to input error\n",
    "        start_row_index_for_df = 0      # the 0-indexed row in the original 'df' to start processing from\n",
    "\n",
    "        latest_checkpoint_file, num_rows_in_latest_file = find_latest_checkpoint_info(checkpoint_base_path)\n",
    "\n",
    "        if latest_checkpoint_file:\n",
    "            full_checkpoint_path = os.path.join(checkpoint_base_path, latest_checkpoint_file)\n",
    "            try:\n",
    "                tqdm.write(f\"loading data from checkpoint: {full_checkpoint_path}\")\n",
    "                checkpoint_df = pd.read_csv(full_checkpoint_path)\n",
    "                if not checkpoint_df.empty:\n",
    "                    if \"ERROR\" in latest_checkpoint_file and checkpoint_df.iloc[-1].get('error') is not None and str(checkpoint_df.iloc[-1].get('error')).strip() != \"\":\n",
    "                        # last row in error checkpoint was a failure, so we need to retry it.\n",
    "                        # load all rows *except* the last one into finalized results.\n",
    "                        all_finalized_results_list = checkpoint_df.iloc[:-1].to_dict('records')\n",
    "                        start_row_index_for_df = len(all_finalized_results_list) \n",
    "                        tqdm.write(f\"loaded {len(all_finalized_results_list)} finalized rows. will retry row index {start_row_index_for_df} (0-indexed).\")\n",
    "                    else:\n",
    "                        # regular checkpoint, or error checkpoint where last row wasn't a script-logged failure\n",
    "                        all_finalized_results_list = checkpoint_df.to_dict('records')\n",
    "                        start_row_index_for_df = len(all_finalized_results_list)\n",
    "                        tqdm.write(f\"loaded {len(all_finalized_results_list)} finalized rows. resuming from next row (index {start_row_index_for_df}).\")\n",
    "                else: # checkpoint file was empty\n",
    "                    tqdm.write(\"checkpoint file was empty. starting fresh.\")\n",
    "                    start_row_index_for_df = 0\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"error loading checkpoint file {full_checkpoint_path}: {e}. starting from scratch.\")\n",
    "                all_finalized_results_list = []\n",
    "                start_row_index_for_df = 0\n",
    "        else:\n",
    "            tqdm.write(\"no checkpoint found. starting processing from the beginning.\")\n",
    "\n",
    "        identifier_columns = [\"cik\", \"tickers_sec\", \"companyName_sec\", \"sector_user\"]\n",
    "        text_columns = [\"risk_factors_text\", \"item7_mda_text\"]\n",
    "        all_required_cols_for_df = identifier_columns + text_columns\n",
    "\n",
    "        missing_schema_cols = [col for col in all_required_cols_for_df if col not in df.columns]\n",
    "        if missing_schema_cols:\n",
    "            print(f\"critical error: dataframe 'df' is missing required columns: {missing_schema_cols}. aborting.\")\n",
    "        else:\n",
    "            df_to_process = df.iloc[start_row_index_for_df:]\n",
    "            \n",
    "            if df_to_process.empty and start_row_index_for_df > 0 :\n",
    "                tqdm.write(f\"all {start_row_index_for_df} rows from input dataframe appear to be processed based on checkpoints.\")\n",
    "            elif df_to_process.empty and start_row_index_for_df == 0:\n",
    "                tqdm.write(\"input dataframe is empty. no data to process.\")\n",
    "\n",
    "            rows_processed_this_session = 0 # counts rows finalized in the current run\n",
    "            stop_all_processing_flag = False\n",
    "\n",
    "            # outer loop: iterates through rows of the input dataframe that need processing\n",
    "            for original_df_index_val, row_data in tqdm(df_to_process.iterrows(), total=df_to_process.shape[0], desc=\"Processing DataFrame Rows\"):\n",
    "                # original_df_index_val is the true index from the original 'df'\n",
    "                \n",
    "                cik_val = str(row_data.get(\"cik\", \"N/A\"))\n",
    "                ticker_val = str(row_data.get(\"tickers_sec\", \"N/A\"))\n",
    "                company_name_val = str(row_data.get(\"companyName_sec\", \"N/A\"))\n",
    "                sector_val = str(row_data.get(\"sector_user\", \"N/A\"))\n",
    "\n",
    "                company_name_for_llm_analysis = company_name_val if company_name_val and company_name_val.lower() != \"nan\" else ticker_val\n",
    "                if not company_name_for_llm_analysis or company_name_for_llm_analysis.lower() == \"nan\":\n",
    "                    company_name_for_llm_analysis = f\"CIK_{cik_val}\"\n",
    "\n",
    "                # this dictionary will hold the result for the current row\n",
    "                current_row_result_dict = {\n",
    "                    'CIK': cik_val, 'Ticker': ticker_val, 'Company Name': company_name_val, 'Sector': sector_val,\n",
    "                    'Overall Summary': \"N/A\", 'Strategic Depth': \"N/A\", 'Disclosure Sentiment': \"N/A\",\n",
    "                    'Risk - Own Adoption': \"N/A\", 'Risk - External Threats': \"N/A\", 'Risk - Non-Adoption': \"N/A\",\n",
    "                    'Forward-Looking': \"N/A\", 'AI Washing Index': \"N/A\", 'Talent & Investment': \"N/A\",\n",
    "                    'Key AI Terms': \"\", 'error': None\n",
    "                }\n",
    "\n",
    "                # check for valid input data for this row before attempting api calls\n",
    "                valid_row_for_api_analysis = True\n",
    "                for col in text_columns:\n",
    "                    if pd.isna(row_data[col]) or not str(row_data[col]).strip():\n",
    "                        tqdm.write(f\"skipping cik {cik_val} (original index: {original_df_index_val}): missing or empty data in column '{col}'.\")\n",
    "                        current_row_result_dict['error'] = f'missing/empty data in column {col}'\n",
    "                        valid_row_for_api_analysis = False\n",
    "                        break\n",
    "                \n",
    "                if not valid_row_for_api_analysis:\n",
    "                    all_finalized_results_list.append(current_row_result_dict)\n",
    "                    rows_processed_this_session += 1\n",
    "                    # regular checkpoint saving based on finalized rows\n",
    "                    if rows_processed_this_session > 0 and (rows_processed_this_session % checkpoint_interval == 0):\n",
    "                        save_checkpoint(all_finalized_results_list, len(all_finalized_results_list), checkpoint_base_path, is_error_save=False)\n",
    "                    continue # move to the next row in the outer loop\n",
    "\n",
    "                # inner loop to retry the current row if api call is needed\n",
    "                consecutive_api_failures_for_current_row = 0\n",
    "                current_row_successfully_processed = False\n",
    "\n",
    "                while True: # retry loop for the current original_df_index_val\n",
    "                    analysis_object = analyze_text_with_gemini_instructor(\n",
    "                        str(row_data[\"risk_factors_text\"]), \n",
    "                        str(row_data[\"item7_mda_text\"]), \n",
    "                        company_name_for_llm_analysis, \n",
    "                        temperature=0.0\n",
    "                    )\n",
    "\n",
    "                    if analysis_object:\n",
    "                        tqdm.write(f\"success: cik {cik_val} (original index: {original_df_index_val}) processed successfully.\")\n",
    "                        current_row_result_dict.update({\n",
    "                            'Overall Summary': analysis_object.overall_ai_preparedness_summary_cot,\n",
    "                            'Strategic Depth': extract_grade_letter(analysis_object.ai_strategic_depth.bucket_assessment),\n",
    "                            'Disclosure Sentiment': extract_grade_letter(analysis_object.ai_disclosure_sentiment.bucket_assessment),\n",
    "                            'Risk - Own Adoption': extract_grade_letter(analysis_object.ai_risk_disclosure.risk_from_own_ai_adoption.bucket_assessment),\n",
    "                            'Risk - External Threats': extract_grade_letter(analysis_object.ai_risk_disclosure.risk_from_external_ai_threats.bucket_assessment),\n",
    "                            'Risk - Non-Adoption': extract_grade_letter(analysis_object.ai_risk_disclosure.risk_of_not_adopting_ai_or_competitive_ai_threat.bucket_assessment),\n",
    "                            'Forward-Looking': extract_grade_letter(analysis_object.forward_looking_ai_statements.bucket_assessment),\n",
    "                            'AI Washing Index': extract_grade_letter(analysis_object.ai_washing_hype_index.bucket_assessment),\n",
    "                            'Talent & Investment': extract_grade_letter(analysis_object.ai_talent_and_investment_focus.bucket_assessment),\n",
    "                            'Key AI Terms': ', '.join(analysis_object.key_ai_related_terminology_found if analysis_object.key_ai_related_terminology_found else [])\n",
    "                        })\n",
    "                        current_row_result_dict['error'] = None # clear any previous error for this row\n",
    "                        consecutive_api_failures_for_current_row = 0 \n",
    "                        current_row_successfully_processed = True\n",
    "                        break # exit retry loop for this row\n",
    "                    else: # api call failed for this row\n",
    "                        current_row_result_dict['error'] = 'gemini analysis failed or returned no result' # update error for this attempt\n",
    "                        consecutive_api_failures_for_current_row += 1\n",
    "                        \n",
    "                        # save an error checkpoint. it includes previously finalized rows + current failing row's state.\n",
    "                        # the number of rows in the filename will be len(all_finalized_results_list) + 1\n",
    "                        temp_list_for_error_save = list(all_finalized_results_list)\n",
    "                        temp_list_for_error_save.append(current_row_result_dict)\n",
    "                        save_checkpoint(temp_list_for_error_save, \n",
    "                                        len(temp_list_for_error_save), # number of rows including the current failing one\n",
    "                                        checkpoint_base_path, \n",
    "                                        is_error_save=True)\n",
    "\n",
    "                        if consecutive_api_failures_for_current_row <= max_retries_per_row:\n",
    "                            delay_index = consecutive_api_failures_for_current_row - 1\n",
    "                            current_delay = backoff_delays[delay_index]\n",
    "                            tqdm.write(f\"api error for cik {cik_val} (original index: {original_df_index_val}). attempt {consecutive_api_failures_for_current_row}/{max_retries_per_row}. waiting {current_delay}s before retrying same row.\")\n",
    "                            time.sleep(current_delay)\n",
    "                            # continue to the next iteration of the inner while loop to retry\n",
    "                        else: \n",
    "                            tqdm.write(f\"persistent api errors for cik {cik_val} (original index: {original_df_index_val}) after {consecutive_api_failures_for_current_row} attempts. stopping script.\")\n",
    "                            stop_all_processing_flag = True\n",
    "                            # current_row_result_dict already has the final error\n",
    "                            break # exit retry loop for this row, it failed permanently\n",
    "                # end of inner retry loop for the current row\n",
    "\n",
    "                all_finalized_results_list.append(current_row_result_dict) # add the final state of this row (success or max retries failed)\n",
    "                rows_processed_this_session += 1\n",
    "                \n",
    "                if stop_all_processing_flag:\n",
    "                    tqdm.write(\"stopping all processing due to persistent error on a row.\")\n",
    "                    break # exit the main outer for loop\n",
    "\n",
    "                # regular interval checkpoint saving (based on finalized rows)\n",
    "                if rows_processed_this_session > 0 and (rows_processed_this_session % checkpoint_interval == 0):\n",
    "                    save_checkpoint(all_finalized_results_list, len(all_finalized_results_list), checkpoint_base_path, is_error_save=False)\n",
    "                \n",
    "                # apply normal delay only if the row was successfully processed in its last attempt\n",
    "                if current_row_successfully_processed and api_request_delay_on_success > 0 :\n",
    "                    time.sleep(api_request_delay_on_success)\n",
    "                \n",
    "            # final save after loop completion (or break)\n",
    "            if all_finalized_results_list:\n",
    "                df_results_final = pd.DataFrame(all_finalized_results_list)\n",
    "                final_results_filename = os.path.join(checkpoint_base_path, \"gemini_analysis_FINAL_ALL_ROWS.csv\")\n",
    "                try:\n",
    "                    df_results_final.to_csv(final_results_filename, index=False)\n",
    "                    print(f\"\\n\\n--- processing finished. final results saved to: {final_results_filename} ({len(df_results_final)} total rows) ---\")\n",
    "                    if not df_results_final.empty:\n",
    "                        print(df_results_final.head().to_string())\n",
    "                    else:\n",
    "                        print(\"final dataframe is empty.\")\n",
    "                except Exception as e_final_save:\n",
    "                    print(f\"error saving final results {final_results_filename}: {e_final_save}\")\n",
    "            else:\n",
    "                print(\"\\nno results were processed or loaded to create a final dataframe.\")\n",
    "    \n",
    "    print(\"\\n--- main execution finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e66d2e-9d8b-4a61-a5be-8fbc4e265a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import GenerationConfig, HarmCategory, HarmBlockThreshold\n",
    "from google.api_core.exceptions import ResourceExhausted, InternalServerError, ServiceUnavailable, DeadlineExceeded\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import instructor\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "from typing import List, Literal, Optional, Union\n",
    "\n",
    "# pydantic model definitions\n",
    "AIStrategicDepthBucket = Literal[\n",
    "    \"A (Core Strategic Enabler)\",\n",
    "    \"B (Significant Operational/Product Integration)\",\n",
    "    \"C (Emerging/Exploratory Mentions)\",\n",
    "    \"D (Superficial/Generic Mentions OR Risk of Non-Adoption Only)\",\n",
    "    \"E (No Mention of Own AI Strategy/Adoption)\"\n",
    "]\n",
    "\n",
    "AIDisclosureSentimentBucket = Literal[\n",
    "    \"A (Clearly Positive)\",\n",
    "    \"B (Mostly Positive/Balanced)\",\n",
    "    \"C (Neutral/Factual)\",\n",
    "    \"D (Cautious/Mixed)\",\n",
    "    \"E (Negative OR Not Applicable - No Own AI Discussion)\"\n",
    "]\n",
    "\n",
    "AIRiskOwnAdoptionBucket = Literal[\n",
    "    \"A (Detailed & Specific Discussion)\",\n",
    "    \"B (General Mention of Own AI Adoption Risk)\",\n",
    "    \"C (No Mention of Own AI Adoption Risk)\"\n",
    "]\n",
    "\n",
    "AIRiskExternalThreatsBucket = Literal[\n",
    "    \"A (Detailed & Specific Discussion)\",\n",
    "    \"B (General Mention of External AI Threat)\",\n",
    "    \"C (No Mention of External AI Threat)\"\n",
    "]\n",
    "\n",
    "AIRiskNonAdoptionBucket = Literal[\n",
    "    \"A (Explicitly Discussed)\",\n",
    "    \"B (Implicitly Suggested)\",\n",
    "    \"C (No Mention of Non-Adoption/Competitive AI Risk)\"\n",
    "]\n",
    "\n",
    "AIForwardLookingBucket = Literal[\n",
    "    \"A (Specific & Detailed Future Plans)\",\n",
    "    \"B (General Future Intent)\",\n",
    "    \"C (Implicit Future Focus Only)\",\n",
    "    \"D (No Mention of Future AI Plans)\"\n",
    "]\n",
    "\n",
    "AIWashingHypeBucket = Literal[\n",
    "    \"A (Substantive & Grounded)\",\n",
    "    \"B (Mostly Substantive)\",\n",
    "    \"C (Mixed - Some Substance, Some Hype)\",\n",
    "    \"D (Mostly Hype)\",\n",
    "    \"E (Pure Hype OR Not Applicable - No Positive AI Claims)\"\n",
    "]\n",
    "\n",
    "AITalentInvestmentBucket = Literal[\n",
    "    \"A (Explicit & Significant Focus)\",\n",
    "    \"B (General Mention of AI Talent/Investment)\",\n",
    "    \"C (Implicit Focus Only)\",\n",
    "    \"D (No Mention of AI Talent/Investment)\"\n",
    "]\n",
    "\n",
    "class BaseAssessment(BaseModel):\n",
    "    supporting_evidence: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Key verbatim quotes (or concise summaries) from the text that justify your assessment. Limit to 2-3 key pieces of evidence. If no direct evidence, provide an empty list [].\"\n",
    "    )\n",
    "    chain_of_thought_reasoning: str = Field(\n",
    "        description=\"A brief explanation (1-2 sentences) of your thought process for arriving at the bucket assessment, referencing the criteria and evidence.\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"supporting_evidence\", mode='before')\n",
    "    @classmethod\n",
    "    def normalize_supporting_evidence(cls, v):\n",
    "        if v is None: return []\n",
    "        if isinstance(v, str):\n",
    "            stripped_v = v.strip()\n",
    "            if stripped_v.lower() in [\"no direct evidence found.\", \"no evidence found.\"]:\n",
    "                return []\n",
    "            return [stripped_v] if stripped_v else []\n",
    "        if isinstance(v, list):\n",
    "            return [\n",
    "                str(item).strip() for item in v\n",
    "                if isinstance(item, str) and\n",
    "                str(item).strip() and\n",
    "                str(item).strip().lower() not in [\"no direct evidence found.\", \"no evidence found.\"]\n",
    "            ]\n",
    "        return []\n",
    "\n",
    "class AIStrategicDepthAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIStrategicDepthBucket\n",
    "\n",
    "class AIDisclosureSentimentAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIDisclosureSentimentBucket\n",
    "\n",
    "class AIRiskOwnAdoptionAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIRiskOwnAdoptionBucket\n",
    "\n",
    "class AIRiskExternalThreatsAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIRiskExternalThreatsBucket\n",
    "\n",
    "class AIRiskNonAdoptionAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIRiskNonAdoptionBucket\n",
    "\n",
    "class AIRiskDisclosure(BaseModel):\n",
    "    risk_from_own_ai_adoption: AIRiskOwnAdoptionAssessment\n",
    "    risk_from_external_ai_threats: AIRiskExternalThreatsAssessment\n",
    "    risk_of_not_adopting_ai_or_competitive_ai_threat: AIRiskNonAdoptionAssessment\n",
    "\n",
    "class AIForwardLookingAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIForwardLookingBucket\n",
    "\n",
    "class AIWashingHypeAssessment(BaseAssessment):\n",
    "    bucket_assessment: AIWashingHypeBucket\n",
    "\n",
    "class AITalentInvestmentAssessment(BaseAssessment):\n",
    "    bucket_assessment: AITalentInvestmentBucket\n",
    "\n",
    "class CompanyAIAnalysis(BaseModel):\n",
    "    company_identifier: Optional[str] = Field(default=None, description=\"EXTRACT_COMPANY_NAME_IF_POSSIBLE_ELSE_NULL\")\n",
    "    overall_ai_preparedness_summary_cot: str = Field(description=\"LLM_GENERATED_1_2_SENTENCE_SUMMARY_OF_OVERALL_AI_POSTURE_BASED_ON_ALL_FACTORS\")\n",
    "    ai_strategic_depth: AIStrategicDepthAssessment\n",
    "    ai_disclosure_sentiment: AIDisclosureSentimentAssessment\n",
    "    ai_risk_disclosure: AIRiskDisclosure\n",
    "    forward_looking_ai_statements: AIForwardLookingAssessment\n",
    "    ai_washing_hype_index: AIWashingHypeAssessment\n",
    "    ai_talent_and_investment_focus: AITalentInvestmentAssessment\n",
    "    key_ai_related_terminology_found: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List any explicit AI-related terms found (e.g., 'artificial intelligence', 'machine learning'). If none, provide an empty list [].\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"key_ai_related_terminology_found\", mode='before')\n",
    "    @classmethod\n",
    "    def normalize_key_terms(cls, v):\n",
    "        if v is None: return []\n",
    "        if isinstance(v, str):\n",
    "            return [term.strip() for term in v.split(',') if term.strip()]\n",
    "        if isinstance(v, list):\n",
    "            return [str(item).strip() for item in v if isinstance(item, str) and str(item).strip()]\n",
    "        return []\n",
    "\n",
    "# prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert financial analyst and AI strategist, specializing in evaluating corporate disclosures for insights into a company's engagement with Artificial Intelligence (AI). Your task is to meticulously analyze the provided text from a company's 10-K filing (specifically excerpts from \"Item 1. Risk Factors\" and \"Item 7. Management's Discussion and Analysis\") and provide a structured JSON output assessing the company's AI preparedness based on the defined factors and criteria.\n",
    "\n",
    "**DOCUMENT CONTEXT:**\n",
    "The provided text below, enclosed in triple backticks (```), contains excerpts from \"Item 1. Risk Factors\" and \"Item 7. Management's Discussion and Analysis (MD&A)\" of a single company's 10-K report.\n",
    "\n",
    "```\n",
    "{document_text}\n",
    "```\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Carefully read the entire provided text.\n",
    "2.  For each of the AI Assessment Factors listed below, evaluate the text based on the defined Buckets & Criteria.\n",
    "3.  Your entire response MUST be a single, valid JSON object. Do not include any explanatory text before or after the JSON object. The JSON object MUST conform to the Pydantic schema `CompanyAIAnalysis`.\n",
    "4.  For each factor, you MUST include in the JSON:\n",
    "    * `bucket_assessment`: The exact string for the category you've assigned. **CRITICAL: This value MUST be one of the precise string values listed in the Buckets & Criteria for that factor (e.g., for AI Strategic Depth, it must be exactly \"A (Core Strategic Enabler)\" or \"B (Significant Operational/Product Integration)\", etc. Do NOT use abbreviations or variations).**\n",
    "    * `chain_of_thought_reasoning`: A brief explanation (1-2 sentences) of your thought process for arriving at the bucket assessment, referencing the criteria and evidence.\n",
    "    * `supporting_evidence`: Key verbatim quotes (or concise summaries of multiple related statements) from the text that justify your assessment. Limit to 2-3 key pieces of evidence. If no direct evidence is found for a factor, you MUST provide an empty list `[]` for this field.\n",
    "5.  **CRITICAL FOR `ai_risk_disclosure` FIELD**: The `ai_risk_disclosure` field in the JSON output MUST be an object containing three specific nested assessment objects as its keys: `risk_from_own_ai_adoption`, `risk_from_external_ai_threats`, and `risk_of_not_adopting_ai_or_competitive_ai_threat`. Each of these three nested objects must follow the `BaseAssessment` structure (i.e., have `bucket_assessment`, `chain_of_thought_reasoning`, and `supporting_evidence`). Refer to the example JSON structure below.\n",
    "6.  If information for any other factor or sub-factor (within `ai_risk_disclosure`) is not present in the text, use the appropriate \"No Mention\" bucket assessment for that factor (e.g., \"E (No Mention of Own AI Strategy/Adoption)\" or \"C (No Mention of Own AI Adoption Risk)\"). For `supporting_evidence` in such cases, provide an empty list `[]`.\n",
    "    *IMPORTANT*: Ensure ALL main assessment factor objects (ai_strategic_depth, ai_disclosure_sentiment, ai_risk_disclosure, forward_looking_ai_statements, ai_washing_hype_index, ai_talent_and_investment_focus) are ALWAYS present as keys in your JSON output. If a factor has no relevant information, its `bucket_assessment` should reflect its defined \"No Mention\" or equivalent lowest grade for that factor's rubric, and `supporting_evidence` should be an empty list. Do not omit these top-level keys from the JSON.\n",
    "7.  For the `company_identifier` field, extract the company name from the \"Company Name (if known):\" line in the DOCUMENT CONTEXT. If it's \"Unknown Company\" or not clearly identifiable, set it to null.\n",
    "8.  For the `overall_ai_preparedness_summary_cot` field, you MUST provide a 1-2 sentence summary of the company's overall AI posture based on your analysis of all factors.\n",
    "9.  For the `key_ai_related_terminology_found` field, you MUST list any explicit AI-related terms found in the text (e.g., \"artificial intelligence\", \"machine learning\", \"NLP\", \"generative AI\"). If no such terms are found, provide an empty list `[]`.\n",
    "\n",
    "**AI ASSESSMENT FACTORS, BUCKETS & CRITERIA:**\n",
    "\n",
    "**1. AI Strategic Depth:**\n",
    "    * *Concept:* How deeply and explicitly AI is integrated into the company's core business strategy and future plans...\n",
    "    * *Buckets & Criteria:*\n",
    "        * \"A (Core Strategic Enabler)\"\n",
    "        * \"B (Significant Operational/Product Integration)\"\n",
    "        * \"C (Emerging/Exploratory Mentions)\"\n",
    "        * \"D (Superficial/Generic Mentions OR Risk of Non-Adoption Only)\"\n",
    "        * \"E (No Mention of Own AI Strategy/Adoption)\"\n",
    "\n",
    "**2. AI Disclosure Sentiment:**\n",
    "    * *Concept:* The overall tone of the company's *own statements* regarding its AI initiatives...\n",
    "    * *Buckets & Criteria:*\n",
    "        * \"A (Clearly Positive)\"\n",
    "        * \"B (Mostly Positive/Balanced)\"\n",
    "        * \"C (Neutral/Factual)\"\n",
    "        * \"D (Cautious/Mixed)\"\n",
    "        * \"E (Negative OR Not Applicable - No Own AI Discussion)\"\n",
    "\n",
    "**3. AI Risk Disclosure:** (This section requires careful attention to the nested structure)\n",
    "    * *Concept:* How thoroughly and specifically the company acknowledges and discusses potential risks associated with AI. This factor itself is an object in the JSON, containing the following three sub-assessments:\n",
    "    * **3a. `risk_from_own_ai_adoption`:** (e.g., AI system failure, data bias, ethical concerns, implementation challenges).\n",
    "        * *Buckets & Criteria for 3a:*\n",
    "            * \"A (Detailed & Specific Discussion)\"\n",
    "            * \"B (General Mention of Own AI Adoption Risk)\"\n",
    "            * \"C (No Mention of Own AI Adoption Risk)\"\n",
    "    * **3b. `risk_from_external_ai_threats`:** (e.g., AI used in cyberattacks by others, AI-driven misinformation).\n",
    "        * *Buckets & Criteria for 3b:*\n",
    "            * \"A (Detailed & Specific Discussion)\"\n",
    "            * \"B (General Mention of External AI Threat)\"\n",
    "            * \"C (No Mention of External AI Threat)\"\n",
    "    * **3c. `risk_of_not_adopting_ai_or_competitive_ai_threat`:** (e.g., falling behind AI-leveraging competitors, failure to innovate using AI).\n",
    "        * *Buckets & Criteria for 3c:*\n",
    "            * \"A (Explicitly Discussed)\"\n",
    "            * \"B (Implicitly Suggested)\"\n",
    "            * \"C (No Mention of Non-Adoption/Competitive AI Risk)\"\n",
    "\n",
    "**4. Forward-Looking AI Statements:**\n",
    "    * *Concept:* The extent and specificity of discussion about future plans...\n",
    "    * *Buckets & Criteria:*\n",
    "        * \"A (Specific & Detailed Future Plans)\"\n",
    "        * \"B (General Future Intent)\"\n",
    "        * \"C (Implicit Future Focus Only)\"\n",
    "        * \"D (No Mention of Future AI Plans)\"\n",
    "\n",
    "**5. \"AI Washing\" Hype Index:** (Factor is only applicable if positive claims about own AI are made)\n",
    "    * *Concept:* Degree to which a company's positive AI claims appear substantive vs. exaggerated or vague.\n",
    "    * *Buckets & Criteria:*\n",
    "        * \"A (Substantive & Grounded)\"\n",
    "        * \"B (Mostly Substantive)\"\n",
    "        * \"C (Mixed - Some Substance, Some Hype)\"\n",
    "        * \"D (Mostly Hype)\"\n",
    "        * \"E (Pure Hype OR Not Applicable - No Positive AI Claims)\"\n",
    "\n",
    "**6. AI Talent & Investment Focus:**\n",
    "    * *Concept:* Explicit discussion of focus on acquiring/developing AI talent...\n",
    "    * *Buckets & Criteria:*\n",
    "        * \"A (Explicit & Significant Focus)\"\n",
    "        * \"B (General Mention of AI Talent/Investment)\"\n",
    "        * \"C (Implicit Focus Only)\"\n",
    "        * \"D (No Mention of AI Talent/Investment)\"\n",
    "\n",
    "**EXPECTED JSON OUTPUT STRUCTURE (Example for LLM guidance, actual structure enforced by Pydantic):**\n",
    "```json\n",
    "{{\n",
    "  \"company_identifier\": \"EXTRACT_COMPANY_NAME_IF_POSSIBLE_ELSE_NULL\",\n",
    "  \"overall_ai_preparedness_summary_cot\": \"LLM_GENERATED_1_2_SENTENCE_SUMMARY_OF_OVERALL_AI_POSTURE_BASED_ON_ALL_FACTORS\",\n",
    "  \"ai_strategic_depth\": {{\n",
    "    \"bucket_assessment\": \"A (Core Strategic Enabler)\",\n",
    "    \"supporting_evidence\": [\"quote1\", \"quote2\"],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"ai_disclosure_sentiment\": {{\n",
    "    \"bucket_assessment\": \"B (Mostly Positive/Balanced)\",\n",
    "    \"supporting_evidence\": [\"quote1\"],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"ai_risk_disclosure\": {{\n",
    "    \"risk_from_own_ai_adoption\": {{\n",
    "      \"bucket_assessment\": \"C (No Mention of Own AI Adoption Risk)\",\n",
    "      \"supporting_evidence\": [],\n",
    "      \"chain_of_thought_reasoning\": \"No specific risks from the company's own AI adoption were mentioned.\"\n",
    "    }},\n",
    "    \"risk_from_external_ai_threats\": {{\n",
    "      \"bucket_assessment\": \"B (General Mention of External AI Threat)\",\n",
    "      \"supporting_evidence\": [\"The company mentions general cybersecurity threats which could involve AI.\"],\n",
    "      \"chain_of_thought_reasoning\": \"General mention of external threats that could be AI-related.\"\n",
    "    }},\n",
    "    \"risk_of_not_adopting_ai_or_competitive_ai_threat\": {{\n",
    "      \"bucket_assessment\": \"A (Explicitly Discussed)\",\n",
    "      \"supporting_evidence\": [\"Failure to adopt new technologies like AI could impact our competitive position.\"],\n",
    "      \"chain_of_thought_reasoning\": \"The company explicitly discusses the risk of not keeping up with technological advancements including AI.\"\n",
    "    }}\n",
    "  }},\n",
    "  \"forward_looking_ai_statements\": {{\n",
    "    \"bucket_assessment\": \"D (No Mention of Future AI Plans)\",\n",
    "    \"supporting_evidence\": [],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"ai_washing_hype_index\": {{\n",
    "    \"bucket_assessment\": \"E (Pure Hype OR Not Applicable - No Positive AI Claims)\",\n",
    "    \"supporting_evidence\": [],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"ai_talent_and_investment_focus\": {{\n",
    "    \"bucket_assessment\": \"D (No Mention of AI Talent/Investment)\",\n",
    "    \"supporting_evidence\": [],\n",
    "    \"chain_of_thought_reasoning\": \"Your reasoning for this factor.\"\n",
    "  }},\n",
    "  \"key_ai_related_terminology_found\": [\"artificial intelligence\", \"machine learning\"]\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# global variables for client and configuration\n",
    "CONFIGURED_GEMINI_API_KEY = None\n",
    "GENAI_CONFIGURED_SUCCESSFULLY = False\n",
    "INSTRUCTOR_GEMINI_CLIENT = None \n",
    "BASE_GEMINI_MODEL_INSTANCE = None \n",
    "GEMINI_MODEL_NAME = 'models/gemini-2.5-flash-preview-05-20' \n",
    "\n",
    "# configuration & client initialization for gemini\n",
    "def initialize_gemini_client():\n",
    "    global CONFIGURED_GEMINI_API_KEY, GENAI_CONFIGURED_SUCCESSFULLY, \\\n",
    "           INSTRUCTOR_GEMINI_CLIENT, BASE_GEMINI_MODEL_INSTANCE, GEMINI_MODEL_NAME\n",
    "\n",
    "    print(\"--- starting gemini configuration and client initialization ---\")\n",
    "    GEMINI_API_KEY_FROM_ENV = os.getenv('GEMINI_API_KEY')\n",
    "    if GEMINI_API_KEY_FROM_ENV:\n",
    "        print(\"found 'GEMINI_API_KEY' in environment variables.\")\n",
    "        CONFIGURED_GEMINI_API_KEY = GEMINI_API_KEY_FROM_ENV\n",
    "    elif 'gemini_api_key' in globals() and globals()['gemini_api_key'] is not None: \n",
    "        print(\"found 'gemini_api_key' in global script variables.\")\n",
    "        CONFIGURED_GEMINI_API_KEY = globals()['gemini_api_key']\n",
    "    else:\n",
    "        print(\"error: 'GEMINI_API_KEY' is not defined in environment or global script variables.\")\n",
    "\n",
    "    if CONFIGURED_GEMINI_API_KEY:\n",
    "        if isinstance(CONFIGURED_GEMINI_API_KEY, str) and CONFIGURED_GEMINI_API_KEY.strip():\n",
    "            print(f\"success: gemini api key loaded.\")\n",
    "        else:\n",
    "            print(f\"error: gemini api key is not a valid non-empty string.\")\n",
    "            CONFIGURED_GEMINI_API_KEY = None \n",
    "\n",
    "    if CONFIGURED_GEMINI_API_KEY:\n",
    "        try:\n",
    "            genai.configure(api_key=CONFIGURED_GEMINI_API_KEY)\n",
    "            print(\"success: genai.configure(api_key=...) called successfully.\")\n",
    "            GENAI_CONFIGURED_SUCCESSFULLY = True\n",
    "        except Exception as e:\n",
    "            print(f\"error: exception during genai.configure(api_key=...): {e}\")\n",
    "    else:\n",
    "        print(\"skipped: genai.configure() because api key was not valid or not found.\")\n",
    "\n",
    "    if GENAI_CONFIGURED_SUCCESSFULLY:\n",
    "        print(\"attempting: to create instructor-patched gemini client...\")\n",
    "        try:\n",
    "            safety_settings_config = {\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            }\n",
    "            BASE_GEMINI_MODEL_INSTANCE = genai.GenerativeModel(\n",
    "                model_name=GEMINI_MODEL_NAME,\n",
    "                safety_settings=safety_settings_config\n",
    "            )\n",
    "            print(f\"  success: base genai.generativemodel('{GEMINI_MODEL_NAME}') created with custom safety settings.\")\n",
    "\n",
    "            INSTRUCTOR_GEMINI_CLIENT = instructor.from_gemini(\n",
    "                client=BASE_GEMINI_MODEL_INSTANCE, \n",
    "                mode=instructor.Mode.GEMINI_JSON\n",
    "            )\n",
    "            print(\"  success: client created with instructor.from_gemini using GEMINI_JSON mode.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  error: exception during instructor.from_gemini client creation: {e}\")\n",
    "            INSTRUCTOR_GEMINI_CLIENT = None\n",
    "            if not BASE_GEMINI_MODEL_INSTANCE:\n",
    "                try:\n",
    "                    BASE_GEMINI_MODEL_INSTANCE = genai.GenerativeModel(\n",
    "                        model_name=GEMINI_MODEL_NAME,\n",
    "                        safety_settings=safety_settings_config\n",
    "                    )\n",
    "                    print(f\"  note: base genai.generativemodel created successfully despite instructor client failure.\")\n",
    "                except Exception as e_base:\n",
    "                    print(f\"  error: failed to create BASE_GEMINI_MODEL_INSTANCE as well: {e_base}\")\n",
    "                    BASE_GEMINI_MODEL_INSTANCE = None\n",
    "    else:\n",
    "        print(\"skipped: instructor client creation for gemini because genai library was not configured successfully.\")\n",
    "\n",
    "    if not INSTRUCTOR_GEMINI_CLIENT:\n",
    "        print(\"failure: instructor_gemini_client could not be initialized.\")\n",
    "    if not BASE_GEMINI_MODEL_INSTANCE:\n",
    "        print(\"failure: base_gemini_model_instance could not be initialized. fallback path will not work.\")\n",
    "    print(\"--- gemini configuration and client initialization finished ---\")\n",
    "\n",
    "initialize_gemini_client()\n",
    "\n",
    "GEMINI_2_5_FLASH_INPUT_COST_PER_MILLION_TOKENS = 0.35\n",
    "GEMINI_2_5_FLASH_OUTPUT_COST_PER_MILLION_TOKENS = 0.70\n",
    "print(f\"info: using placeholder pricing for gemini 2.5 flash: input=${GEMINI_2_5_FLASH_INPUT_COST_PER_MILLION_TOKENS}/m, output=${GEMINI_2_5_FLASH_OUTPUT_COST_PER_MILLION_TOKENS}/m. please verify and update if necessary.\")\n",
    "\n",
    "cumulative_prompt_tokens_gemini = 0\n",
    "cumulative_completion_tokens_gemini = 0\n",
    "cumulative_cost_gemini = 0.0\n",
    "\n",
    "def calculate_and_update_cost_gemini(prompt_tokens: int, completion_tokens: int, api_call_made_and_tokens_returned: bool):\n",
    "    global cumulative_prompt_tokens_gemini, cumulative_completion_tokens_gemini, cumulative_cost_gemini\n",
    "    call_cost = 0.0\n",
    "    if prompt_tokens > 0 or completion_tokens > 0:\n",
    "        input_cost = (prompt_tokens / 1_000_000) * GEMINI_2_5_FLASH_INPUT_COST_PER_MILLION_TOKENS\n",
    "        output_cost = (completion_tokens / 1_000_000) * GEMINI_2_5_FLASH_OUTPUT_COST_PER_MILLION_TOKENS\n",
    "        call_cost = input_cost + output_cost\n",
    "        tqdm.write(f\"gemini api call tokens: prompt={prompt_tokens}, completion={completion_tokens}. cost for this call attempt: ${call_cost:.6f}.\")\n",
    "        if api_call_made_and_tokens_returned:\n",
    "            cumulative_prompt_tokens_gemini += prompt_tokens\n",
    "            cumulative_completion_tokens_gemini += completion_tokens\n",
    "            cumulative_cost_gemini += call_cost\n",
    "    tqdm.write(f\"gemini cumulative: prompt tokens={cumulative_prompt_tokens_gemini}, completion tokens={cumulative_completion_tokens_gemini}, cost=${cumulative_cost_gemini:.6f}\")\n",
    "    return call_cost\n",
    "\n",
    "def analyze_text_with_gemini_instructor(\n",
    "    risk_factors_text: str,\n",
    "    mda_text: str,\n",
    "    company_name: str = \"Unknown Company\",\n",
    "    temperature: float = 0.0\n",
    ") -> (Optional[CompanyAIAnalysis], int, int):\n",
    "\n",
    "    global INSTRUCTOR_GEMINI_CLIENT, BASE_GEMINI_MODEL_INSTANCE\n",
    "\n",
    "    if not INSTRUCTOR_GEMINI_CLIENT and not BASE_GEMINI_MODEL_INSTANCE:\n",
    "        raise Exception(\"critical: neither INSTRUCTOR_GEMINI_CLIENT nor BASE_GEMINI_MODEL_INSTANCE is available.\")\n",
    "\n",
    "    document_text = f\"\"\"Company Name (if known): {company_name}\n",
    "Item 1. Risk Factors:\\n{risk_factors_text}\\n\\nItem 7. Management's Discussion and Analysis (MD&A):\\n{mda_text}\"\"\"\n",
    "    full_prompt = PROMPT_TEMPLATE.format(document_text=document_text)\n",
    "\n",
    "    analysis_pydantic_object = None\n",
    "    prompt_tokens_used, completion_tokens_used = 0, 0\n",
    "    json_text_response_for_debug = None \n",
    "\n",
    "    try:\n",
    "        if not INSTRUCTOR_GEMINI_CLIENT:\n",
    "            tqdm.write(f\"instructor client not available for {company_name}, attempting fallback directly.\")\n",
    "            raise TypeError(\"instructor client not available, force fallback.\") \n",
    "\n",
    "        tqdm.write(f\"attempting analysis for {company_name} using instructor client.\")\n",
    "        api_response_object = INSTRUCTOR_GEMINI_CLIENT.generate_content(\n",
    "            contents=[full_prompt],\n",
    "            response_model=CompanyAIAnalysis,\n",
    "            generation_config=GenerationConfig(temperature=temperature)\n",
    "        )\n",
    "        analysis_pydantic_object = api_response_object\n",
    "\n",
    "        if hasattr(analysis_pydantic_object, '_raw_response') and analysis_pydantic_object._raw_response:\n",
    "            original_gemini_response = analysis_pydantic_object._raw_response\n",
    "            if hasattr(original_gemini_response, 'usage_metadata') and original_gemini_response.usage_metadata:\n",
    "                prompt_tokens_used = original_gemini_response.usage_metadata.prompt_token_count\n",
    "                completion_tokens_used = original_gemini_response.usage_metadata.candidates_token_count\n",
    "            elif hasattr(original_gemini_response, 'prompt_feedback') and original_gemini_response.prompt_feedback.block_reason:\n",
    "                tqdm.write(f\"warning: gemini prompt for {company_name} (via instructor) may have been blocked. reason: {original_gemini_response.prompt_feedback.block_reason_message or 'Unknown'}\")\n",
    "            elif hasattr(api_response_object, 'usage_metadata') and api_response_object.usage_metadata:\n",
    "                prompt_tokens_used = api_response_object.usage_metadata.prompt_token_count\n",
    "                completion_tokens_used = api_response_object.usage_metadata.candidates_token_count\n",
    "            else:\n",
    "                tqdm.write(f\"warning: could not retrieve token usage from instructor response for {company_name}. estimating input tokens.\")\n",
    "                if BASE_GEMINI_MODEL_INSTANCE:\n",
    "                    prompt_tokens_used = BASE_GEMINI_MODEL_INSTANCE.count_tokens(contents=[full_prompt]).total_tokens\n",
    "                completion_tokens_used = 0\n",
    "\n",
    "    except TypeError as te:\n",
    "        if (\"unexpected keyword argument 'response_model'\" in str(te) or \"instructor client not available\" in str(te)) and BASE_GEMINI_MODEL_INSTANCE:\n",
    "            tqdm.write(f\"instructor TypeError or unavailability for {company_name}: {te}. falling back to manual json parsing with base gemini client.\")\n",
    "\n",
    "            json_generation_config = GenerationConfig(\n",
    "                temperature=temperature,\n",
    "                response_mime_type=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            sdk_response = BASE_GEMINI_MODEL_INSTANCE.generate_content(\n",
    "                contents=[full_prompt],\n",
    "                generation_config=json_generation_config\n",
    "            )\n",
    "\n",
    "            if not sdk_response.candidates or not sdk_response.candidates[0].content or not sdk_response.candidates[0].content.parts:\n",
    "                block_message = \"Unknown reason.\"\n",
    "                if hasattr(sdk_response, 'prompt_feedback') and sdk_response.prompt_feedback.block_reason:\n",
    "                    block_message = sdk_response.prompt_feedback.block_reason_message or sdk_response.prompt_feedback.block_reason.name \n",
    "                elif sdk_response.candidates and hasattr(sdk_response.candidates[0], 'finish_reason') and sdk_response.candidates[0].finish_reason != genai.types.FinishReason.STOP:\n",
    "                    block_message = f\"Finish reason: {sdk_response.candidates[0].finish_reason.name}.\" \n",
    "                elif sdk_response.candidates and hasattr(sdk_response.candidates[0], 'safety_ratings'):\n",
    "                    problematic_ratings = [str(sr) for sr in sdk_response.candidates[0].safety_ratings if sr.probability not in [genai.types.HarmProbability.NEGLIGIBLE, genai.types.HarmProbability.LOW]]\n",
    "                    if problematic_ratings:\n",
    "                        block_message = f\"safety ratings indicate potential blocking: {', '.join(problematic_ratings)}\"\n",
    "                raise Exception(f\"gemini sdk call (fallback) blocked or returned no content parts for {company_name}. {block_message}\")\n",
    "\n",
    "            json_text_response_for_debug = sdk_response.candidates[0].content.parts[0].text\n",
    "            try:\n",
    "                analysis_pydantic_object = CompanyAIAnalysis.model_validate_json(json_text_response_for_debug)\n",
    "            except ValidationError as ve_fallback:\n",
    "                tqdm.write(f\"pydantic validationerror during fallback for {company_name}: {str(ve_fallback)[:500]}\")\n",
    "                tqdm.write(f\"problematic json from fallback for {company_name}: {json_text_response_for_debug[:1000]}\") \n",
    "                raise ve_fallback \n",
    "\n",
    "            if hasattr(sdk_response, 'usage_metadata') and sdk_response.usage_metadata:\n",
    "                prompt_tokens_used = sdk_response.usage_metadata.prompt_token_count\n",
    "                completion_tokens_used = sdk_response.usage_metadata.candidates_token_count\n",
    "            else: # fallback token counting for manual path\n",
    "                if BASE_GEMINI_MODEL_INSTANCE:\n",
    "                    prompt_tokens_used = BASE_GEMINI_MODEL_INSTANCE.count_tokens(contents=[full_prompt]).total_tokens\n",
    "                completion_tokens_used = 0\n",
    "        else:\n",
    "            raise te\n",
    "\n",
    "    except ValidationError as ve: \n",
    "        tqdm.write(f\"pydantic validationerror (primary path) for {company_name}: {str(ve)[:500]}\")\n",
    "        raise ve\n",
    "\n",
    "    except (ResourceExhausted, InternalServerError, ServiceUnavailable, DeadlineExceeded) as api_err:\n",
    "        raise api_err\n",
    "\n",
    "    except Exception as e_unexpected:\n",
    "        tqdm.write(f\"unexpected error in analyze_text_with_gemini_instructor for {company_name}: {type(e_unexpected).__name__} - {str(e_unexpected)[:200]}\")\n",
    "        if json_text_response_for_debug: \n",
    "            tqdm.write(f\"json text at time of unexpected error for {company_name}: {json_text_response_for_debug[:500]}\")\n",
    "        raise e_unexpected\n",
    "\n",
    "    return analysis_pydantic_object, prompt_tokens_used, completion_tokens_used\n",
    "\n",
    "def extract_grade_letter(bucket_assessment_string: str) -> str:\n",
    "    if bucket_assessment_string and isinstance(bucket_assessment_string, str):\n",
    "        grade_part = bucket_assessment_string.split(\" \")[0]\n",
    "        if len(grade_part) == 1 and grade_part.isalpha(): return grade_part.upper()\n",
    "    return \"N/A\"\n",
    "\n",
    "def find_latest_gemini_checkpoint_info(checkpoint_dir: str, filename_prefix: str) -> (Optional[str], int):\n",
    "    latest_checkpoint_filename = None\n",
    "    max_original_df_rows_processed = 0\n",
    "    checkpoint_pattern = re.compile(rf\"{filename_prefix}(?:ERROR_)?checkpoint_rows_upto_(\\d+)\\.csv\")\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        tqdm.write(f\"checkpoint directory not found: {checkpoint_dir}\")\n",
    "        return None, 0\n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        match = checkpoint_pattern.match(filename)\n",
    "        if match:\n",
    "            try:\n",
    "                rows_in_this_filename = int(match.group(1))\n",
    "                if rows_in_this_filename > max_original_df_rows_processed:\n",
    "                    max_original_df_rows_processed = rows_in_this_filename\n",
    "                    latest_checkpoint_filename = filename\n",
    "                elif rows_in_this_filename == max_original_df_rows_processed and latest_checkpoint_filename:\n",
    "                    if \"ERROR\" in latest_checkpoint_filename and \"ERROR\" not in filename:\n",
    "                        latest_checkpoint_filename = filename\n",
    "            except ValueError:\n",
    "                tqdm.write(f\"warning: could not parse row count from checkpoint filename: {filename}\")\n",
    "                continue\n",
    "    if latest_checkpoint_filename:\n",
    "        tqdm.write(f\"found latest gemini checkpoint: {latest_checkpoint_filename} (represents processing up to original_df_index {max_original_df_rows_processed -1}).\")\n",
    "    else:\n",
    "        tqdm.write(f\"no valid '{filename_prefix}' checkpoint found in {checkpoint_dir}. starting fresh.\")\n",
    "    return latest_checkpoint_filename, max_original_df_rows_processed\n",
    "\n",
    "def save_gemini_checkpoint(results_list_to_save: list, total_original_df_rows_covered: int, base_path: str, filename_prefix: str, is_error_save: bool = False):\n",
    "    if not results_list_to_save:\n",
    "        tqdm.write(\"no new results to save in this checkpoint interval (results_list_to_save is empty).\")\n",
    "        return\n",
    "    temp_df = pd.DataFrame(results_list_to_save)\n",
    "    if not temp_df.empty:\n",
    "        temp_df['cumulative_prompt_tokens_gemini_at_save'] = cumulative_prompt_tokens_gemini\n",
    "        temp_df['cumulative_completion_tokens_gemini_at_save'] = cumulative_completion_tokens_gemini\n",
    "        temp_df['cumulative_cost_gemini_at_save'] = cumulative_cost_gemini\n",
    "    \n",
    "    error_tag = \"ERROR_\" if is_error_save else \"\"\n",
    "    filename = os.path.join(base_path, f\"{filename_prefix}{error_tag}checkpoint_rows_upto_{total_original_df_rows_covered}.csv\")\n",
    "    try:\n",
    "        temp_df.to_csv(filename, index=False)\n",
    "        tqdm.write(f\"--- {'ERROR' if is_error_save else 'Regular'} gemini checkpoint saved: {filename} (covers {total_original_df_rows_covered} original df rows) ---\")\n",
    "    except Exception as e_save:\n",
    "        tqdm.write(f\"error saving gemini checkpoint {filename}: {e_save}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- main execution for gemini 2.5 flash detailed analysis ---\")\n",
    "\n",
    "    if 'df' not in globals():\n",
    "        print(\"info: 'df' not found in globals. creating a dummy dataframe for testing.\")\n",
    "        data = {\n",
    "            'cik': ['0000001750', '0000001751', '0000001752'],\n",
    "            'tickers_sec': ['TICKA', 'TICKB', 'TICKC'],\n",
    "            'companyName_sec': ['Test Company A', 'Test Company B', 'Test Company C (No MDA)'],\n",
    "            'sector_user': ['Tech', 'Finance', 'Retail'],\n",
    "            'year': [2023, 2023, 2023],\n",
    "            'filingDate': ['2024-03-01', '2024-03-02', '2024-03-03'],\n",
    "            'risk_factors_text': [\n",
    "                \"The company faces risks from AI development and cyber threats. We are investing in AI. Our AI strategy is to enhance products.\",\n",
    "                \"Market competition is a risk. AI could be an opportunity if we adopt it. We are exploring AI for efficiency.\",\n",
    "                \"Standard retail risks. Supply chain is a concern.\" \n",
    "            ],\n",
    "            'item7_mda_text': [\n",
    "                \"Our AI strategy is to enhance products. We see AI as a key future driver. We plan to launch new AI features next year.\",\n",
    "                \"We are exploring AI for efficiency. No major AI initiatives yet. The future is uncertain regarding AI adoption.\",\n",
    "                None \n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        if not os.getenv('GEMINI_API_KEY') and ('gemini_api_key' not in globals() or globals()['gemini_api_key'] is None):\n",
    "            print(\"info: GEMINI_API_KEY not found. please set it as an environment variable or a global 'gemini_api_key' string for the script to run.\")\n",
    "\n",
    "    OUTPUT_BASE_DIR_GEMINI = \"./Gemini_2_5_Flash_Analysis_Results/\"\n",
    "    CHECKPOINT_FILENAME_PREFIX_GEMINI = \"gemini_2_5_flash_detailed_analysis_\"\n",
    "    FINAL_RESULTS_FILENAME_GEMINI = os.path.join(OUTPUT_BASE_DIR_GEMINI, f\"{CHECKPOINT_FILENAME_PREFIX_GEMINI}FINAL_ALL_ROWS.csv\")\n",
    "\n",
    "    try:\n",
    "        os.makedirs(OUTPUT_BASE_DIR_GEMINI, exist_ok=True)\n",
    "        print(f\"gemini output directory set to: {OUTPUT_BASE_DIR_GEMINI}\")\n",
    "        test_file_path = os.path.join(OUTPUT_BASE_DIR_GEMINI, \"test_write_gemini.txt\")\n",
    "        with open(test_file_path, \"w\") as f_test: f_test.write(\"test\")\n",
    "        os.remove(test_file_path)\n",
    "        print(f\"successfully tested write access to {OUTPUT_BASE_DIR_GEMINI}\")\n",
    "    except Exception as e_dir:\n",
    "        print(f\"critical error: could not create or write to gemini output directory {OUTPUT_BASE_DIR_GEMINI}: {e_dir}. exiting.\")\n",
    "        exit()\n",
    "\n",
    "    CHECKPOINT_INTERVAL = 50\n",
    "    API_REQUEST_DELAY_ON_SUCCESS = 1.0 \n",
    "    BACKOFF_DELAYS = [10, 30, 60, 180, 300] \n",
    "    MAX_RETRIES_PER_ROW = len(BACKOFF_DELAYS)\n",
    "\n",
    "    if not GENAI_CONFIGURED_SUCCESSFULLY:\n",
    "        print(\"critical error: genai not configured (api key issue likely). exiting.\")\n",
    "        exit()\n",
    "    if not INSTRUCTOR_GEMINI_CLIENT and not BASE_GEMINI_MODEL_INSTANCE:\n",
    "        print(\"critical error: gemini client (INSTRUCTOR_GEMINI_CLIENT or BASE_GEMINI_MODEL_INSTANCE) not initialized. exiting.\")\n",
    "        exit()\n",
    "    if 'df' not in globals() or not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        print(\"critical error: input dataframe 'df' not found, not a pandas dataframe, or is empty. exiting.\")\n",
    "        exit()\n",
    "    print(f\"input dataframe 'df' loaded with {len(df)} rows.\")\n",
    "\n",
    "    input_col_cik = \"cik\"\n",
    "    input_col_ticker = \"tickers_sec\"\n",
    "    input_col_company_name = \"companyName_sec\"\n",
    "    input_col_sector = \"sector_user\"\n",
    "    input_col_year = \"year\"\n",
    "    input_col_filing_date = \"filingDate\"\n",
    "    input_col_risk_factors = \"risk_factors_text\"\n",
    "    input_col_mda = \"item7_mda_text\"\n",
    "\n",
    "    if input_col_cik in df.columns:\n",
    "        df[input_col_cik] = df[input_col_cik].astype(str).str.zfill(10)\n",
    "        print(f\"standardized '{input_col_cik}' column in input dataframe 'df'.\")\n",
    "    else:\n",
    "        print(f\"critical error: input cik column '{input_col_cik}' not found in dataframe 'df'. exiting.\")\n",
    "        exit()\n",
    "\n",
    "    essential_text_cols = [input_col_risk_factors, input_col_mda]\n",
    "    missing_text_cols = [col for col in essential_text_cols if col not in df.columns]\n",
    "    if missing_text_cols:\n",
    "        print(f\"critical error: input dataframe 'df' is missing essential text columns: {missing_text_cols}. exiting.\")\n",
    "        exit()\n",
    "\n",
    "    all_processed_results_gemini = []\n",
    "    start_processing_from_original_df_index = 0\n",
    "    \n",
    "    latest_checkpoint_file, max_original_rows_covered_by_checkpoint = find_latest_gemini_checkpoint_info(OUTPUT_BASE_DIR_GEMINI, CHECKPOINT_FILENAME_PREFIX_GEMINI)\n",
    "\n",
    "    if latest_checkpoint_file:\n",
    "        full_checkpoint_path = os.path.join(OUTPUT_BASE_DIR_GEMINI, latest_checkpoint_file)\n",
    "        try:\n",
    "            tqdm.write(f\"loading data from gemini checkpoint: {full_checkpoint_path}\")\n",
    "            checkpoint_df = pd.read_csv(full_checkpoint_path, dtype={'CIK': str}) \n",
    "            if 'CIK' in checkpoint_df.columns: \n",
    "                checkpoint_df['CIK'] = checkpoint_df['CIK'].astype(str).str.zfill(10)\n",
    "\n",
    "            if not checkpoint_df.empty:\n",
    "                last_chkpt_row = checkpoint_df.iloc[-1]\n",
    "                cumulative_prompt_tokens_gemini = int(last_chkpt_row.get('cumulative_prompt_tokens_gemini_at_save', 0))\n",
    "                cumulative_completion_tokens_gemini = int(last_chkpt_row.get('cumulative_completion_tokens_gemini_at_save', 0))\n",
    "                cumulative_cost_gemini = float(last_chkpt_row.get('cumulative_cost_gemini_at_save', 0.0))\n",
    "                tqdm.write(f\"restored gemini cumulative stats: cost=${cumulative_cost_gemini:.6f}, ptokens={cumulative_prompt_tokens_gemini}, ctokens={cumulative_completion_tokens_gemini}\")\n",
    "\n",
    "                last_row_error_info = last_chkpt_row.get('error')\n",
    "                is_last_row_an_error = pd.notna(last_row_error_info) and str(last_row_error_info).strip() != \"\"\n",
    "\n",
    "                if \"ERROR\" in latest_checkpoint_file and is_last_row_an_error:\n",
    "                    all_processed_results_gemini = checkpoint_df.iloc[:-1].to_dict('records')\n",
    "                    start_processing_from_original_df_index = max_original_rows_covered_by_checkpoint - 1\n",
    "                    tqdm.write(f\"loaded {len(all_processed_results_gemini)} successful gemini results. will re-attempt original df index {start_processing_from_original_df_index}.\")\n",
    "                else:\n",
    "                    all_processed_results_gemini = checkpoint_df.to_dict('records')\n",
    "                    start_processing_from_original_df_index = max_original_rows_covered_by_checkpoint\n",
    "                    tqdm.write(f\"loaded {len(all_processed_results_gemini)} gemini results. resuming from original df index {start_processing_from_original_df_index}.\")\n",
    "            else: tqdm.write(\"gemini checkpoint was empty.\")\n",
    "        except Exception as e_load_chkpt:\n",
    "            tqdm.write(f\"error loading gemini checkpoint {full_checkpoint_path}: {e_load_chkpt}. starting fresh.\")\n",
    "            all_processed_results_gemini = []; start_processing_from_original_df_index = 0\n",
    "            cumulative_cost_gemini = 0.0; cumulative_prompt_tokens_gemini = 0; cumulative_completion_tokens_gemini = 0\n",
    "    else: tqdm.write(\"no gemini checkpoint found. starting fresh.\")\n",
    "\n",
    "    df_to_process_slice = df.iloc[start_processing_from_original_df_index:]\n",
    "\n",
    "    if df_to_process_slice.empty and start_processing_from_original_df_index >= len(df):\n",
    "        tqdm.write(f\"all {len(df)} rows appear to have been processed for gemini based on checkpoints.\")\n",
    "    elif len(df) == 0: \n",
    "        tqdm.write(\"input dataframe 'df' is empty. exiting.\"); exit()\n",
    "\n",
    "    newly_processed_results_gemini_this_session = []\n",
    "    stop_all_processing_flag = False\n",
    "\n",
    "    for original_df_index, row_data in tqdm(df_to_process_slice.iterrows(),\n",
    "                                             initial=0, \n",
    "                                             total=len(df_to_process_slice),\n",
    "                                             desc=f\"Processing Rows for Gemini (Original df index {start_processing_from_original_df_index} to {len(df)-1})\"):\n",
    "\n",
    "        current_row_result_dict = {\n",
    "            'CIK': str(row_data.get(input_col_cik, \"N/A_CIK\")).zfill(10),\n",
    "            'Ticker': str(row_data.get(input_col_ticker, \"N/A_Ticker\")),\n",
    "            'Company Name': str(row_data.get(input_col_company_name, \"N/A_CompName\")),\n",
    "            'Sector': str(row_data.get(input_col_sector, \"N/A_Sector\")),\n",
    "            'Year': row_data.get(input_col_year, None),\n",
    "            'filingDate': row_data.get(input_col_filing_date, None),\n",
    "            'Overall Summary': \"N/A\", 'Strategic Depth': \"N/A\", 'Disclosure Sentiment': \"N/A\",\n",
    "            'Risk - Own Adoption': \"N/A\", 'Risk - External Threats': \"N/A\", 'Risk - Non-Adoption': \"N/A\",\n",
    "            'Forward-Looking': \"N/A\", 'AI Washing Index': \"N/A\", 'Talent & Investment': \"N/A\",\n",
    "            'Key AI Terms': \"\", 'error': None,\n",
    "            'api_call_cost_for_row': 0.0,\n",
    "            'prompt_tokens_for_row': 0,\n",
    "            'completion_tokens_for_row': 0\n",
    "        }\n",
    "        cik_val_for_log = current_row_result_dict['CIK']\n",
    "        company_name_for_llm = current_row_result_dict['Company Name'] if current_row_result_dict['Company Name'] not in [\"N/A_CompName\", \"nan\", \"None\", \"\"] else current_row_result_dict['Ticker']\n",
    "        if not company_name_for_llm or company_name_for_llm.lower() == \"n/a_ticker\" or company_name_for_llm.lower() == \"nan\":\n",
    "            company_name_for_llm = f\"CIK_{cik_val_for_log}\"\n",
    "\n",
    "        risk_text_val = str(row_data.get(input_col_risk_factors, \"\"))\n",
    "        mda_text_val = str(row_data.get(input_col_mda, \"\"))\n",
    "        \n",
    "        risk_text_is_missing_or_empty = pd.isna(row_data.get(input_col_risk_factors)) or not risk_text_val.strip()\n",
    "        mda_text_is_missing_or_empty = pd.isna(row_data.get(input_col_mda)) or not mda_text_val.strip()\n",
    "\n",
    "        if risk_text_is_missing_or_empty and mda_text_is_missing_or_empty: \n",
    "            error_message = f\"Missing/empty {input_col_risk_factors} AND {input_col_mda}\"\n",
    "            current_row_result_dict['error'] = error_message\n",
    "            tqdm.write(f\"skipping cik {cik_val_for_log} (original index: {original_df_index}) for gemini: {current_row_result_dict['error']}.\")\n",
    "            newly_processed_results_gemini_this_session.append(current_row_result_dict)\n",
    "            current_total_original_df_rows_covered = start_processing_from_original_df_index + len(newly_processed_results_gemini_this_session)\n",
    "            if len(newly_processed_results_gemini_this_session) > 0 and \\\n",
    "               (current_total_original_df_rows_covered % CHECKPOINT_INTERVAL == 0 or \\\n",
    "                original_df_index == df.index[-1]): \n",
    "                save_gemini_checkpoint(all_processed_results_gemini + newly_processed_results_gemini_this_session,\n",
    "                                      current_total_original_df_rows_covered,\n",
    "                                      OUTPUT_BASE_DIR_GEMINI, CHECKPOINT_FILENAME_PREFIX_GEMINI,\n",
    "                                      is_error_save=True) \n",
    "            continue\n",
    "\n",
    "        consecutive_api_failures_for_row = 0\n",
    "        api_call_successful_and_parsed = False\n",
    "\n",
    "        while consecutive_api_failures_for_row < MAX_RETRIES_PER_ROW:\n",
    "            analysis_object = None\n",
    "            attempt_p_tokens, attempt_c_tokens = 0, 0\n",
    "\n",
    "            try:\n",
    "                tqdm.write(f\"attempt {consecutive_api_failures_for_row + 1}/{MAX_RETRIES_PER_ROW} for cik {cik_val_for_log} (gemini) (original index: {original_df_index})\")\n",
    "                analysis_object, attempt_p_tokens, attempt_c_tokens = analyze_text_with_gemini_instructor(\n",
    "                    risk_text_val, mda_text_val, company_name_for_llm, temperature=0.0\n",
    "                )\n",
    "\n",
    "                current_row_result_dict['prompt_tokens_for_row'] += attempt_p_tokens\n",
    "                current_row_result_dict['completion_tokens_for_row'] += attempt_c_tokens\n",
    "                current_call_cost = calculate_and_update_cost_gemini(attempt_p_tokens, attempt_c_tokens, True)\n",
    "                current_row_result_dict['api_call_cost_for_row'] += current_call_cost\n",
    "\n",
    "                if analysis_object:\n",
    "                    tqdm.write(f\"success: cik {cik_val_for_log} (gemini) (original index: {original_df_index}) parsed.\")\n",
    "                    current_row_result_dict.update({\n",
    "                        'Overall Summary': analysis_object.overall_ai_preparedness_summary_cot,\n",
    "                        'Strategic Depth': extract_grade_letter(analysis_object.ai_strategic_depth.bucket_assessment),\n",
    "                        'Disclosure Sentiment': extract_grade_letter(analysis_object.ai_disclosure_sentiment.bucket_assessment),\n",
    "                        'Risk - Own Adoption': extract_grade_letter(analysis_object.ai_risk_disclosure.risk_from_own_ai_adoption.bucket_assessment),\n",
    "                        'Risk - External Threats': extract_grade_letter(analysis_object.ai_risk_disclosure.risk_from_external_ai_threats.bucket_assessment),\n",
    "                        'Risk - Non-Adoption': extract_grade_letter(analysis_object.ai_risk_disclosure.risk_of_not_adopting_ai_or_competitive_ai_threat.bucket_assessment),\n",
    "                        'Forward-Looking': extract_grade_letter(analysis_object.forward_looking_ai_statements.bucket_assessment),\n",
    "                        'AI Washing Index': extract_grade_letter(analysis_object.ai_washing_hype_index.bucket_assessment),\n",
    "                        'Talent & Investment': extract_grade_letter(analysis_object.ai_talent_and_investment_focus.bucket_assessment),\n",
    "                        'Key AI Terms': ', '.join(analysis_object.key_ai_related_terminology_found or []),\n",
    "                        'error': None\n",
    "                    })\n",
    "                    api_call_successful_and_parsed = True\n",
    "                    break \n",
    "                else: # should not happen if analyze_text_with_gemini_instructor raises exceptions on failure\n",
    "                    current_row_result_dict['error'] = 'gemini analysis returned no object without specific exception (logic error).'\n",
    "                    tqdm.write(f\"error for cik {cik_val_for_log}: {current_row_result_dict['error']}\")\n",
    "                    break \n",
    "\n",
    "            except (ResourceExhausted, InternalServerError, ServiceUnavailable, DeadlineExceeded, TypeError, ValidationError) as specific_err:\n",
    "                error_type_name = type(specific_err).__name__\n",
    "                tqdm.write(f\"gemini api/validation error (attempt {consecutive_api_failures_for_row + 1}) for cik {cik_val_for_log}: {error_type_name} - {str(specific_err)[:200]}\")\n",
    "                current_row_result_dict['error'] = f'gemini error attempt {consecutive_api_failures_for_row + 1}: {error_type_name} - {str(specific_err)[:100]}'\n",
    "                consecutive_api_failures_for_row += 1\n",
    "\n",
    "                if consecutive_api_failures_for_row >= MAX_RETRIES_PER_ROW:\n",
    "                    tqdm.write(f\"max retries for gemini errors on cik {cik_val_for_log}. recording error.\")\n",
    "                    break\n",
    "                else:\n",
    "                    delay = BACKOFF_DELAYS[consecutive_api_failures_for_row -1] \n",
    "                    tqdm.write(f\"waiting {delay}s before next retry for cik {cik_val_for_log} (gemini).\")\n",
    "                    time.sleep(delay)\n",
    "            \n",
    "            except Exception as e_main_loop_unexpected: \n",
    "                tqdm.write(f\"main loop unexpected error (attempt {consecutive_api_failures_for_row + 1}) for cik {cik_val_for_log}: {type(e_main_loop_unexpected).__name__} - {str(e_main_loop_unexpected)[:200]}\")\n",
    "                current_row_result_dict['error'] = f'unexpected main loop gemini attempt {consecutive_api_failures_for_row + 1}: {str(e_main_loop_unexpected)[:100]}'\n",
    "                consecutive_api_failures_for_row += 1\n",
    "                if consecutive_api_failures_for_row >= MAX_RETRIES_PER_ROW:\n",
    "                    tqdm.write(f\"max retries for unexpected errors on cik {cik_val_for_log} (gemini). recording error.\")\n",
    "                    break\n",
    "                else:\n",
    "                    delay = BACKOFF_DELAYS[consecutive_api_failures_for_row -1]\n",
    "                    tqdm.write(f\"waiting {delay}s before next retry for cik {cik_val_for_log} (gemini).\")\n",
    "                    time.sleep(delay)\n",
    "\n",
    "        newly_processed_results_gemini_this_session.append(current_row_result_dict)\n",
    "\n",
    "        if stop_all_processing_flag: \n",
    "            tqdm.write(\"stop flag activated. ending processing loop for gemini.\")\n",
    "            break\n",
    "\n",
    "        current_total_original_df_rows_covered = start_processing_from_original_df_index + len(newly_processed_results_gemini_this_session)\n",
    "        is_current_row_error = bool(current_row_result_dict.get('error'))\n",
    "        \n",
    "        if len(newly_processed_results_gemini_this_session) > 0 and \\\n",
    "           (current_total_original_df_rows_covered % CHECKPOINT_INTERVAL == 0 or \\\n",
    "            original_df_index == df.index[-1] or \\\n",
    "            (is_current_row_error and consecutive_api_failures_for_row >= MAX_RETRIES_PER_ROW ) ): \n",
    "\n",
    "            combined_results_for_checkpoint = all_processed_results_gemini + newly_processed_results_gemini_this_session\n",
    "            save_gemini_checkpoint(combined_results_for_checkpoint,\n",
    "                                  current_total_original_df_rows_covered,\n",
    "                                  OUTPUT_BASE_DIR_GEMINI,\n",
    "                                  CHECKPOINT_FILENAME_PREFIX_GEMINI,\n",
    "                                  is_error_save=is_current_row_error) \n",
    "\n",
    "        if api_call_successful_and_parsed and API_REQUEST_DELAY_ON_SUCCESS > 0:\n",
    "            time.sleep(API_REQUEST_DELAY_ON_SUCCESS)\n",
    "\n",
    "    final_complete_list_of_results_gemini = all_processed_results_gemini + newly_processed_results_gemini_this_session\n",
    "\n",
    "    if final_complete_list_of_results_gemini:\n",
    "        final_save_is_error_state = False\n",
    "        if final_complete_list_of_results_gemini and final_complete_list_of_results_gemini[-1].get('error'):\n",
    "            final_save_is_error_state = True\n",
    "        \n",
    "        final_rows_covered_count = start_processing_from_original_df_index + len(newly_processed_results_gemini_this_session)\n",
    "        if final_rows_covered_count > len(df): \n",
    "            final_rows_covered_count = len(df)\n",
    "\n",
    "        save_gemini_checkpoint(final_complete_list_of_results_gemini,\n",
    "                              final_rows_covered_count, \n",
    "                              OUTPUT_BASE_DIR_GEMINI,\n",
    "                              CHECKPOINT_FILENAME_PREFIX_GEMINI,\n",
    "                              is_error_save=final_save_is_error_state) \n",
    "\n",
    "        df_results_gemini_2_5_flash = pd.DataFrame(final_complete_list_of_results_gemini)\n",
    "        try:\n",
    "            if 'CIK' in df_results_gemini_2_5_flash.columns: \n",
    "                df_results_gemini_2_5_flash['CIK'] = df_results_gemini_2_5_flash['CIK'].astype(str).str.zfill(10)\n",
    "\n",
    "            output_columns_ordered = [\n",
    "                'CIK', 'Ticker', 'Company Name', 'Sector', 'Year', 'filingDate',\n",
    "                'Overall Summary', 'Strategic Depth', 'Disclosure Sentiment',\n",
    "                'Risk - Own Adoption', 'Risk - External Threats', 'Risk - Non-Adoption',\n",
    "                'Forward-Looking', 'AI Washing Index', 'Talent & Investment', 'Key AI Terms',\n",
    "                'error', 'api_call_cost_for_row', 'prompt_tokens_for_row', 'completion_tokens_for_row',\n",
    "                'cumulative_prompt_tokens_gemini_at_save',\n",
    "                'cumulative_completion_tokens_gemini_at_save',\n",
    "                'cumulative_cost_gemini_at_save'\n",
    "            ]\n",
    "            final_output_cols_present = [col for col in output_columns_ordered if col in df_results_gemini_2_5_flash.columns]\n",
    "            \n",
    "            df_results_gemini_2_5_flash[final_output_cols_present].to_csv(FINAL_RESULTS_FILENAME_GEMINI, index=False)\n",
    "            print(f\"\\n--- processing finished. final gemini 2.5 flash results saved to: {FINAL_RESULTS_FILENAME_GEMINI} ({len(df_results_gemini_2_5_flash)} rows) ---\")\n",
    "            if not df_results_gemini_2_5_flash.empty:\n",
    "                print(df_results_gemini_2_5_flash[final_output_cols_present].head().to_string())\n",
    "        except Exception as e_final_save:\n",
    "            print(f\"error saving final gemini 2.5 flash results to {FINAL_RESULTS_FILENAME_GEMINI}: {e_final_save}\")\n",
    "    else:\n",
    "        print(\"\\nno gemini 2.5 flash results were processed or loaded from checkpoints to save in the final file.\")\n",
    "\n",
    "    print(f\"\\n--- final gemini 2.5 flash cost summary ---\\ntotal prompt tokens: {cumulative_prompt_tokens_gemini}\\ntotal completion tokens: {cumulative_completion_tokens_gemini}\\ntotal estimated cost: ${cumulative_cost_gemini:.6f}\")\n",
    "    print(\"\\n--- main execution for gemini 2.5 flash finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4761f9-1e84-499c-aa1c-190de72d97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# part 1: load all of our data\n",
    "# panel data and the results from the three different ai models.\n",
    "print(\"--- loading all source dataframes ---\")\n",
    "\n",
    "# this is the main folder where all my thesis data is located.\n",
    "BASE_DIR = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/\"\n",
    "\n",
    "path_panel_data = os.path.join(BASE_DIR, 'panel_data_charactermax200k.parquet')\n",
    "path_openai = os.path.join(BASE_DIR, 'OpenAI_Analysis_Results/openai_detailed_analysis_FINAL_ALL_ROWS.csv')\n",
    "path_gemini_1_5 = os.path.join(BASE_DIR, 'Gemini_1_5_Flash_Analysis_Results/gemini_1_5_flash_detailed_analysis_FINAL_ALL_ROWS.csv')\n",
    "path_gemini_2_5 = os.path.join(BASE_DIR, 'Gemini_2_5_Flash_Analysis_Results/gemini_2_5_flash_detailed_analysis_FINAL_ALL_ROWS.csv')\n",
    "\n",
    "def load_data(filepath, is_parquet=False):\n",
    "    \"\"\"a little helper function to load files safely.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"error: couldn't find the file at {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        if is_parquet:\n",
    "            df = pd.read_parquet(filepath)\n",
    "        else:\n",
    "            # making sure the CIK is always read as text to keep the leading zeros.\n",
    "            df = pd.read_csv(filepath, dtype={'CIK': str})\n",
    "        print(f\"successfully loaded {os.path.basename(filepath)} with {len(df)} rows.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"ran into an error loading {filepath}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# now, let's load everything into memory.\n",
    "panel_df = load_data(path_panel_data, is_parquet=True)\n",
    "df_gpt4o = load_data(path_openai)\n",
    "df_gemini_1_5 = load_data(path_gemini_1_5)\n",
    "df_gemini_2_5 = load_data(path_gemini_2_5)\n",
    "\n",
    "\n",
    "# part 2: getting the ai model results ready to merge\n",
    "print(\"\\n--- preparing and merging ai model results ---\")\n",
    "\n",
    "def prepare_df_for_merge(df, suffix):\n",
    "    \"\"\"this function gets each model's results ready for the big merge.\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    base_cols = ['CIK', 'Year']\n",
    "    score_cols = [\n",
    "        'Overall Summary', 'Strategic Depth', 'Disclosure Sentiment',\n",
    "        'Risk - Own Adoption', 'Risk - External Threats', 'Risk - Non-Adoption',\n",
    "        'Forward-Looking', 'AI Washing Index', 'Talent & Investment', 'Key AI Terms'\n",
    "    ]\n",
    "    \n",
    "    # making sure (cik and year) are in the right format.\n",
    "    df['CIK'] = df['CIK'].astype(str).str.zfill(10)\n",
    "    df['Year'] = df['Year'].astype(int)\n",
    "    \n",
    "    cols_to_use = base_cols + [col for col in score_cols if col in df.columns]\n",
    "    df_subset = df[cols_to_use].copy()\n",
    "    \n",
    "    # adding a suffix to each score column \n",
    "    rename_dict = {col: f\"{col}_{suffix}\" for col in score_cols if col in df.columns}\n",
    "    df_renamed = df_subset.rename(columns=rename_dict)\n",
    "    \n",
    "    return df_renamed\n",
    "\n",
    "# running the prep function on each model's dataframe.\n",
    "df_gpt4o_prep = prepare_df_for_merge(df_gpt4o, 'gpt4o_mini')\n",
    "df_gemini_1_5_prep = prepare_df_for_merge(df_gemini_1_5, 'gemini_1_5')\n",
    "df_gemini_2_5_prep = prepare_df_for_merge(df_gemini_2_5, 'gemini_2_5')\n",
    "\n",
    "# time to combine the scores from all three models using CIK and Year as the link.\n",
    "merge_keys = ['CIK', 'Year']\n",
    "df_merged_scores = pd.merge(df_gpt4o_prep, df_gemini_1_5_prep, on=merge_keys, how='outer')\n",
    "df_merged_scores = pd.merge(df_merged_scores, df_gemini_2_5_prep, on=merge_keys, how='outer')\n",
    "\n",
    "print(f\"successfully merged ai scores. shape: {df_merged_scores.shape}\")\n",
    "\n",
    "\n",
    "# part 3: bringing the ai scores into the main dataset\n",
    "print(\"\\n--- merging ai scores with main panel dataset ---\")\n",
    "\n",
    "# making sure the keys in our main panel dataframe match the format of the scores dataframe.\n",
    "panel_df = panel_df.rename(columns={'cik': 'CIK', 'year': 'Year'})\n",
    "panel_df['CIK'] = panel_df['CIK'].astype(str).str.zfill(10)\n",
    "panel_df['Year'] = panel_df['Year'].astype(int)\n",
    "\n",
    "# here's the main merge: adding the AI scores to the big panel dataset.\n",
    "final_df = pd.merge(panel_df, df_merged_scores, on=['CIK', 'Year'], how='left')\n",
    "\n",
    "print(f\"final merged dataset shape: {final_df.shape}\")\n",
    "\n",
    "\n",
    "# part 4: the final cleanup to create a perfect, balanced dataset\n",
    "print(\"\\n--- final data cleaning and balancing ---\")\n",
    "\n",
    "#  panel should already be balanced from notebook 01, but double-check.\n",
    "required_years = set(range(2020, 2025))\n",
    "year_counts = final_df.groupby('CIK')['Year'].nunique()\n",
    "complete_ciks = year_counts[year_counts == 5].index\n",
    "\n",
    "df_balanced = final_df[final_df['CIK'].isin(complete_ciks)].copy()\n",
    "print(f\"filtered for complete companies. kept {len(complete_ciks)} companies.\")\n",
    "\n",
    "# let's do one last verification to be sure.\n",
    "final_companies = df_balanced['CIK'].nunique()\n",
    "expected_rows = final_companies * 5\n",
    "print(f\"final shape: {df_balanced.shape}\")\n",
    "print(f\"unique companies: {final_companies}\")\n",
    "print(f\"expected rows for balanced panel (companies * 5 years): {expected_rows}\")\n",
    "\n",
    "if len(df_balanced) == expected_rows:\n",
    "    print(\"✅ success: the final dataset is a perfectly balanced panel.\")\n",
    "else:\n",
    "    print(\"❌ error: the dataset is not balanced. something went wrong.\")\n",
    "\n",
    "\n",
    "# part 5: putting everything in a nice order and saving the final file\n",
    "print(\"\\n--- saving final analysis-ready dataset ---\")\n",
    "\n",
    "final_output_path = os.path.join(BASE_DIR, 'final_panel_with_scores.parquet')\n",
    "\n",
    "if not df_balanced.empty:\n",
    "    try:\n",
    "        # i like my columns in a specific order, so let's arrange them.\n",
    "        id_cols = ['CIK', 'Ticker', 'Company Name', 'Sector', 'Year', 'filingDate', 'form']\n",
    "        \n",
    "        # grouping the scores by metric makes the file easier to read.\n",
    "        score_metrics = [\n",
    "            'Strategic Depth', 'Disclosure Sentiment', 'Risk - Own Adoption',\n",
    "            'Risk - External Threats', 'Risk - Non-Adoption', 'Forward-Looking',\n",
    "            'Talent & Investment', 'AI Washing Index', 'Overall Summary', 'Key AI Terms'\n",
    "        ]\n",
    "        model_suffixes = ['_gpt4o_mini', '_gemini_1_5', '_gemini_2_5']\n",
    "        \n",
    "        ordered_score_cols = []\n",
    "        for metric in score_metrics:\n",
    "            for suffix in model_suffixes:\n",
    "                col_name = f\"{metric}{suffix}\"\n",
    "                if col_name in df_balanced.columns:\n",
    "                    ordered_score_cols.append(col_name)\n",
    "\n",
    "        text_cols = ['risk_factors_text', 'item7_mda_text']\n",
    "        \n",
    "        # bringing all the column lists together.\n",
    "        final_order = [col for col in id_cols if col in df_balanced.columns] + \\\n",
    "                      ordered_score_cols + \\\n",
    "                      [col for col in text_cols if col in df_balanced.columns]\n",
    "        \n",
    "        # just in case any columns were missed.\n",
    "        other_cols = [col for col in df_balanced.columns if col not in final_order]\n",
    "        final_order.extend(other_cols)\n",
    "\n",
    "        # finally, reordering the dataframe and saving it.\n",
    "        df_to_save = df_balanced[final_order]\n",
    "        df_to_save.to_parquet(final_output_path, index=False)\n",
    "        \n",
    "        print(f\"✅ final analysis-ready dataset saved to: {final_output_path}\")\n",
    "        print(f\"shape: {df_to_save.shape}\")\n",
    "        print(f\"\\nsample of final data:\\n\")\n",
    "        print(df_to_save[id_cols].head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error saving final dataset: {e}\")\n",
    "else:\n",
    "    print(\"final dataframe is empty, so i didn't save anything.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539883fc-0dcb-4fdc-93f8-e35732d941f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CompleteAIFactorDatasetBuilder:\n",
    "    \"\"\"\n",
    "    this is my all-in-one dataset builder.\n",
    "    i'm putting everything in one place to avoid confusion.\n",
    "    it loads all my data, adds financial factors, calculates returns with\n",
    "    quality checks built-in, and gets everything ready for my regressions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_ai_dataset, daily_path, fundamentals_path, ff_factors_path):\n",
    "        \"\"\"\n",
    "        here i set up all the initial parameters. i'm defining my file paths,\n",
    "        return horizons, and most importantly, my quality control settings like\n",
    "        minimum stock price. this way, quality is handled from the very beginning.\n",
    "        \"\"\"\n",
    "        self.base_df = base_ai_dataset.copy()\n",
    "        self.daily_path = daily_path\n",
    "        self.fundamentals_path = fundamentals_path\n",
    "        self.ff_factors_path = ff_factors_path\n",
    "        \n",
    "        # these are the time windows i'm interested in for calculating stock returns.\n",
    "        self.price_offsets = [0, 3, 6, 9, 12]  \n",
    "        self.return_horizons = [3, 6, 9, 12]   \n",
    "        \n",
    "        # setting a cutoff date for my data to keep things consistent.\n",
    "        self.data_cutoff = pd.Timestamp('2025-05-31')\n",
    "        \n",
    "        # my quality rules. i want to avoid penny stocks and crazy outlier returns.\n",
    "        self.min_price = 1.0       # minimum price to avoid penny stocks\n",
    "        self.min_end_price = 0.1   # minimum ending price (avoid total crashes)\n",
    "        self.max_return = 5.0      # maximum allowed return (500%)\n",
    "        self.min_return = -0.95    # minimum allowed return (-95%)\n",
    "        \n",
    "        # i'm only selecting the columns i actually need to keep my dataframes tidy.\n",
    "        self.daily_cols_wanted = ['gvkey', 'datadate', 'prccd', 'cshoc', 'cshtrd', \n",
    "                                  'ggroup', 'gind', 'gsector', 'gsubind']\n",
    "        self.fund_cols_wanted = ['gvkey', 'datadate', 'atq', 'cheq', 'cshoq', \n",
    "                                 'dlttq', 'epsfxq', 'niq', 'oiadpq', 'teqq', 'revty']\n",
    "        \n",
    "        # just getting my main ai dataset ready by making sure the dates and gvkey are in the right format.\n",
    "        self.base_df['filingDate'] = pd.to_datetime(self.base_df['filingDate'])\n",
    "        self.base_df['gvkey'] = self.base_df['gvkey'].astype(str).str.strip()\n",
    "        \n",
    "        print(\"🎯 COMPLETE AI FACTOR DATASET BUILDER\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"📊 Base AI dataset: {self.base_df.shape}\")\n",
    "        print(f\"💰 Quality settings:\")\n",
    "        print(f\"    Min starting price: ${self.min_price}\")\n",
    "        print(f\"    Return bounds: {self.min_return:.0%} to {self.max_return:.0%}\")\n",
    "        print(f\"🧹 Selected columns only (no bloat)\")\n",
    "        print(f\"🎯 One clean process - no dataset confusion!\")\n",
    "        \n",
    "    def load_source_data(self):\n",
    "        \"\"\"this function loads all my external data sources: fama-french, daily prices, and fundamentals.\"\"\"\n",
    "        print(\"\\n📊 LOADING SOURCE DATA\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # loading the fama-french factors.\n",
    "        print(\"Loading FF factors...\")\n",
    "        self.ff_factors = pd.read_csv(self.ff_factors_path)\n",
    "        if 'date' in self.ff_factors.columns:\n",
    "            try:\n",
    "                self.ff_factors['date'] = pd.to_datetime(self.ff_factors['date'], format='%Y%m%d')\n",
    "            except:\n",
    "                self.ff_factors['date'] = pd.to_datetime(self.ff_factors['date'])\n",
    "        \n",
    "        # sometimes the ff factors are in percentages, so i check and convert them to decimals if needed.\n",
    "        factor_cols = ['mktrf', 'smb', 'hml', 'rmw', 'cma', 'rf', 'umd']\n",
    "        for col in factor_cols:\n",
    "            if col in self.ff_factors.columns and self.ff_factors[col].abs().max() > 1:\n",
    "                self.ff_factors[col] = self.ff_factors[col] / 100\n",
    "        \n",
    "        self.ff_factors = self.ff_factors.sort_values('date').reset_index(drop=True)\n",
    "        print(f\"✅ FF Factors: {self.ff_factors.shape}\")\n",
    "        \n",
    "        # loading the daily stock data. i'm using `usecols` to only grab what i need, which saves memory.\n",
    "        print(\"Loading daily market data...\")\n",
    "        try:\n",
    "            sample_daily = pd.read_csv(self.daily_path, nrows=1)\n",
    "            available_cols = list(sample_daily.columns)\n",
    "            cols_to_use = [col for col in self.daily_cols_wanted if col in available_cols]\n",
    "            missing_cols = [col for col in self.daily_cols_wanted if col not in available_cols]\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"⚠️ Missing daily columns: {missing_cols}\")\n",
    "            \n",
    "            self.df_daily = pd.read_csv(self.daily_path, usecols=cols_to_use, dtype={'gvkey': str})\n",
    "            self.df_daily['gvkey'] = self.df_daily['gvkey'].str.strip()\n",
    "            self.df_daily['datadate'] = pd.to_datetime(self.df_daily['datadate'])\n",
    "            \n",
    "            print(f\"✅ Daily Data: {self.df_daily.shape}, columns: {len(cols_to_use)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading daily data: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # same thing for the fundamentals data.\n",
    "        print(\"Loading fundamentals...\")\n",
    "        try:\n",
    "            sample_fund = pd.read_csv(self.fundamentals_path, nrows=1)\n",
    "            available_fund_cols = list(sample_fund.columns)\n",
    "            fund_cols_to_use = [col for col in self.fund_cols_wanted if col in available_fund_cols]\n",
    "            missing_fund_cols = [col for col in self.fund_cols_wanted if col not in available_fund_cols]\n",
    "            \n",
    "            if missing_fund_cols:\n",
    "                print(f\"⚠️ Missing fundamental columns: {missing_fund_cols}\")\n",
    "            \n",
    "            self.df_fundamentals = pd.read_csv(self.fundamentals_path, usecols=fund_cols_to_use, dtype={'gvkey': str})\n",
    "            self.df_fundamentals['gvkey'] = self.df_fundamentals['gvkey'].str.strip()\n",
    "            self.df_fundamentals['datadate'] = pd.to_datetime(self.df_fundamentals['datadate'])\n",
    "            \n",
    "            print(f\"✅ Fundamentals: {self.df_fundamentals.shape}, columns: {len(fund_cols_to_use)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading fundamentals: {e}\")\n",
    "            return None\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def calculate_cumulative_rf(self, start_date, months):\n",
    "        \"\"\"a helper function to calculate the cumulative risk-free rate over a specific period. i'll need this to calculate excess returns.\"\"\"\n",
    "        end_date = start_date + pd.DateOffset(months=months)\n",
    "        \n",
    "        period_rf = self.ff_factors[\n",
    "            (self.ff_factors['date'] >= start_date) & \n",
    "            (self.ff_factors['date'] <= end_date)\n",
    "        ]['rf'].copy()\n",
    "        \n",
    "        if len(period_rf) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # the formula for cumulative return.\n",
    "        cumulative_rf = 1.0\n",
    "        valid_days = 0\n",
    "        \n",
    "        for daily_rf in period_rf:\n",
    "            if pd.notna(daily_rf):\n",
    "                cumulative_rf *= (1 + daily_rf)\n",
    "                valid_days += 1\n",
    "        \n",
    "        # i'll only calculate the rate if i have enough data points for the period.\n",
    "        min_days_required = max(30, months * 15)\n",
    "        if valid_days < min_days_required:\n",
    "            return np.nan\n",
    "        \n",
    "        return cumulative_rf - 1\n",
    "    \n",
    "    def add_fama_french_and_rf(self):\n",
    "        \"\"\"this is where i add the fama-french factors and the cumulative risk-free rate i just calculated to my main dataset.\"\"\"\n",
    "        print(\"\\n📊 ADDING FAMA-FRENCH FACTORS + CUMULATIVE RF\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # starting with my base dataset.\n",
    "        self.result_df = self.base_df.copy()\n",
    "        \n",
    "        # getting the names of the factor columns.\n",
    "        factor_cols = [col for col in self.ff_factors.columns if col != 'date']\n",
    "        \n",
    "        # it's more efficient to create the empty columns first.\n",
    "        for col in factor_cols:\n",
    "            self.result_df[f'ff_{col}'] = np.nan\n",
    "        \n",
    "        for horizon in self.return_horizons:\n",
    "            self.result_df[f'rf_{horizon}m_cumulative'] = np.nan\n",
    "        \n",
    "        successful_ff = 0\n",
    "        successful_rf = 0\n",
    "        \n",
    "        print(\"Matching FF factors and calculating cumulative RF...\")\n",
    "        for idx, row in tqdm(self.result_df.iterrows(), total=len(self.result_df), desc=\"FF + RF\"):\n",
    "            filing_date = row['filingDate']\n",
    "            \n",
    "            # matching the closest fama-french data to my filing date.\n",
    "            date_diffs = abs(self.ff_factors['date'] - filing_date).dt.days\n",
    "            valid_matches = date_diffs <= 31\n",
    "            \n",
    "            if valid_matches.any():\n",
    "                closest_idx = date_diffs[valid_matches].idxmin()\n",
    "                closest_ff_row = self.ff_factors.loc[closest_idx]\n",
    "                \n",
    "                for col in factor_cols:\n",
    "                    if pd.notna(closest_ff_row[col]):\n",
    "                        self.result_df.at[idx, f'ff_{col}'] = closest_ff_row[col]\n",
    "                \n",
    "                successful_ff += 1\n",
    "            \n",
    "            # calculating the cumulative risk-free rate for each horizon.\n",
    "            for horizon in self.return_horizons:\n",
    "                try:\n",
    "                    cumulative_rf = self.calculate_cumulative_rf(filing_date, horizon)\n",
    "                    if pd.notna(cumulative_rf):\n",
    "                        self.result_df.at[idx, f'rf_{horizon}m_cumulative'] = cumulative_rf\n",
    "                        successful_rf += 1\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        print(f\"✅ FF factors: {successful_ff:,}/{len(self.result_df):,}\")\n",
    "        print(f\"✅ Cumulative RF: {successful_rf:,} calculations\")\n",
    "        return self\n",
    "    \n",
    "    def add_prices_and_calculate_quality_returns(self):\n",
    "        \"\"\"\n",
    "        this is a key step. i'm getting the stock prices and calculating returns,\n",
    "        but i'm applying my quality filters right away.\n",
    "        \"\"\"\n",
    "        print(\"\\n💰 ADDING PRICES + CALCULATING QUALITY RETURNS\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"🛡️ Quality filters applied during calculation:\")\n",
    "        print(f\"    Min starting price: ${self.min_price}\")\n",
    "        print(f\"    Return bounds: {self.min_return:.0%} to {self.max_return:.0%}\")\n",
    "        \n",
    "        # creating the columns i'll fill in this step.\n",
    "        for offset in self.price_offsets:\n",
    "            self.result_df[f'price_t{offset}'] = np.nan\n",
    "        \n",
    "        for horizon in self.return_horizons:\n",
    "            self.result_df[f'return_{horizon}mo'] = np.nan\n",
    "            self.result_df[f'return_{horizon}mo_annualized'] = np.nan\n",
    "            self.result_df[f'excess_return_{horizon}mo'] = np.nan\n",
    "            self.result_df[f'excess_return_{horizon}mo_annualized'] = np.nan\n",
    "        \n",
    "        # getting the sector info at the same time.\n",
    "        sector_cols = ['ggroup', 'gind', 'gsector', 'gsubind']\n",
    "        for col in sector_cols:\n",
    "            self.result_df[f'sector_{col}'] = np.nan\n",
    "        \n",
    "        price_matches = 0\n",
    "        return_calculations = 0\n",
    "        quality_filtered = 0\n",
    "        \n",
    "        print(\"Processing prices and calculating quality returns...\")\n",
    "        for idx, row in tqdm(self.result_df.iterrows(), total=len(self.result_df), desc=\"Prices+Returns\"):\n",
    "            gvkey = row['gvkey']\n",
    "            filing_date = row['filingDate']\n",
    "            \n",
    "            # looking up the daily data for the specific company in the current row.\n",
    "            company_daily = self.df_daily[self.df_daily['gvkey'] == gvkey].copy()\n",
    "            if company_daily.empty:\n",
    "                continue\n",
    "            \n",
    "            # getting sector info.\n",
    "            filing_match = abs(company_daily['datadate'] - filing_date).dt.days\n",
    "            if filing_match.min() <= 15:\n",
    "                closest_sector_idx = filing_match.idxmin()\n",
    "                closest_sector_row = company_daily.loc[closest_sector_idx]\n",
    "                \n",
    "                for col in sector_cols:\n",
    "                    if col in closest_sector_row.index and pd.notna(closest_sector_row[col]):\n",
    "                        self.result_df.at[idx, f'sector_{col}'] = closest_sector_row[col]\n",
    "            \n",
    "            # getting prices.\n",
    "            prices = {}\n",
    "            for offset_months in self.price_offsets:\n",
    "                target_date = filing_date + pd.DateOffset(months=offset_months)\n",
    "                \n",
    "                if target_date > self.data_cutoff:\n",
    "                    continue\n",
    "                \n",
    "                # finding the trading day price that is closest to my target date.\n",
    "                date_diffs = abs(company_daily['datadate'] - target_date).dt.days\n",
    "                valid_matches = date_diffs <= 15\n",
    "                \n",
    "                if valid_matches.any():\n",
    "                    closest_idx = date_diffs[valid_matches].idxmin()\n",
    "                    closest_row = company_daily.loc[closest_idx]\n",
    "                    \n",
    "                    if pd.notna(closest_row.get('prccd')) and closest_row['prccd'] > 0:\n",
    "                        prices[offset_months] = closest_row['prccd']\n",
    "                        self.result_df.at[idx, f'price_t{offset_months}'] = closest_row['prccd']\n",
    "                        price_matches += 1\n",
    "            \n",
    "            # calculating returns, but only if the starting price is valid.\n",
    "            if 0 in prices:\n",
    "                base_price = prices[0]\n",
    "                \n",
    "                # my first quality check: the starting price.\n",
    "                if base_price < self.min_price:\n",
    "                    quality_filtered += len(self.return_horizons)\n",
    "                    continue\n",
    "                \n",
    "                for horizon in self.return_horizons:\n",
    "                    if horizon in prices:\n",
    "                        end_price = prices[horizon]\n",
    "                        \n",
    "                        # another quality check on the ending price.\n",
    "                        if end_price < self.min_end_price:\n",
    "                            quality_filtered += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # calculating the raw return.\n",
    "                        raw_return = (end_price / base_price) - 1\n",
    "                        \n",
    "                        # my main quality check on the return value itself.\n",
    "                        if not (self.min_return <= raw_return <= self.max_return):\n",
    "                            quality_filtered += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # a sanity check on the price ratio.\n",
    "                        price_ratio = end_price / base_price\n",
    "                        if not (0.001 <= price_ratio <= 1000):\n",
    "                            quality_filtered += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # if a return passes all my checks, i'll add it to my dataframe.\n",
    "                        self.result_df.at[idx, f'return_{horizon}mo'] = raw_return\n",
    "                        \n",
    "                        # i also want to annualize the returns.\n",
    "                        if horizon != 12:\n",
    "                            annualized = (1 + raw_return) ** (12/horizon) - 1\n",
    "                            self.result_df.at[idx, f'return_{horizon}mo_annualized'] = annualized\n",
    "                        else:\n",
    "                            self.result_df.at[idx, f'return_{horizon}mo_annualized'] = raw_return\n",
    "                        \n",
    "                        # now calculating the excess return.\n",
    "                        rf_col = f'rf_{horizon}m_cumulative'\n",
    "                        if rf_col in self.result_df.columns and pd.notna(self.result_df.at[idx, rf_col]):\n",
    "                            excess_return = raw_return - self.result_df.at[idx, rf_col]\n",
    "                            self.result_df.at[idx, f'excess_return_{horizon}mo'] = excess_return\n",
    "                            \n",
    "                            # annualizing the excess return too.\n",
    "                            if horizon != 12:\n",
    "                                ann_excess = (1 + excess_return) ** (12/horizon) - 1\n",
    "                                self.result_df.at[idx, f'excess_return_{horizon}mo_annualized'] = ann_excess\n",
    "                            else:\n",
    "                                self.result_df.at[idx, f'excess_return_{horizon}mo_annualized'] = excess_return\n",
    "                            \n",
    "                        return_calculations += 1\n",
    "        \n",
    "        print(f\"✅ Price matches: {price_matches:,}\")\n",
    "        print(f\"✅ Quality returns calculated: {return_calculations:,}\")\n",
    "        print(f\"🛡️ Quality filtered: {quality_filtered:,}\")\n",
    "        print(f\"💡 Quality filtering rate: {quality_filtered/(return_calculations+quality_filtered):.1%}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_fundamentals_and_ratios(self):\n",
    "        \"\"\"now, i'm adding the company fundamentals and calculating some key financial ratios.\"\"\"\n",
    "        print(\"\\n💼 ADDING FUNDAMENTALS + CALCULATING RATIOS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # creating empty columns for the fundamental data.\n",
    "        fund_cols = [col for col in self.fund_cols_wanted if col not in ['gvkey', 'datadate']]\n",
    "        for col in fund_cols:\n",
    "            self.result_df[f'fund_{col}'] = np.nan\n",
    "        \n",
    "        successful_fund = 0\n",
    "        \n",
    "        # add fundamental data by finding the closest quarter.\n",
    "        print(\"Adding fundamental data...\")\n",
    "        for idx, row in tqdm(self.result_df.iterrows(), total=len(self.result_df), desc=\"Fundamentals\"):\n",
    "            gvkey = row['gvkey']\n",
    "            filing_date = row['filingDate']\n",
    "            \n",
    "            company_fund = self.df_fundamentals[self.df_fundamentals['gvkey'] == gvkey].copy()\n",
    "            if company_fund.empty:\n",
    "                continue\n",
    "            \n",
    "            date_diffs = abs(company_fund['datadate'] - filing_date).dt.days\n",
    "            valid_quarters = date_diffs <= 90\n",
    "            \n",
    "            if valid_quarters.any():\n",
    "                closest_idx = date_diffs[valid_quarters].idxmin()\n",
    "                closest_quarter = company_fund.loc[closest_idx]\n",
    "                \n",
    "                for col in fund_cols:\n",
    "                    if col in closest_quarter.index and pd.notna(closest_quarter[col]):\n",
    "                        self.result_df.at[idx, f'fund_{col}'] = closest_quarter[col]\n",
    "                \n",
    "                successful_fund += 1\n",
    "        \n",
    "        print(f\"✅ Fundamental matches: {successful_fund:,}\")\n",
    "        \n",
    "        # calculating the ratios.\n",
    "        print(\"Calculating financial ratios...\")\n",
    "        \n",
    "        # just mapping the database codes to readable names.\n",
    "        fund_mapping = {\n",
    "            'niq': 'net_income', 'atq': 'total_assets', 'teqq': 'shareholders_equity',\n",
    "            'dlttq': 'long_term_debt', 'cheq': 'cash_equivalents', 'revty': 'revenue',\n",
    "            'oiadpq': 'operating_income', 'epsfxq': 'eps_diluted', 'cshoq': 'shares_outstanding'\n",
    "        }\n",
    "        \n",
    "        for fund_code, clean_name in fund_mapping.items():\n",
    "            fund_col = f'fund_{fund_code}'\n",
    "            if fund_col in self.result_df.columns:\n",
    "                self.result_df[clean_name] = self.result_df[fund_col]\n",
    "        \n",
    "        ratios_calculated = 0\n",
    "        \n",
    "        # size metrics.\n",
    "        if 'total_assets' in self.result_df.columns:\n",
    "            assets = self.result_df['total_assets'].replace([0, np.inf, -np.inf], np.nan)\n",
    "            self.result_df['calc_log_total_assets'] = np.log(assets)\n",
    "            ratios_calculated += 1\n",
    "        \n",
    "        if 'price_t0' in self.result_df.columns:\n",
    "            if 'shares_outstanding' in self.result_df.columns:\n",
    "                market_cap = self.result_df['price_t0'] * self.result_df['shares_outstanding']\n",
    "                market_cap = market_cap.replace([0, np.inf, -np.inf], np.nan)\n",
    "                self.result_df['calc_market_cap'] = market_cap\n",
    "                self.result_df['calc_log_market_cap'] = np.log(market_cap)\n",
    "                ratios_calculated += 2\n",
    "        \n",
    "        # profitability ratios.\n",
    "        if 'net_income' in self.result_df.columns and 'total_assets' in self.result_df.columns:\n",
    "            self.result_df['calc_roa'] = self.result_df['net_income'] / self.result_df['total_assets'].replace(0, np.nan)\n",
    "            ratios_calculated += 1\n",
    "        \n",
    "        if 'net_income' in self.result_df.columns and 'shareholders_equity' in self.result_df.columns:\n",
    "            self.result_df['calc_roe'] = self.result_df['net_income'] / self.result_df['shareholders_equity'].replace(0, np.nan)\n",
    "            ratios_calculated += 1\n",
    "        \n",
    "        if 'net_income' in self.result_df.columns and 'revenue' in self.result_df.columns:\n",
    "            self.result_df['calc_profit_margin'] = self.result_df['net_income'] / self.result_df['revenue'].replace(0, np.nan)\n",
    "            ratios_calculated += 1\n",
    "        \n",
    "        # financial health ratios.\n",
    "        if 'long_term_debt' in self.result_df.columns and 'total_assets' in self.result_df.columns:\n",
    "            self.result_df['calc_debt_to_assets'] = self.result_df['long_term_debt'] / self.result_df['total_assets'].replace(0, np.nan)\n",
    "            ratios_calculated += 1\n",
    "        \n",
    "        if 'long_term_debt' in self.result_df.columns and 'shareholders_equity' in self.result_df.columns:\n",
    "            self.result_df['calc_debt_to_equity'] = self.result_df['long_term_debt'] / self.result_df['shareholders_equity'].replace(0, np.nan)\n",
    "            ratios_calculated += 1\n",
    "        \n",
    "        # valuation metrics.\n",
    "        if 'price_t0' in self.result_df.columns and 'shareholders_equity' in self.result_df.columns and 'shares_outstanding' in self.result_df.columns:\n",
    "            book_value_per_share = self.result_df['shareholders_equity'] / self.result_df['shares_outstanding'].replace(0, np.nan)\n",
    "            self.result_df['calc_price_to_book'] = self.result_df['price_t0'] / book_value_per_share\n",
    "            ratios_calculated += 1\n",
    "        \n",
    "        if 'price_t0' in self.result_df.columns and 'eps_diluted' in self.result_df.columns:\n",
    "            self.result_df['calc_price_to_earnings'] = self.result_df['price_t0'] / self.result_df['eps_diluted'].replace(0, np.nan)\n",
    "            ratios_calculated += 1\n",
    "        \n",
    "        print(f\"✅ Financial ratios calculated: {ratios_calculated}\")\n",
    "        \n",
    "        # getting rid of the temporary columns i created for the ratio calculations.\n",
    "        helper_cols = list(fund_mapping.values())\n",
    "        existing_helpers = [col for col in helper_cols if col in self.result_df.columns]\n",
    "        if existing_helpers:\n",
    "            self.result_df = self.result_df.drop(columns=existing_helpers)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def organize_final_structure(self):\n",
    "        \"\"\"the final step is to organize all the columns in a logical order to make the dataset easy to work with.\"\"\"\n",
    "        print(\"\\n📋 ORGANIZING FINAL STRUCTURE\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # i'm grouping my columns into categories to make them easier to find.\n",
    "        column_groups = {\n",
    "            'identifiers': ['gvkey', 'CIK', 'Ticker', 'Company Name', 'Sector', 'Year', 'filingDate'],\n",
    "            'ai_factors': [col for col in self.result_df.columns if any(x in col for x in \n",
    "                             ['Strategic Depth', 'Disclosure Sentiment', 'AI Washing', 'Forward-Looking', \n",
    "                              'Talent', 'Risk', 'Overall Summary', 'Key AI Terms', 'Cum_Score'])],\n",
    "            'prices': [col for col in self.result_df.columns if col.startswith('price_t')],\n",
    "            'returns_raw': [col for col in self.result_df.columns if col.startswith('return_') and 'excess' not in col],\n",
    "            'returns_excess': [col for col in self.result_df.columns if col.startswith('excess_return_')],\n",
    "            'ff_factors': [col for col in self.result_df.columns if col.startswith('ff_')],\n",
    "            'rf_cumulative': [col for col in self.result_df.columns if 'rf_' in col and 'cumulative' in col],\n",
    "            'sectors': [col for col in self.result_df.columns if col.startswith('sector_')],\n",
    "            'fundamentals': [col for col in self.result_df.columns if col.startswith('fund_')],\n",
    "            'calculated_ratios': [col for col in self.result_df.columns if col.startswith('calc_')]\n",
    "        }\n",
    "        \n",
    "        ordered_columns = []\n",
    "        for group_name, group_cols in column_groups.items():\n",
    "            if group_cols:\n",
    "                print(f\"    📊 {group_name}: {len(group_cols)} columns\")\n",
    "                ordered_columns.extend(sorted(group_cols))\n",
    "        \n",
    "        # adding any columns i might have missed.\n",
    "        remaining_cols = [col for col in self.result_df.columns if col not in ordered_columns]\n",
    "        if remaining_cols:\n",
    "            print(f\"    📋 Other: {len(remaining_cols)} columns\")\n",
    "            ordered_columns.extend(sorted(remaining_cols))\n",
    "        \n",
    "        # reordering the dataframe.\n",
    "        self.result_df = self.result_df[ordered_columns]\n",
    "        \n",
    "        # sorting the whole dataset.\n",
    "        if 'Ticker' in self.result_df.columns and 'Year' in self.result_df.columns:\n",
    "            self.result_df = self.result_df.sort_values(['Ticker', 'Year']).reset_index(drop=True)\n",
    "            print(f\"    ✅ Sorted by Ticker and Year\")\n",
    "        \n",
    "        print(f\"✅ Final structure organized: {self.result_df.shape}\")\n",
    "        return self\n",
    "    \n",
    "    def validate_final_dataset(self):\n",
    "        \"\"\"one last check to make sure everything looks right.\"\"\"\n",
    "        print(\"\\n🔍 FINAL DATASET VALIDATION\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # checking that all the data groups are present.\n",
    "        checks = {\n",
    "            'AI Factors': len([col for col in self.result_df.columns if 'Strategic Depth' in col or 'Sentiment' in col]) > 0,\n",
    "            'FF Factors': len([col for col in self.result_df.columns if col.startswith('ff_')]) >= 7,\n",
    "            'Cumulative RF': len([col for col in self.result_df.columns if 'cumulative' in col]) >= 4,\n",
    "            'Quality Returns': len([col for col in self.result_df.columns if col.startswith('return_')]) >= 4,\n",
    "            'Excess Returns': len([col for col in self.result_df.columns if col.startswith('excess_return_')]) >= 4,\n",
    "            'Fundamentals': len([col for col in self.result_df.columns if col.startswith('fund_')]) >= 8,\n",
    "            'Calculated Ratios': len([col for col in self.result_df.columns if col.startswith('calc_')]) >= 5\n",
    "        }\n",
    "        \n",
    "        all_passed = True\n",
    "        for check_name, result in checks.items():\n",
    "            status = \"✅\" if result else \"❌\"\n",
    "            print(f\"    {status} {check_name}\")\n",
    "            if not result:\n",
    "                all_passed = False\n",
    "        \n",
    "        # checking the quality of the returns i calculated.\n",
    "        print(f\"\\n    💰 RETURN QUALITY CHECK:\")\n",
    "        for horizon in [3, 6, 12]:\n",
    "            return_col = f'return_{horizon}mo'\n",
    "            if return_col in self.result_df.columns:\n",
    "                returns = self.result_df[return_col].dropna()\n",
    "                if len(returns) > 0:\n",
    "                    mean_ret, std_ret, min_ret, max_ret = returns.mean(), returns.std(), returns.min(), returns.max()\n",
    "                    \n",
    "                    # making sure the returns are within my predefined bounds.\n",
    "                    bounds_ok = min_ret >= self.min_return and max_ret <= self.max_return\n",
    "                    status = \"✅\" if bounds_ok else \"⚠️\"\n",
    "                    print(f\"        {status} {horizon}mo: μ={mean_ret:.1%}, σ={std_ret:.1%}, range=[{min_ret:.1%}, {max_ret:.1%}]\")\n",
    "        \n",
    "        if all_passed:\n",
    "            print(f\"\\n🎉 ALL VALIDATION CHECKS PASSED!\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️ SOME VALIDATION CHECKS FAILED\")\n",
    "        \n",
    "        return all_passed\n",
    "    \n",
    "    def build_complete_dataset(self):\n",
    "        \"\"\"this is the main function that runs all the steps in the correct order.\"\"\"\n",
    "        print(\"\\n🚀 BUILDING COMPLETE AI FACTOR DATASET\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # running the whole pipeline.\n",
    "            self.load_source_data()\n",
    "            self.add_fama_french_and_rf()\n",
    "            self.add_prices_and_calculate_quality_returns()\n",
    "            self.add_fundamentals_and_ratios()\n",
    "            self.organize_final_structure()\n",
    "            validation_passed = self.validate_final_dataset()\n",
    "            \n",
    "            if validation_passed:\n",
    "                print(f\"\\n🎉 COMPLETE DATASET READY!\")\n",
    "                print(f\"📊 Final shape: {self.result_df.shape}\")\n",
    "                print(f\"🛡️ Quality filters applied during calculation\")\n",
    "                print(f\"🧹 Clean structure - no dataset confusion\")\n",
    "                print(f\"🎯 Ready for AI factor analysis!\")\n",
    "                \n",
    "                return self.result_df\n",
    "            else:\n",
    "                print(f\"\\n⚠️ VALIDATION FAILED\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ ERROR DURING BUILDING:\")\n",
    "            print(f\"    {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "# --- usage: running the builder ---\n",
    "# here i'm creating an instance of my builder class and running the process.\n",
    "# i need to make sure my `merged_df` from the previous step is loaded correctly here.\n",
    "print(\"\\n🎯 INSTANTIATING AND RUNNING THE DATASET BUILDER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# assuming 'final_df' from the previous merging step is available, if not, load it.\n",
    "if 'final_df' not in locals() or final_df.empty:\n",
    "    print(\"loading `final_panel_with_scores.parquet` as base dataset...\")\n",
    "    path_to_base = os.path.join(BASE_DIR, 'final_panel_with_scores.parquet')\n",
    "    final_df = load_data(path_to_base, is_parquet=True)\n",
    "\n",
    "\n",
    "if not final_df.empty:\n",
    "    complete_builder = CompleteAIFactorDatasetBuilder(\n",
    "        base_ai_dataset=final_df,\n",
    "        daily_path=\"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Controls/daily.csv\",\n",
    "        fundamentals_path=\"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Controls/fundamentals.csv\",\n",
    "        ff_factors_path=\"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Controls/5factors.csv\"\n",
    "    )\n",
    "\n",
    "    # building the complete dataset.\n",
    "    final_enhanced_dataset = complete_builder.build_complete_dataset()\n",
    "\n",
    "    if final_enhanced_dataset is not None:\n",
    "        print(f\"\\n✅ READY TO SAVE!\")\n",
    "        \n",
    "        # saving the final, enhanced dataset.\n",
    "        save_path = \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/final_enhanced_dataset.csv\"\n",
    "        final_enhanced_dataset.to_csv(save_path, index=False)\n",
    "        \n",
    "        print(f\"💾 Saved: {save_path}\")\n",
    "        print(f\"📊 Shape: {final_enhanced_dataset.shape}\")\n",
    "        print(f\"🎯 ONE DATASET, NO CONFUSION!\")\n",
    "else:\n",
    "    print(\"could not load the base AI dataset, so i can't build the final version.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27ec63-0dbe-4b99-9480-f304d07fc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FactorLoadingsCalculator:\n",
    "    \"\"\"\n",
    "    this is my factor loadings calculator. i'm using it to figure out the betas\n",
    "    for each company against different fama-french factors. this version has\n",
    "    all the bugs i found earlier fixed, especially the look-ahead bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, daily_prices_path, ff_factors_path, final_dataset_path):\n",
    "        \"\"\"\n",
    "        i'm setting up the calculator with the paths to all the data i need.\n",
    "        \"\"\"\n",
    "        self.daily_prices_path = daily_prices_path\n",
    "        self.ff_factors_path = ff_factors_path\n",
    "        self.final_dataset_path = final_dataset_path\n",
    "        \n",
    "        print(\"🔧 Factor Loadings Calculator Initialized (version 1)\")\n",
    "        print(f\"    - using daily prices from: {daily_prices_path}\")\n",
    "        print(f\"    - using ff factors from: {ff_factors_path}\")\n",
    "        print(f\"    - starting with dataset: {final_dataset_path}\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        here i'm loading all the necessary files into memory.\n",
    "        \"\"\"\n",
    "        print(\"\\n# loading my data files...\")\n",
    "        \n",
    "        # loading the fama-french factors and making sure they're in decimals, not percentages.\n",
    "        self.ff_factors = pd.read_csv(self.ff_factors_path)\n",
    "        try:\n",
    "            self.ff_factors['date'] = pd.to_datetime(self.ff_factors['date'], format='%Y%m%d')\n",
    "        except:\n",
    "            self.ff_factors['date'] = pd.to_datetime(self.ff_factors['date'])\n",
    "        \n",
    "        factor_cols = ['mktrf', 'smb', 'hml', 'rmw', 'cma', 'rf', 'umd']\n",
    "        for col in factor_cols:\n",
    "            if col in self.ff_factors.columns and self.ff_factors[col].abs().max() > 1:\n",
    "                self.ff_factors[col] = self.ff_factors[col] / 100\n",
    "        \n",
    "        self.ff_factors = self.ff_factors.sort_values('date').reset_index(drop=True)\n",
    "        print(f\"  ...loaded Fama-French Factors: {self.ff_factors.shape}\")\n",
    "        \n",
    "        # loading the daily stock prices and making sure the gvkey is a standard format.\n",
    "        self.daily_prices = pd.read_csv(self.daily_prices_path, dtype={'gvkey': str})\n",
    "        self.daily_prices['gvkey'] = self.daily_prices['gvkey'].str.strip().str.zfill(6)\n",
    "        self.daily_prices['datadate'] = pd.to_datetime(self.daily_prices['datadate'])\n",
    "        self.daily_prices = self.daily_prices[self.daily_prices['prccd'].notna() & (self.daily_prices['prccd'] > 0)].copy()\n",
    "        self.daily_prices = self.daily_prices.sort_values(['gvkey', 'datadate']).reset_index(drop=True)\n",
    "        print(f\"  ...loaded Daily Prices: {self.daily_prices.shape}\")\n",
    "\n",
    "        # loading my main dataset that i created with the builder.\n",
    "        self.final_dataset = pd.read_csv(self.final_dataset_path, dtype={'gvkey': str, 'CIK': str})\n",
    "        self.final_dataset['gvkey'] = self.final_dataset['gvkey'].str.strip().str.zfill(6)\n",
    "        if 'filingDate' in self.final_dataset.columns:\n",
    "            self.final_dataset['filingDate'] = pd.to_datetime(self.final_dataset['filingDate'])\n",
    "        print(f\"  ...loaded Final Enhanced Dataset: {self.final_dataset.shape}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def calculate_daily_returns(self):\n",
    "        \"\"\"\n",
    "        calculating daily returns from the price data. this is a crucial step.\n",
    "        \"\"\"\n",
    "        print(\"\\n# calculating daily stock returns...\")\n",
    "        \n",
    "        returns_list = []\n",
    "        \n",
    "        # i'm only processing firms that are in both my daily price file and my final dataset.\n",
    "        processable_gvkeys = set(self.daily_prices['gvkey'].unique()).intersection(set(self.final_dataset['gvkey'].unique()))\n",
    "        print(f\"  ...processing returns for {len(processable_gvkeys)} firms.\")\n",
    "        \n",
    "        for gvkey in tqdm(processable_gvkeys, desc=\"calculating daily returns\"):\n",
    "            firm_data = self.daily_prices[self.daily_prices['gvkey'] == gvkey].copy()\n",
    "            \n",
    "            # i need at least 100 days of data to get a reliable estimate.\n",
    "            if len(firm_data) < 100:\n",
    "                continue\n",
    "            \n",
    "            firm_data = firm_data.sort_values('datadate')\n",
    "            firm_data['price_lag'] = firm_data['prccd'].shift(1)\n",
    "            firm_data['daily_return'] = (firm_data['prccd'] / firm_data['price_lag']) - 1\n",
    "            \n",
    "            # i'll winsorize the returns to handle extreme outliers.\n",
    "            firm_data['daily_return'] = firm_data['daily_return'].clip(-1, 1)\n",
    "            \n",
    "            returns_list.append(firm_data[['gvkey', 'datadate', 'daily_return']].dropna())\n",
    "        \n",
    "        if returns_list:\n",
    "            self.daily_returns = pd.concat(returns_list, ignore_index=True)\n",
    "            print(f\"  ...finished calculating daily returns. total observations: {len(self.daily_returns):,}\")\n",
    "        else:\n",
    "            raise ValueError(\"could not calculate any daily returns.\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def merge_with_ff_factors(self):\n",
    "        \"\"\"\n",
    "        now i'll merge the daily returns i just calculated with the fama-french factors.\n",
    "        \"\"\"\n",
    "        print(\"\\n# merging daily returns with fama-french factors...\")\n",
    "        \n",
    "        self.returns_with_factors = pd.merge(\n",
    "            self.daily_returns, self.ff_factors, left_on='datadate', right_on='date', how='inner'\n",
    "        )\n",
    "        \n",
    "        # this gives me the daily excess return.\n",
    "        if 'rf' in self.returns_with_factors.columns:\n",
    "            self.returns_with_factors['excess_return'] = (\n",
    "                self.returns_with_factors['daily_return'] - self.returns_with_factors['rf']\n",
    "            )\n",
    "        \n",
    "        print(f\"  ...merge complete. dataset shape: {self.returns_with_factors.shape}\")\n",
    "        return self\n",
    "\n",
    "    def calculate_loadings(self, estimation_months=6, min_observations=100):\n",
    "        \"\"\"\n",
    "        this is the main part where i run the regressions to get the factor loadings.\n",
    "        i'm using a standard academic approach: a 6-month window before the filing date.\n",
    "        \"\"\"\n",
    "        print(\"\\n# calculating factor loadings...\")\n",
    "        \n",
    "        factor_loadings_list = []\n",
    "        \n",
    "        processable_gvkeys = set(self.final_dataset['gvkey'].unique()).intersection(set(self.returns_with_factors['gvkey'].unique()))\n",
    "        print(f\"  ...calculating for {len(processable_gvkeys)} firms.\")\n",
    "        \n",
    "        for gvkey in tqdm(processable_gvkeys, desc=\"calculating factor loadings\"):\n",
    "            firm_data = self.returns_with_factors[self.returns_with_factors['gvkey'] == gvkey]\n",
    "            firm_filings = self.final_dataset[self.final_dataset['gvkey'] == gvkey]\n",
    "            \n",
    "            for _, filing_row in firm_filings.iterrows():\n",
    "                filing_date = filing_row['filingDate']\n",
    "                year = filing_row.get('Year', filing_date.year)\n",
    "                \n",
    "                # this is the estimation window: 6 months ending 1 day before the filing. no look-ahead bias.\n",
    "                estimation_end = filing_date - pd.Timedelta(days=1)\n",
    "                estimation_start = estimation_end - pd.DateOffset(months=estimation_months)\n",
    "                \n",
    "                window_data = firm_data[(firm_data['datadate'] >= estimation_start) & (firm_data['datadate'] <= estimation_end)]\n",
    "                \n",
    "                # again, i need enough data points to run a meaningful regression.\n",
    "                if len(window_data) < min_observations:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # setting up the regression: y is the excess return, x are the factors.\n",
    "                    y = window_data['excess_return']\n",
    "                    factor_names = ['mktrf', 'smb', 'hml', 'rmw', 'cma', 'umd']\n",
    "                    X = window_data[factor_names]\n",
    "                    X = sm.add_constant(X) # for the alpha (intercept).\n",
    "                    \n",
    "                    model = OLS(y, X).fit()\n",
    "                    \n",
    "                    # storing all the results: betas, t-stats, p-values, and r-squared.\n",
    "                    result = {'gvkey': gvkey, 'Year': year}\n",
    "                    result[f'alpha'] = model.params.get('const')\n",
    "                    result[f'r_squared'] = model.rsquared\n",
    "                    result[f'n_obs'] = model.nobs\n",
    "                    for factor in factor_names:\n",
    "                        result[f'beta_{factor}'] = model.params.get(factor)\n",
    "                        result[f'tstat_{factor}'] = model.tvalues.get(factor)\n",
    "                        result[f'pvalue_{factor}'] = model.pvalues.get(factor)\n",
    "                    \n",
    "                    factor_loadings_list.append(result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # just in case a regression fails for some reason.\n",
    "                    print(f\"could not run regression for gvkey {gvkey}, year {year}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        if factor_loadings_list:\n",
    "            self.factor_loadings = pd.DataFrame(factor_loadings_list)\n",
    "            print(f\"  ...finished calculating loadings. created {len(self.factor_loadings)} results.\")\n",
    "        else:\n",
    "            raise ValueError(\"could not calculate any factor loadings.\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def merge_loadings_with_final_dataset(self):\n",
    "        \"\"\"\n",
    "        the final step is to merge these new factor loadings back into my main dataset.\n",
    "        \"\"\"\n",
    "        print(\"\\n# merging factor loadings into the final dataset...\")\n",
    "        \n",
    "        self.final_dataset_with_loadings = pd.merge(\n",
    "            self.final_dataset, self.factor_loadings, on=['gvkey', 'Year'], how='left'\n",
    "        )\n",
    "        print(f\"  ...merge complete. final shape: {self.final_dataset_with_loadings.shape}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# --- running the entire process ---\n",
    "print(\"\\n--- initiating the factor loading construction process (version 1) ---\")\n",
    "\n",
    "# here i create an instance of my builder and run all the steps in order.\n",
    "try:\n",
    "    calculator = FactorLoadingsCalculator(\n",
    "        daily_prices_path=path_daily_prices,\n",
    "        ff_factors_path=path_ff_factors,\n",
    "        final_dataset_path=path_merged_ai_scores\n",
    "    )\n",
    "\n",
    "    # running the full pipeline.\n",
    "    calculator.load_data()\n",
    "    calculator.calculate_daily_returns()\n",
    "    calculator.merge_with_ff_factors()\n",
    "    calculator.calculate_loadings()\n",
    "    calculator.merge_loadings_with_final_dataset()\n",
    "\n",
    "    # getting the final dataframe with everything included.\n",
    "    final_dataset_with_factors = calculator.final_dataset_with_loadings\n",
    "    \n",
    "    # saving the final result. this is the dataset i'll use for my regressions.\n",
    "    final_save_path = os.path.join(BASE_DIR, \"final_dataset_with_loadings.parquet\")\n",
    "    final_dataset_with_factors.to_parquet(final_save_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ SUCCESS! final dataset with factor loadings is complete.\")\n",
    "    print(f\"   saved to: {final_save_path}\")\n",
    "    print(f\"   final shape: {final_dataset_with_factors.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ an error occurred during the process: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baacb7d-0736-4fb7-878b-95a611ad9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FinancialFactorsCalculator:\n",
    "    \"\"\"\n",
    "    this is my all-in-one calculator for financial factors. it now handles both:\n",
    "    1. factor loadings (betas) for the fama-french model.\n",
    "    2. momentum returns from the months prior to my filing date.\n",
    "    this will be the last major step before i start my regressions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, daily_prices_path, ff_factors_path, base_dataset_path):\n",
    "        \"\"\"\n",
    "        setting up the calculator with the paths to all my data.\n",
    "        the base_dataset_path should point to the file created by the CompleteAIFactorDatasetBuilder.\n",
    "        \"\"\"\n",
    "        self.daily_prices_path = daily_prices_path\n",
    "        self.ff_factors_path = ff_factors_path\n",
    "        self.base_dataset_path = base_dataset_path\n",
    "        \n",
    "        print(\"🔧 Financial Factors Calculator Initialized (version 1)\")\n",
    "        print(f\"    - using daily prices from: {daily_prices_path}\")\n",
    "        print(f\"    - using ff factors from: {ff_factors_path}\")\n",
    "        print(f\"    - starting with dataset: {base_dataset_path}\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        loading all the necessary files into memory.\n",
    "        \"\"\"\n",
    "        print(\"\\n# loading my data files...\")\n",
    "        \n",
    "        # loading fama-french factors and making sure they're decimals.\n",
    "        self.ff_factors = pd.read_csv(self.ff_factors_path)\n",
    "        try:\n",
    "            self.ff_factors['date'] = pd.to_datetime(self.ff_factors['date'], format='%Y%m%d')\n",
    "        except:\n",
    "            self.ff_factors['date'] = pd.to_datetime(self.ff_factors['date'])\n",
    "        \n",
    "        factor_cols = ['mktrf', 'smb', 'hml', 'rmw', 'cma', 'rf', 'umd']\n",
    "        for col in factor_cols:\n",
    "            if col in self.ff_factors.columns and self.ff_factors[col].abs().max() > 1:\n",
    "                self.ff_factors[col] = self.ff_factors[col] / 100\n",
    "        \n",
    "        self.ff_factors = self.ff_factors.sort_values('date').reset_index(drop=True)\n",
    "        print(f\"  ...loaded Fama-French Factors: {self.ff_factors.shape}\")\n",
    "        \n",
    "        # loading daily stock prices and standardizing the gvkey.\n",
    "        self.daily_prices = pd.read_csv(self.daily_prices_path, dtype={'gvkey': str})\n",
    "        self.daily_prices['gvkey'] = self.daily_prices['gvkey'].str.strip().str.zfill(6)\n",
    "        self.daily_prices['datadate'] = pd.to_datetime(self.daily_prices['datadate'])\n",
    "        self.daily_prices = self.daily_prices[self.daily_prices['prccd'].notna() & (self.daily_prices['prccd'] > 0)].copy()\n",
    "        self.daily_prices = self.daily_prices.sort_values(['gvkey', 'datadate']).reset_index(drop=True)\n",
    "        print(f\"  ...loaded Daily Prices: {self.daily_prices.shape}\")\n",
    "\n",
    "        # loading my main dataset created by the previous builder step.\n",
    "        self.final_dataset = pd.read_csv(self.base_dataset_path, dtype={'gvkey': str, 'CIK': str})\n",
    "        self.final_dataset['gvkey'] = self.final_dataset['gvkey'].str.strip().str.zfill(6)\n",
    "        if 'filingDate' in self.final_dataset.columns:\n",
    "            self.final_dataset['filingDate'] = pd.to_datetime(self.final_dataset['filingDate'])\n",
    "        print(f\"  ...loaded Final Enhanced Dataset: {self.final_dataset.shape}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def calculate_daily_returns(self):\n",
    "        \"\"\"\n",
    "        calculating daily returns, which i'll need for both loadings and momentum.\n",
    "        \"\"\"\n",
    "        print(\"\\n# calculating daily stock returns...\")\n",
    "        \n",
    "        returns_list = []\n",
    "        processable_gvkeys = set(self.daily_prices['gvkey'].unique()).intersection(set(self.final_dataset['gvkey'].unique()))\n",
    "        print(f\"  ...processing returns for {len(processable_gvkeys)} firms.\")\n",
    "        \n",
    "        for gvkey in tqdm(processable_gvkeys, desc=\"calculating daily returns\"):\n",
    "            firm_data = self.daily_prices[self.daily_prices['gvkey'] == gvkey].copy()\n",
    "            if len(firm_data) < 100:\n",
    "                continue\n",
    "            \n",
    "            firm_data = firm_data.sort_values('datadate')\n",
    "            firm_data['price_lag'] = firm_data['prccd'].shift(1)\n",
    "            firm_data['daily_return'] = (firm_data['prccd'] / firm_data['price_lag']) - 1\n",
    "            firm_data['daily_return'] = firm_data['daily_return'].clip(-0.5, 0.5) # winsorizing daily returns\n",
    "            \n",
    "            returns_list.append(firm_data[['gvkey', 'datadate', 'daily_return']].dropna())\n",
    "        \n",
    "        if not returns_list:\n",
    "            raise ValueError(\"could not calculate any daily returns.\")\n",
    "            \n",
    "        self.daily_returns = pd.concat(returns_list, ignore_index=True)\n",
    "        print(f\"  ...finished calculating daily returns. total observations: {len(self.daily_returns):,}\")\n",
    "        return self\n",
    "\n",
    "    def merge_with_ff_factors(self):\n",
    "        \"\"\"\n",
    "        merging the daily returns with the fama-french factors to get daily excess returns.\n",
    "        \"\"\"\n",
    "        print(\"\\n# merging daily returns with fama-french factors...\")\n",
    "        \n",
    "        self.returns_with_factors = pd.merge(\n",
    "            self.daily_returns, self.ff_factors, left_on='datadate', right_on='date', how='inner'\n",
    "        )\n",
    "        \n",
    "        if 'rf' in self.returns_with_factors.columns:\n",
    "            self.returns_with_factors['excess_return'] = self.returns_with_factors['daily_return'] - self.returns_with_factors['rf']\n",
    "        \n",
    "        print(f\"  ...merge complete. dataset shape: {self.returns_with_factors.shape}\")\n",
    "        return self\n",
    "\n",
    "    def calculate_factor_loadings(self, estimation_months=6, min_observations=100):\n",
    "        \"\"\"\n",
    "        running the regressions to get the factor loadings (betas).\n",
    "        \"\"\"\n",
    "        print(\"\\n# calculating factor loadings...\")\n",
    "        \n",
    "        factor_loadings_list = []\n",
    "        processable_gvkeys = set(self.final_dataset['gvkey'].unique()).intersection(set(self.returns_with_factors['gvkey'].unique()))\n",
    "        print(f\"  ...calculating for {len(processable_gvkeys)} firms.\")\n",
    "        \n",
    "        for gvkey in tqdm(processable_gvkeys, desc=\"calculating factor loadings\"):\n",
    "            firm_data = self.returns_with_factors[self.returns_with_factors['gvkey'] == gvkey]\n",
    "            firm_filings = self.final_dataset[self.final_dataset['gvkey'] == gvkey]\n",
    "            \n",
    "            for _, filing_row in firm_filings.iterrows():\n",
    "                filing_date = filing_row['filingDate']\n",
    "                year = filing_row.get('Year', filing_date.year)\n",
    "                \n",
    "                estimation_end = filing_date - pd.Timedelta(days=1)\n",
    "                estimation_start = estimation_end - pd.DateOffset(months=estimation_months)\n",
    "                window_data = firm_data[(firm_data['datadate'] >= estimation_start) & (firm_data['datadate'] <= estimation_end)]\n",
    "                \n",
    "                if len(window_data) < min_observations:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    y = window_data['excess_return']\n",
    "                    factor_names = ['mktrf', 'smb', 'hml', 'rmw', 'cma', 'umd']\n",
    "                    X = window_data[factor_names]\n",
    "                    X = sm.add_constant(X)\n",
    "                    model = OLS(y, X).fit()\n",
    "                    \n",
    "                    result = {'gvkey': gvkey, 'Year': year}\n",
    "                    result['alpha'] = model.params.get('const')\n",
    "                    result['r_squared'] = model.rsquared\n",
    "                    result['n_obs'] = model.nobs\n",
    "                    for factor in factor_names:\n",
    "                        result[f'beta_{factor}'] = model.params.get(factor)\n",
    "                    \n",
    "                    factor_loadings_list.append(result)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                    \n",
    "        self.factor_loadings = pd.DataFrame(factor_loadings_list)\n",
    "        print(f\"  ...finished calculating loadings. created {len(self.factor_loadings)} results.\")\n",
    "        return self\n",
    "        \n",
    "    def calculate_momentum(self, months_back=[1, 2, 3]):\n",
    "        \"\"\"\n",
    "        calculating momentum as the cumulative return over prior months.\n",
    "        \"\"\"\n",
    "        print(\"\\n# calculating momentum factors...\")\n",
    "        \n",
    "        momentum_results = []\n",
    "        processable_gvkeys = set(self.final_dataset['gvkey'].unique()).intersection(set(self.daily_returns['gvkey'].unique()))\n",
    "        print(f\"  ...calculating for {len(processable_gvkeys)} firms.\")\n",
    "        \n",
    "        for gvkey in tqdm(processable_gvkeys, desc=\"calculating momentum\"):\n",
    "            firm_data = self.daily_returns[self.daily_returns['gvkey'] == gvkey]\n",
    "            firm_filings = self.final_dataset[self.final_dataset['gvkey'] == gvkey]\n",
    "            \n",
    "            for _, filing_row in firm_filings.iterrows():\n",
    "                filing_date = filing_row['filingDate']\n",
    "                year = filing_row.get('Year', filing_date.year)\n",
    "                result = {'gvkey': gvkey, 'Year': year}\n",
    "                \n",
    "                for months in months_back:\n",
    "                    lookback_end = filing_date - pd.Timedelta(days=5)\n",
    "                    lookback_start = lookback_end - pd.DateOffset(months=months)\n",
    "                    window_data = firm_data[(firm_data['datadate'] >= lookback_start) & (firm_data['datadate'] <= lookback_end)]\n",
    "                    \n",
    "                    if len(window_data) > 10:\n",
    "                        cumulative_return = (1 + window_data['daily_return']).prod() - 1\n",
    "                        result[f'momentum_{months}m'] = cumulative_return\n",
    "                \n",
    "                momentum_results.append(result)\n",
    "\n",
    "        self.momentum_data = pd.DataFrame(momentum_results)\n",
    "        print(f\"  ...finished calculating momentum. created {len(self.momentum_data)} results.\")\n",
    "        return self\n",
    "\n",
    "    def merge_factors_into_final_dataset(self):\n",
    "        \"\"\"\n",
    "        merging both the factor loadings and the momentum factors into my main dataset.\n",
    "        \"\"\"\n",
    "        print(\"\\n# merging all new factors into the final dataset...\")\n",
    "        \n",
    "        # merging factor loadings.\n",
    "        self.final_dataset_with_factors = pd.merge(\n",
    "            self.final_dataset, self.factor_loadings, on=['gvkey', 'Year'], how='left'\n",
    "        )\n",
    "        print(f\"  ...merged factor loadings. shape: {self.final_dataset_with_factors.shape}\")\n",
    "\n",
    "        # merging momentum.\n",
    "        if hasattr(self, 'momentum_data'):\n",
    "            self.final_dataset_with_factors = pd.merge(\n",
    "                self.final_dataset_with_factors, self.momentum_data, on=['gvkey', 'Year'], how='left'\n",
    "            )\n",
    "            print(f\"  ...merged momentum factors. shape: {self.final_dataset_with_factors.shape}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# --- running the entire process ---\n",
    "print(\"\\n--- initiating the financial factor construction process (version 1) ---\")\n",
    "\n",
    "try:\n",
    "    # i'm creating an instance of my calculator class.\n",
    "    # IMPORTANT: i'm making sure it uses the correct dataset from the previous builder step.\n",
    "    calculator = FinancialFactorsCalculator(\n",
    "        daily_prices_path=\"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Controls/daily.csv\",\n",
    "        ff_factors_path=\"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Controls/5factors.csv\",\n",
    "        base_dataset_path=\"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master Finance/MasterThesis/ThesisData/Regression/final_enhanced_dataset.csv\"\n",
    "    )\n",
    "\n",
    "    # running the full pipeline.\n",
    "    calculator.load_data()\n",
    "    calculator.calculate_daily_returns()\n",
    "    calculator.merge_with_ff_factors()\n",
    "    calculator.calculate_factor_loadings()\n",
    "    calculator.calculate_momentum()\n",
    "    calculator.merge_factors_into_final_dataset()\n",
    "\n",
    "    # getting the final dataframe with everything included.\n",
    "    final_dataset_with_all_factors = calculator.final_dataset_with_factors\n",
    "    \n",
    "    # saving the final result. this is the one i'll use for my regressions.\n",
    "    final_save_path = os.path.join(BASE_DIR, \"Regression/final_dataset_with_all_factors.parquet\")\n",
    "    final_dataset_with_all_factors.to_parquet(final_save_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ SUCCESS! final dataset with loadings and momentum is complete.\")\n",
    "    print(f\"   saved to: {final_save_path}\")\n",
    "    print(f\"   final shape: {final_dataset_with_all_factors.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ an error occurred during the process: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a298ca-7535-43ff-8385-09b5f6c9761b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
